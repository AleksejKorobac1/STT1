00:00:01 the following is a conversation with Stuart Russell he's a professor of computer science at UC Berkeley and a
00:00:08 co-author of a book that introduced me and millions of other people to the amazing world of AI called artificial
00:00:16 intelligence a modern approach so it was an honor for me to have this conversation as part of MIT course and
00:00:23 artificial general intelligence and the artificial intelligence podcast if you enjoy it please subscribe on youtube
00:00:30 itunes or your podcast provider of choice or simply connect with me on twitter at Lex Friedman spelled Fri D
00:00:38 and now here's my conversation with Stuart Russell so you've mentioned in 1975 in high school you've created one
00:00:49 year first AI programs that play chess were you ever able to build a program that beat you a chess or another board
00:01:01 game so my program never beat me at chess I actually wrote the program at Imperial College so I used to take the
00:01:11 bus every Wednesday with a box of cards this big and shove them into the card reader and they gave us eight seconds of
00:01:18 CPU time it took about five seconds to read the cards in and compile the code so we had
00:01:26 three seconds of CPU time which was enough to make one move you know with a not very deep search and then we would
00:01:32 print that move out and then we'd have to go to the back of the queue and wait to feed the cards in again how do you
00:01:40 post a search well I would talk to no I think we got we got an eight move eight you know depth eight with alpha beta and
00:01:49 we had some tricks of our own about move ordering and some pruning of the tree and we were still able to beat that
00:01:57 program yeah yeah I I was a reasonable chess player in my youth I did Anna fellow program and a backgammon program
00:02:04 so when I go to Berkley I worked a lot on what we call meta reasoning which really means reasoning about reasoning
00:02:13 and in the case of a game playing program you need to reason about what parts of the search tree you're actually
00:02:20 going to explore because the search tree is enormous or you know bigger than the number of atoms in the universe and the
00:02:29 way programs succeed and the way humans succeed is by only looking at a small fraction of the search tree and if you
00:02:36 look at the right fraction you play really well if you look at the wrong fraction if you waste your time thinking
00:02:42 about things that are never gonna happen the moves that no one's ever gonna make then you're gonna lose because you you
00:02:48 won't be able to figure out the right decision so that question of how machines can manage their own computation either how
00:02:58 they decide what to think about is the meta-reasoning question we developed some methods for doing that
00:03:05 and very simply a machine should think about whatever thoughts are going to improve its decision quality we were
00:03:13 able to show that both for a fellow which is a standard to play game and for backgammon which includes dice for also
00:03:21 it's a two-player game with uncertainty for both of those cases we could come up with algorithms that were actually much
00:03:29 more efficient than the standard alpha beta search which chess programs at the time we're using and that those programs
00:03:41 could beat me and I think you can see same basic ideas in alphago and alpha zero today the way they explored the
00:03:51 tree is using a former meta reasoning to select what to think about based on how useful it is to think about it is there
00:03:59 any insights you can describe without Greek symbols of how do we select which paths to go down there's really two
00:04:08 kinds of learning going on so as you say alphago learns to evaluate board position so it can it can look at a go
00:04:17 board and it actually has probably a superhuman ability to instantly tell how promising that situation is to me the
00:04:28 amazing thing about alphago is not that it can be the world champion with its hands tied behind his back but the fact
00:04:38 that if you stop it from searching altogether so you say okay you're not allowed to do any thinking ahead
00:04:44 right you can just consider each of your legal moves and then look at the resulting situation and evaluate it so
00:04:52 what we call a depth one search so just the immediate outcome of your moves and decide if that's good or bad
00:05:00 that version of alphago can still play at a professional level right and human professionals are sitting there for five
00:05:07 ten minutes deciding what to do and alphago in less than a second instantly into it what is the right move
00:05:15 to make based on its ability to evaluate positions and that is remarkable because you know we don't have that level of
00:05:24 intuition about go we actually have to think about the situation so anyway that capability that alphago has is one big
00:05:36 part of why it beats humans the other big part is that it's able to look ahead 40 50 60 moves into the future mm-hmm
00:05:48 and you know if it was considering all possibilities 40 or 50 or 60 moves into the future that would be you know 10 to
00:05:57 the 200 possibility so wait way more than you know atoms in the universe and and so on
00:06:05 so it's very very selective about what it looks at so let me try to give you an intuition
00:06:12 about how you decide what to think about it's a combination of two things one is how promising it is right so if you're
00:06:21 already convinced that a move is terrible there's no point spending a lot more time convincing yourself that it's
00:06:29 terrible because it's probably not gonna change your mind so the the real reason you think is because there's some
00:06:35 possibility of changing your mind about what to do mm-hmm right and is that changing your mind
00:06:43 that would result then in a better final action in the real world so that's the purpose of thinking is to improve the
00:06:51 final action in the real world and so if you think about a move that is guaranteed to be terrible you can
00:06:57 convince yourself is terrible and you're still not gonna change your mind all right but on the other hand you I suppose you
00:07:04 had a choice between two moves one of them you've already figured out is guaranteed to be a draw let's say and
00:07:10 then the other one looks a little bit worse like it looks fairly likely that if you make that move you're gonna lose
00:07:17 but there's still some uncertainty about the value of that move there's still some possibility that it will turn out
00:07:23 to be a win all right then it's worth thinking about that so even though it's less promising on average than the other
00:07:30 move which is guaranteed to be a draw there's still some purpose in thinking about it because there's a chance that
00:07:36 you will change your mind and discover that in fact it's a better move so it's a combination of how good the move
00:07:43 appears to be and how much I'm certainty there is about its value the more uncertainty the more it's worth thinking
00:07:50 about because there's a higher upside if you want to think of it that way and of course in the beginning especially in
00:07:58 the alphago 0 formulation it's everything is shrouded in uncertainty so you're really swimming in a sea of
00:08:06 uncertainty so it benefits you too I mean actually following the same process as you described but because you're so
00:08:13 uncertain about everything you you basically have to try a lot of different directions yeah so so the early parts of
00:08:20 the search tree a fairly bushy that it will when looking a lot of different possibilities but fairly
00:08:26 quickly the degree of certainty about some of the moves I mean if a movies are really terrible you'll pretty quickly
00:08:32 find out right you lose half your pieces or half your territory and and then you'll say okay this this is not worth
00:08:39 thinking about any more and then so a further down the tree becomes very long and narrow and you're following various
00:08:50 lines of play you know 10 20 30 40 50 moves into the future and you know that's again it's something that human
00:08:58 beings have a very hard time doing mainly because they just lacked the short-term memory you just can't
00:09:05 remember a sequence of moves that's 50 movies long and you can't you can't imagine the board correctly for that
00:09:13 money moves into the future of course the top players I'm much more familiar with chess but the top players probably
00:09:21 have they have echoes of the same kind of intuition instinct that in a moment's time alphago applies when they see a board
00:09:30 I mean they've seen those patterns human beings have seen those patterns before at the top at the Grandmaster level it
00:09:41 seems that there is some similarities or maybe it's it's our imagination creates a vision of those similarities but it
00:09:48 feels like this kind of pattern recognition that the alphago approaches are using is similar to what human
00:09:56 beings at the top level or using I think there's there's some truth to that but not entirely yeah I mean I think the the
00:10:08 extent to which a human Grandmaster can reliably wreak instantly recognize the right move instantly recognize the value
00:10:15 of a position I think that's a little bit overrated but if you sacrifice a queen for exam I mean there's these
00:10:22 there's these beautiful games of chess with Bobby Fischer somebody where it's seeming to make a bad move and I'm not
00:10:30 sure there's a a perfect degree of calculation involved were they've calculated all the possible
00:10:37 things that happen but there's an instinct there right that somehow adds up to the yeah so I think what happens
00:10:46 is you you you get a sense that there's some possibility in the position even if you make a weird-looking move that it
00:10:57 opens up some some lines of of calculation that otherwise would be definitely bad and and is that intuition
00:11:08 that there's something here in this position that might might yield a win down the side and then you follow that
00:11:17 right and and in some sense when when a chess player is following a line and in his or her mind they're they mentally
00:11:26 simulating what the other person is gonna do while the opponent is gonna do and they can do that as long as the
00:11:33 moves are kind of forced right as long as there's a you know there's a fourth we call a forcing variation where the
00:11:38 opponent doesn't really have much choice how to respond and then you see if you can force them into a situation where
00:11:45 you win you know we see plenty of mistakes even even in Grandmaster games where they just miss some simple three
00:11:56 four five move combination that you know wasn't particularly apparent in in the position but we're still there that's
00:12:02 the thing that makes us human yeah so when you mentioned that in a fellow those games were after some meta
00:12:11 reasoning improvements and research I was able to beat you how did that make you feel part of the meta reasoning
00:12:19 capability that it had was based on learning and and you could sit down the next day and you could just feel that it
00:12:29 had got a lot smarter boom you know and all the sudden you really felt like you sort of pressed against
00:12:37 the wall because it was it was much more aggressive and was totally unforgiving of any minor mistake that you might make
00:12:45 and and actually it seemed understood the game better than I did and you know Gary Kasparov has this quote weary
00:12:54 during his match against deep blue he said he suddenly felt that there was a new kind of intelligence across the
00:13:01 board do you think that's a scary or an exciting possibility that's prevent for yourself in in the context of chess
00:13:12 purely sort of in this like that feeling whatever that is I think it's definitely an exciting feeling you know this is
00:13:22 what made me work on AI in the first place was as soon as I really understood what a computer was I wanted to make it
00:13:29 smart you know I started out with the first program I wrote was for the sinclair programmable calculator and i
00:13:37 think you could write a 21 step algorithm that was the biggest program you could write something like that and
00:13:45 do little arithmetic calculations so I say think I implemented Newton's method for square roots and a few other things
00:13:49 like that but then you know I thought okay if I just had more space I could make this
00:13:58 thing intelligent and I think the the the thing that's scary is not is not the chess program
00:14:15 because you know chess programs they're not in they're taking over the world business but if you extrapolate
00:14:27 you know there are things about chess that don't resemble the real world right chess board is completely visible to the
00:14:40 programmer of course the real world is not most you most the real world is not visible from wherever you're sitting so
00:14:46 to speak and to overcome those kinds of problems you need qualitatively different
00:14:57 algorithms another thing about the real world is that you know we we regularly plan ahead on the timescales involving
00:15:09 billions or trillions of steps now we don't plan that was in detail but you know when you choose to do a PhD at Berkeley
00:15:18 that's a five-year commitment and that amounts to about a trillion motor control steps that you will eventually
00:15:25 be committed to including going up the stairs opening doors drinking water type yeah I mean every every finger movement
00:15:33 while you're typing every character of every paper and the thesis and everything else so you're not commuting
00:15:37 in advance to the specific motor control steps but you're still reasoning on a timescale that will eventually reduce to
00:15:47 trillions of motor control actions and so for all these reasons you know alphago and and deep blue and
00:15:57 so on don't represent any kind of threat to humanity but they are a step towards it right near that and progress in AI
00:16:07 occurs by essentially removing one by one these assumptions that make problems easy like the assumption of complete
00:16:16 observability of the situation right we remove that assumption you need a much more complicated kind of a computing
00:16:23 design and you need something that actually keeps track of all the things you can't see and tries to estimate
00:16:29 what's going on and there's inevitable uncertainty in that so it becomes a much more complicated problem but you know we
00:16:37 are removing those assumptions we are starting to have algorithms that can cope with much longer timescales
00:16:44 they can cope with uncertainty they can cope with partial observability and so each of those steps sort of
00:16:53 magnifies by a thousand the range of things that we can do with AI systems so the way I started me I wanted to be a
00:16:59 psychiatrist for long time to understand the mind in high school and of course program and so on and then I showed up
00:17:08 University of Illinois to an AI lab and they said okay I don't have time for you but here's a book AI a modern approach I
00:17:15 think was the first edition at the time mmm here go go learn this and I remember the lay of the land was well it's
00:17:23 incredible that we solve chess but we'll never solve go I mean it was pretty certain that go in the way we thought
00:17:32 about systems that reason was impossible to solve and now we've solved this as a very I think I would have said that it's
00:17:40 unlikely we could take the kind of algorithm that was used for chess and just get it to scale up and work well
00:17:48 for go and at the time what we thought was that in order to solve go we would have to do
00:18:00 something similar to the way humans manage the complexity of go which is to break it down into kind of sub games so
00:18:07 when a human thinks about a go board they think about different parts of the board as sort of weakly connected to
00:18:13 each other and they think about okay within this part of the board here's how things could go and that part about his
00:18:19 how things could go and now you try to sort of couple those two analyses together and deal with the interactions
00:18:26 and maybe revise your views of how things are going to go in each part and then you've got maybe five six seven ten
00:18:34 parts of the board and that actually resembles the real world much more than chess does because in the real world you
00:18:44 know we have work we have home life we have sport you know whatever different kinds of activities you know shopping
00:18:53 these all are connected to each other but they're weakly connected so when I'm typing a paper you know I don't simul
00:19:01 taneous Li have to decide which order I'm gonna get the you know the milk and the butter you know that doesn't affect
00:19:09 the typing but I do need to realize okay better finish this before the shops closed because I don't have anything you
00:19:13 don't have any food at home all right right so there's some weak connection but not in the way that chess works
00:19:21 where everything is tied into a single stream of thought so the thought was that go just sort of go we'd have to
00:19:28 make progress on stuff that would be useful for the real world and in a way alphago is a little bit disappointing
00:19:35 right because the the program designed for alphago was actually not that different from from deep blue or even
00:19:44 from Arthur Samuels checker playing and in fact the so the two things that make alphago work is one one is is
00:19:54 amazing ability ability to evaluate the positions and the other is the meta-reasoning capability which which
00:20:02 allows it to to explore some paths in the tree very deeply and to abandon other paths very quickly so this word
00:20:11 meta-reasoning while technically correct inspires perhaps the the wrong degree of power that alphago has for example the
00:20:21 word reasonings as a powerful word let me ask you sort of so you were part of the symbolic AI world for a while like
00:20:30 whatever the AI was there's a lot of excellent interesting ideas there that unfortunately met a winter and so it do
00:20:41 you think it really emerges well I would say yeah it's not quite as simple as that so the the AI winter so for the
00:20:52 first window that was actually named as and that came about because in the mid 80s there was a really a concerted
00:21:07 attempt to push AI out into the real world using what was called expert system technology and for the most part
00:21:17 that technology was just not ready for primetime they were trying in many cases to do a form of uncertain reasoning judge you
00:21:27 know judgment combinations of evidence diagnosis those kinds of things which was simply invalid and when you try to
00:21:36 apply invalid reasoning methods to real problems you can fudge it for small versions of the problem but when it
00:21:43 starts to get larger the thing just falls apart so many companies found that the stuff just didn't work and they were
00:21:52 spending tons of money on consultants to try to make it work and there were you know other practical
00:21:59 reasons like you know they they were asking the companies to buy incredibly expensive lisp machine workstations
00:22:09 which were literally between fifty and a hundred thousand dollars in you know in 1980s money which was would be like
00:22:16 between a hundred and fifty and three hundred thousand dollars per workstation in current prices so then the bottom
00:22:23 line they weren't seeing a profit from it yeah they in many cases I think there were
00:22:29 some successes there's no doubt about that but people I would say over invested every major company was
00:22:42 and I worry a bit that we might see similar disappointments not because the technology is invalid but it's limited
00:22:55 in its scope and it's almost the the dual of the you know the scope problems that expert systems had so what have you
00:23:03 learned from that hype cycle and what can we do to prevent another winter for example yeah so when I'm giving talks
00:23:11 these days that's one of the warnings that I give to to pot warning slide one is that you know rather than data being
00:23:21 the new oil data is the new snake oil that's a good line and then and then the other is that we might see a kind of
00:23:34 very visible failure in some of the major application areas and I think self-driving cars would be the flagship
00:23:45 and I think when you look at the history so the first self-driving car was on the freeway driving itself changing lanes
00:23:59 overtaking in 1987 and so it's more than 30 years and that kind of looks like where we are today right you know
00:24:07 prototypes on the freeway changing lanes and overtaking now I think significant progress has been made particularly on
00:24:17 the perception side so we worked a lot on autonomous vehicles in the early mid 90s at Berkley you know and we had our
00:24:25 own big demonstrations you know we we put congressmen into yourself driving cars and and had them zooming along the freeway
00:24:36 and the problem was clearly perception at the time the problem that perception yeah so in simulation with perfect
00:24:44 perception you could actually show that you can drive safely for a long time even if the other cars are misbehaving
00:24:51 and and so on but simultaneously we worked on machine vision for detecting cars and tracking pedestrians and so on
00:25:01 and we couldn't get the reliability of detection and tracking up to a high enough particular level particularly in
00:25:10 bad weather conditions nighttime rainfall good enough for demos but perhaps not good enough to cover the
00:25:16 general the general yeah the thing about driving is you know suppose you're a taxi driver you know and you drive every
00:25:22 day eight hours a day for ten years right that's a hundred million seconds of driving you know and any one of those
00:25:29 seconds you can make a fatal mistake so you're talking about eight nines of reliability right now if your vision
00:25:39 system only detects ninety eight point three percent of the vehicles right and that's sort of you know one on a bit
00:25:46 nines and reliability so you have another seven orders of magnitude to go and and this is what people don't
00:25:55 understand they think oh because I had a successful demo I'm pretty much done but you know you're not even within seven
00:26:05 orders of magnitude of being done and that's the difficulty and it's it's not there can I follow a white line that's
00:26:13 not the problem right we follow a white line all the way across the country but it's the it's the weird stuff that
00:26:22 happens it's some of the edge cases yeah the edge case other drivers doing weird things you know so if you talk to Google
00:26:31 right so they had actually very classical architecture where you know you had machine vision which would
00:26:38 detect all the other cars and pedestrians and the white lines and the road signs and then basically that was
00:26:45 fed into a logical database and then you had a classical 1970s rule-based expert system telling you okay if you're in the
00:26:55 middle lane and there's a bicyclist in the right lane who is signaling this then then then don't need to do that
00:27:01 yeah right and what they found was that every day they go out and there'd be another situation that the rules didn't
00:27:07 cover you know so they they come to a traffic circle and there's a little girl riding a bicycle the wrong way around a
00:27:12 traffic circle okay what do you do we don't have a rule oh my god okay stop and then you know they come back and had
00:27:19 more rules and they just found that this was not really converging and and if you think about it right how
00:27:27 how do you deal with an unexpected situation meaning one that you've never previously encountered and the sort of
00:27:36 the the reasoning required to figure out the solution for that situation has never been done it doesn't match any
00:27:42 previous situation in terms of the kind of reasoning you have to do well you know in chess programs this happens all
00:27:47 the time you're constantly coming up with situations you haven't seen before and
00:27:54 you have to reason about them you have to think about okay here are the possible things I could do here the
00:28:00 outcomes here's how desirable the outcomes are and then pick the right one you know in the 90s we were saying okay
00:28:05 this is how you're gonna have to do automated vehicles they're gonna have to have a look ahead capability but the
00:28:12 look ahead for driving is more difficult than it is for chess because Huysmans the other right there's humans and
00:28:19 they're less predictable than just a standard well then will you have an opponent in chess who's also somewhat
00:28:28 unpredictable but for example in chess you always know the opponent's intention they're trying to beat you right whereas
00:28:34 in driving you don't know is this guy trying to turn left or has he just forgotten to turn off his tone signal or
00:28:42 is he drunk or is he you know changing the channel on his radio or whatever it might be you got to try and figure out
00:28:49 the mental state the intent of the other drivers to forecast the possible evolutions of their trajectories and
00:28:56 then you've got to figure out okay which is the directory for me that's going to be safest and those all interact with
00:29:03 each other because the other drivers going to react to your trajectory and so on so you know they've got the classic
00:29:10 merging onto the freeway a problem where you're kind of racing a vehicle that's already on the freeway and you are you
00:29:16 gonna pull ahead of them or you're gonna let them go first and pull in behind and you get this sort of uncertainty about
00:29:21 who's going first so all those kinds of things mean that you need decision-making
00:29:33 architecture that's very different from either a rule-based system or it seems to me a kind of an end-to-end neural
00:29:40 network system you know so just as alphago is pretty good when it doesn't do any look ahead but it's way way way
00:29:49 way better when it does I think the same is going to be true for driving you can have a driving system that's pretty good
00:29:55 when it doesn't do any look ahead but that's not good enough you know and we've already seen multiple deaths
00:30:05 caused by poorly designed machine learning algorithms that don't really understand what they're doing yeah and
00:30:12 on several levels I think it's on the perception side there's mistakes being made by those algorithms were the
00:30:19 perception is very shallow on the planning side to look ahead like you said and the thing that we come come up
00:30:29 against that's really interesting when you try to deploy systems in the real world is you can't think of an
00:30:35 artificial intelligence system as a thing that responds to the world always you have to realize that it's an agent
00:30:42 that others will respond to as well so in order to drive successfully you can't just try to do obstacle avoidance you
00:30:49 can't pretend that you're invisible thank you right you're the invisible car right just look that way I mean but you
00:30:56 have to assert yet others have to be scared of you just we're all there's this tension there's this game so if we
00:31:03 studied a lot of work with pedestrians if you approach pedestrians as purely an obstacle avoidance so you either doing
00:31:10 look ahead isn't modeling the intent that you're you they're not going to they're going to take advantage of you
00:31:16 they're not going to respect you at all there has to be a tension a fear some amount of uncertainty that's how we have
00:31:25 create we or at least just a kind of a resoluteness right so you have you have to display a certain amount of
00:31:31 resoluteness you can't you can't be too tentative and yeah so the right the the solutions then become pretty complicated right you
00:31:43 get into game theoretic yes analyses and so we're you know Berkeley now we're working a lot on this kind of
00:31:51 interaction between machines and humans and that's exciting yeah and so my colleague and could drag an actually you
00:32:03 know if you if you formulate the problem game theoretically and you just let the system figure out the solution you know
00:32:09 it does interesting unexpected things like sometimes at a stop sign if no one is going first right the car
00:32:18 will actually back up a little all right and just to indicate to the other cars that they should go and that's something
00:32:25 it invented entirely by itself that's interesting you know we didn't say this is the language of communication at stop
00:32:32 signs it figured it out that's really interesting so let me one just step back for a second just this beautiful philosophical
00:32:43 notion so Pamela I'm a quartic in 1979 wrote AI began with the ancient wish to forge the gods so when you think about
00:32:53 the history of our civilization do you think that there is an inherent desire to create let's not say gods but to
00:33:03 create super intelligence is it inherent to us is it in our genes that the natural arc of human civilization is to
00:33:13 create things that are of greater and greater power and perhaps no echoes of ourselves so to create the gods as
00:33:28 if the maybe I mean you know we're all certainly we see over and over again in history individuals who thought about
00:33:40 this possibility hopefully when I'm not being too philosophical here but if you look at the arc of this you know where
00:33:48 this is going and we'll talk about AI safety we'll talk about greater and greater intelligence do you see that
00:33:55 there in when you created the earth Allah program and you felt this excitement what was that excitement was it
00:34:02 excitement of a tinkerer who created something cool like a clock or was there a magic or was it more like a child
00:34:11 being born that yeah you know yeah so I mean I certainly understand that viewpoint and if you look at the light
00:34:21 he'll report which was commit so in the 70s there was a lot of controversy in the UK about AI and you know whether it
00:34:28 was for real and how much the money there was a lot long story but the government commissioned a report by
00:34:42 by light Hill who was a physicist and he wrote a very damning report about AI which I think was the point and he said
00:34:53 that that these are you know frustrated men who unable to have children would like to create and you know create life
00:35:03 you know as a kind of replacement you know which I which I think is really but there is I mean there there is a
00:35:20 kind of magic I would say you when you and what you're building in is really just you're building in some
00:35:30 understanding of the principles of learning and decision-making and to see those principles actually then turn into
00:35:43 intelligent behavior in in specific you know that is naturally going to make you think okay where does this end and
00:36:03 so there's a there's magical optimistic views of word and whatever your view of optimism is whatever your view of utopia
00:36:11 is it's probably different for everybody yeah but you've often talked about concerns you have of how things might go
00:36:25 wrong so I've talked to max tegmark there's a lot of interesting ways to think about AI safety you're one of the
00:36:33 seminal people thinking about this problem among sort of being in the weeds of actually solving specific AI problems
00:36:40 you also think about the big picture of where we're going so can you talk about several elements of it let's just talk
00:36:48 about maybe the control problem so this idea of losing ability to control the behavior and of a AI system so how do
00:36:59 you see that how do you see that coming about what do you think we can do to manage it well so it doesn't take a
00:37:10 genius to realize that if you make something that's smarter than you you might have a problem you know in Turing
00:37:19 Alan Turing you know wrote about the gave lectures about this you know 19 1951 painted a lecture on the radio and
00:37:30 he basically says you know once the machine thinking method stops you know very quickly they'll outstrip humanity
00:37:41 and you know if we're lucky we might be able to I think he says if we may be able to turn off the power at strategic
00:37:49 moments but even so a species would be humbled yeah you can actually I think was wrong about that right here is you
00:37:55 you know if it's a sufficiently intelligent machine is not gonna let you switch it off so it's actually in
00:38:00 competition with you so what do you think is meant just for a quick tangent if we shut off this super intelligent
00:38:15 I think he means that we would realize that we are inferior right that we we only survive by the skin of our teeth
00:38:22 because we happen to get to the off switch just in time you know and if we hadn't then we would
00:38:30 have lost control over the earth so do you are you more worried when you think about this stuff about super
00:38:37 intelligent AI or are you more worried about super powerful AI that's not aligned with our values so the paperclip
00:38:49 scenario is kind of I think so the main problem I'm working on is is the control problem the the problem of machines
00:38:59 pursuing objectives that are as you say not aligned with human objectives and and this has been it has been the way
00:39:07 we've thought about I eyes since the beginning you you build a machine for optimizing and then you put in some
00:39:17 objective and it optimizes right and and you know we we can think of this as the the King Midas problem right because if
00:39:27 you know so King Midas put in this objective right everything I touch you turned to gold and the gods you know
00:39:33 that's like the machine they said okay done you know you now have this power and of course his food and his drink and
00:39:40 his family all turned to gold and then he's sighs misery and starvation and this is you know it's it's a warning
00:39:50 it's it's a failure mode that pretty much every culture in history has had some story along the same lines you know
00:39:57 there's the the genie that gives you three wishes and you know third wish is always you know please undo the first
00:40:04 two wishes because I messed up and you know and when author Samuel wrote his chest his checkup laying
00:40:12 program which learned to play checkers considerably better than Martha Samuel could play and actually reached a pretty
00:40:22 Norbert Wiener who was a one of the major mathematicians of the 20th century sort of a father of modern automation
00:40:29 control systems you know he saw this and he basically extrapolated you know as Turing did and
00:40:37 said okay this is how we could lose control and specifically that we have to be certain that the purpose we put into
00:40:50 the machine as the purpose which we really desire and the problem is we can't do that right you mean we're not
00:41:01 it's a very difficult to encode so to put our values on paper is really difficult or you're just saying it's
00:41:10 impossible your line is writing this so it's it theoretically it's possible but in practice it's extremely unlikely that
00:41:20 we could specify correctly in advance the full range of concerns of humanity that you talked about cultural
00:41:28 transmission of values I think is how humans to human transmission of values happens right what we learned yeah I
00:41:36 mean as we grow up we learn about the values that matter how things how things should go what is reasonable to pursue
00:41:44 and what isn't reasonable to pursue machines can learn in the same kind of way yeah so I think that what we need to
00:41:51 do is to get away from this idea that you build an optimizing machine and you put the objective into it
00:42:01 because if it's possible that you might put in a wrong objective and we already know this is possible because it's
00:42:06 happened lots of times alright that means that the machine should never take an objective that's given as gospel truth
00:42:17 because once it takes them the the objective is gospel truth alright then it's the leaves that whatever actions
00:42:26 it's taking in pursuit of that objective are the correct things to do so you could be jumping up and down and saying
00:42:31 no you know no no no you're gonna destroy the world but the machine knows what the true objective is and it's
00:42:37 pursuing it and tough luck to you you know and this is not restricted to AI right this is you know I think many of
00:42:46 the 20th century technologies right so in statistics you you minimize a loss function the loss function is
00:42:52 exogenously specified in control theory you minimize a cost function in operations research you maximize a
00:42:59 reward function and so on so in all these disciplines this is how we conceive of the problem and it's the
00:43:08 wrong problem because we cannot specify with certainty the correct objective right we need uncertainty we the machine
00:43:17 to be uncertain about a subjective what it is that it's post it's my favorite idea of yours I've heard you say
00:43:24 somewhere well I shouldn't pick favorites but it just sounds beautiful we need to teach machines humility yeah
00:43:32 I mean it's a beautiful way to put it I love it that they humble oh yeah they know that
00:43:40 they don't know what it is they're supposed to be doing and that those those objectives I mean they exist they
00:43:48 are within us but we may not be able to explicate them we may not even know you know how we want our future to go so
00:43:59 exactly and the Machine you know a machine that's uncertain he's going to be deferential to us so if we say don't
00:44:09 do that well now the machines learn something a bit more about our true objectives because something that it
00:44:16 thought was reasonable in pursuit of our objectives turns out not to be so now it's learn something so it's going to
00:44:22 defer because it wants to be doing what we really want and you know that that point I think is
00:44:31 absolutely central to solving the control problem and it's a different kind of AI when you when you take away
00:44:39 this idea that the objective is known then in fact a lot of the theoretical frameworks that we're so familiar with
00:44:51 you know Markov decision processes goal based planning you know standard games research all of these techniques
00:45:01 actually become inapplicable and you get a more complicated problem because because now the interaction with the
00:45:13 human becomes part of the problem because the human by making choices is giving you more information about the
00:45:23 'true objective and that information helps you achieve the objective better and so that really means that you're
00:45:30 mostly dealing with game theoretic problems where you've got the machine and the human and they're coupled
00:45:37 together rather than a machine going off by itself with a fixed objective which is fascinating on the machine and the
00:45:46 human level that we when you don't have an objective means you're together coming up with an objective I mean
00:45:53 there's a lot of philosophy that you know you could argue that life doesn't really have meaning we we together agree
00:46:00 on what gives it meaning and we kind of culturally create things that give why the heck we are in this earth anyway we
00:46:08 together as a society create that meaning and you have to learn that objective and one of the biggest I
00:46:14 thought that's what you were gonna go for a second one of the biggest troubles we've run
00:46:19 into outside of statistics and machine learning and AI and just human civilization is when you look at I came
00:46:27 from the south was born in the Soviet Union and the history of the 20th century we ran into the most trouble us
00:46:37 humans when there was a certainty about the objective and you do whatever it takes to achieve that objective whether
00:46:42 you talking about in Germany or communist Russia oh yeah I get the trouble I would say with you know
00:46:50 corporations in fact some people argue that you know we don't have to look forward to a time when AI systems take
00:46:57 over the world they already have and they call corporations right that corporations happen to be using people
00:47:06 as components right now but they are effectively algorithmic machines and they're optimizing an objective which is
00:47:15 quarterly profit that isn't aligned with overall well-being of the human race and they are destroying the world they are
00:47:22 primarily responsible for our inability to tackle climate change right so I think that's one way
00:47:29 of thinking about what's going on with with cooperations but I think the point you're making you is valid that there
00:47:38 are there are many systems in the real world where we've sort of prematurely fixed on the objective and then
00:47:48 decoupled the the machine from those that's supposed to be serving and I think you see this with government right
00:47:56 government is supposed to be a machine that serves people but instead it tends to be taken over by people who have
00:48:05 their own objective and use government to optimize that objective regardless of what people want do you have do you find
00:48:13 appealing the idea of almost arguing machines where you have multiple I systems with a clear fixed objective we
00:48:20 have in government the red team and the blue team that are very fixed on their objectives and they argue and it kind of
00:48:27 maybe it would disagree but it kind of seems to make it work somewhat that the the duality of it okay let's go a
00:48:39 hundred years back when there was still was going on or at the founding of this country there was disagreement and that
00:48:47 disagreement is where so there's a balance between certainty and forced humility because the power was
00:48:56 distributed yeah I think that the the the nature of debate and disagreement argument takes as a premise the idea
00:49:07 that you could be wrong right which means that you're not necessarily absolutely convinced that your objective
00:49:17 is the correct one right if you were absolutely Guiness there'll be no point in having any discussion or argument
00:49:22 because you would never change your mind and there wouldn't be any any sort of synthesis or or anything like that so so
00:49:30 I think you can think of argumentation as a as an implementation of a form of uncertain reasoning
00:49:42 and you know I I've been reading recently about utilitarianism in the history of efforts to define in a sort
00:49:56 I feel like a formula for moral or political decision-making and it's really interesting that the parallels
00:50:03 between the philosophical discussions going back 200 years and what you see now in discussions about existential
00:50:13 risk because you it's almost exactly the same so someone would say okay well here's a formula for how we should make
00:50:19 decisions right so utilitarianism you know each person has a utility function and then we make decisions to
00:50:27 maximize the sum of everybody's utility mm-hmm right and then people point out well you know in that case the best
00:50:37 policy is one that leads to the enormous lis vast population all of whom are living a life that's barely worth living
00:50:45 right and this is called the repugnant conclusion and you know another version is you know that we we should maximize
00:50:52 pleasure and that's what we mean by utility and then you'll get people effectively saying well in that case you
00:50:59 know we might as well just have everyone hooked up to a heroin drip yeah you know and they didn't use those words but that
00:51:06 debate you know what's happening in the 19th century as it is now about AI that if we get the formula wrong you know
00:51:16 we're going to have AI systems working towards an outcome that in retrospect would be exactly wrong do you think
00:51:24 there's it has beautifully put so the the echoes are there but do you think I mean if you look at sam Harris is our
00:51:33 imagination worries about the AI version of that because of the speed at which the things going wrong in the
00:51:45 utilitarian context could happen yeah is that is that a worry for you yeah I I think that
00:51:52 you know it in most cases not in all but you know if we if we have a wrong political idea you know we see it
00:51:58 starting to go wrong and we're you know we're not completely stupid and so we said okay that was maybe that was a mistake
00:52:08 let's try something different and and also we're very slow and inefficient about implementing these things and so
00:52:14 on so you have to worry when you have corporations or political systems that are extremely efficient
00:52:22 but when we look at AI systems or even just computers in general right they have this different characteristic from
00:52:31 ordinary human activity in the past so let's say you were a surgeon you had some idea about how to do some operation
00:52:39 right well and let's say you were wrong all right that that way of doing the operation would mostly kill the patient
00:52:46 well you'd find out pretty quickly like after three maybe three or four tries right but
00:52:56 that isn't true for pharmaceutical companies because they don't do three or four operations they they manufacture
00:53:04 three or four billion pills and they sell them and then they find out maybe six months or a year later that oh
00:53:10 people are dying of heart attacks or getting cancer from this drug and so that's why we have the FDA right because
00:53:18 of the scalability of pharmaceutical production and you know and there have been some unbelievably bad episodes in
00:53:30 the history of pharmaceuticals and and adulteration of of products and so on that that have killed tens of thousands
00:53:37 or paralysed hundreds of thousands of people now with computers we have that same scalability problem that you can
00:53:47 sit there and type for I equals 1 to 5 billion do right and all of a sudden you're having an impact on a global
00:53:54 scale and yet we have no FDA right there's absolutely no controls at all it's over what a bunch of undergraduates
00:54:02 with too much caffeine can do to the world and you know we look at what happened with Facebook well social media
00:54:10 in general and click-through optimization so you have a simple feedback algorithm that's trying to just
00:54:20 optimize click-through that sounds reasonable right because you don't want to be feeding people ads that they don't
00:54:28 care about I'm not interested in and you might even think of that process as simply adjusting the the feeding of
00:54:40 ads or news articles or whatever it might be to match people's preferences right which sounds like a good idea but
00:54:50 in fact that isn't how the algorithm works right you make more money the algorithm makes more money if it could
00:55:00 better predict what people are going to click on because then it can feed them exactly that right so the way to
00:55:08 maximize click-through is actually to modify the people to make them more predictable and one way to do that is to
00:55:18 feed them information which will change their behavior and preferences towards extremes that make them predictable now
00:55:26 whatever is the nearest extreme or the nearest predictable point that's where you're going to end up
00:55:34 the machines will force you there now and then I think there's a reasonable argument to say that this among other
00:55:41 things is contributing to the and where was the oversight of this process where were the people saying
00:55:54 okay you would like to apply this algorithm to five billion people on the face of the earth can you show me that
00:56:01 it's safe can you show me that it won't have various kinds of negative effects no there was no one asking that question
00:56:10 there was no one placed between you know the undergrads were too much caffeine and the human race well it's just they
00:56:19 just did it and but some way outside the scope of my knowledge so economists would argue that the what is it the
00:56:26 invisible hand so the the capitalist system it was the oversight so if you're going to corrupt society with whatever
00:56:32 decision you make is a company then that's going to be reflected in people not using your product sort of one
00:56:39 that's one model of oversight so we shall see but you know in the meantime you know that but you you might even
00:56:48 have broken the political system that enables capitalism to function well you've changed it and so we should see
00:56:56 yeah change changes often painful so my question is  absolutely it's fascinating you're absolutely right that there is
00:57:04 ZERO oversight on algorithms that can have a profound civilization changing effect so do you think it's possible I
00:57:15 mean I haven't have you seen government so do you think it's possible to create regulatory bodies oversight over AI
00:57:24 algorithms which are inherently such cutting edge set of ideas and technologies yeah but I think it takes time
00:57:36 to figure out what kind of oversight what kinds of controls I mean took time to design the FDA regime you know and
00:57:42 some people still don't like it and they want to fix it and I think there are clear ways that it
00:57:50 could be improved but the whole notion that you have stage 1 stage 2 stage 3 and here are the criteria for what you
00:57:58 have to do to pass a stage 1 trial right we haven't even thought about what those would be
00:58:03 for algorithms so I mean I think there are there are things we could do right now with regard to bias for example we
00:58:13 we have a pretty good technical handle on how to detect algorithms that are propagating bias that exists in data
00:58:25 sets how to D by us those algorithms and and even what it's going to cost you to do that so I think we could start having
00:58:33 some standards on that I think there are there are things to do with impersonation of falsification that we
00:58:43 could we could work on so I thanks ya or you know in a very simple point so impersonation ISM is a machine acting as
00:58:53 if it was a person I can't see a real justification for why we shouldn't insist that machines self-identify as
00:59:03 machines you know where is the social benefit in in fooling people into thinking that this is really a person
00:59:11 when it isn't you know I I don't mind if it uses a human-like voice that's easy to understand that's fine
00:59:17 but it should just say I'm a machine in people are speaking to that I would think relatively obvious factors I think
00:59:27 mostly yeah I mean there is actually a law in California that bans impersonation but only in certain
00:59:36 restricted circumstances so for the purpose of engaging in a for Geling transaction and for the purpose of
00:59:46 modifying someone's voting behavior so those are those are the circumstances where machines have to self-identify but
00:59:54 I think this is you know arguably it should be in all circumstances and then when you talk about deep fakes you know
01:00:03 we're just beginning but already it's possible to make a movie of anybody saying anything in ways that are pretty
01:00:12 hard to detect including yourself because you're on camera now and your voice is coming through with high
01:00:17 resolution so you could take what I'm saying and replaces it with it pretty much anything else you wanted me to be
01:00:22 saying yeah and even it will change my and there's actually not much in the way of real legal protection against that I
01:00:37 think in the commercial area you could say yeah that's you're using my brand and so on that there there are rules
01:00:45 about that but in the political sphere I think it's at the moment it's you know anything goes so like that could be
01:00:55 really really damaging and let me just try to make not an argument but try to look back at history and say something
01:01:06 dark in essence is while regulation seems to be oversight seems to be exactly the right thing to do here
01:01:12 it seems that human beings what they naturally do is they wait for something to go wrong if you're talking about
01:01:17 nuclear weapons you can't talk about nuclear weapons being dangerous until somebody actually
01:01:25 like the United States drops the bomb or Chernobyl melting do you think we will have to wait for things going wrong in a
01:01:36 way that's obviously damaging to society not an existential risk but obviously damaging or do you have faith that I I
01:01:47 hope not but I mean I think we do have to look at history and when you know so the two examples you gave nuclear
01:01:56 weapons and nuclear power are very very interesting because you know in nuclear weapons we knew in the early years of
01:02:07 the 20th century that atoms contained a huge amount of energy right we had e equals mc-squared we knew the the mass
01:02:13 differences between the different atoms you might be able to make an incredibly powerful explosive so HG Wells wrote
01:02:26 science fiction book I think in 1912 Frederick Soddy who was the guy who discovered isotopes so Nobel Prize
01:02:34 winner he gave a speech in 1915 saying that this new explosive would be the equivalent of 150 tons of dynamite which
01:02:46 turns out to be about right and you know Kenton this was in World War one right so he was imagining how much worse the
01:02:55 world would be if we were using that kind of explosive but the physics establishment simply refused to believe
01:03:03 that these things could be made including the people who are making it well so they were doing the nuclear
01:03:10 physics I mean eventually were the ones who made it and Rockwell for me or whoever well so up to the the
01:03:20 development was was mostly theoretical so it was people using sort of primitive kinds of particle acceleration and doing
01:03:28 experiments at the at the level of single particles or collections of yet thinking about how to actually make
01:03:38 a bomb or anything like that they but they knew the energy was there and they figured if they understood it better it
01:03:43 might be possible but the physics establishment their view and I think because they did not want it to be true
01:03:51 their view was that it could not be true that this could not provide a way to make a super weapon and you know there
01:04:02 was this famous speech given by Rutherford who was the sort of leader of nuclear physics and I was on September
01:04:12 11th 1933 and he he said you know anyone who talks about the possibility of obtaining energy from transformation of
01:04:20 atoms is talking complete moonshine and the next the next morning Leo Szilard read about that speech and then invented
01:04:29 the nuclear chain reaction and so as soon as he invented he soon as he had that idea that you could make a chain
01:04:37 reaction with neutrons because neutrons were not repelled by the nucleus so they could enter the nucleus and then
01:04:44 continue the reaction as soon as he has that idea he instantly realized that the world was in deep doo-doo because this
01:04:55 is 1933 right you know Hitler had recently come to power in Germany Zil odd was in London and eventually
01:05:05 became a refugee and and came to the US and the in the process of having the idea about the chain reaction he figured
01:05:15 out basically how to make a bomb and also how to make a reactor and he patented the reactor
01:05:25 2:34 but because of the situation the great power conflict situation that he could see happening he kept that a
01:05:36 secret and so between then and the beginning of World War two people were working including the Germans on how to
01:05:48 actually create Neutron sources right what specific fission reactions would produce neutrons of the right energy to
01:05:58 continue the reaction and and that was demonstrated in Germany I think in 1938 if I remember correctly the first
01:06:08 nuclear weapon patent was 1939 by the French so this was actually you know this was actually
01:06:16 going on you know well before World War two really got going and then you know the British probably had the most
01:06:24 advanced capability in this area but for safety reasons among others and blush which is sort of just resources they
01:06:31 moved the program from Britain to the US and then that became Manhattan Project so the the the reason why we couldn't
01:06:43 have any kind of oversight of nuclear weapons and nuclear technology was because we were basically already in an
01:06:54 arms race in a war and but you you've mentioned then in the 20s and 30s so what are the echoes yeah the way you've
01:07:02 described this story I mean there's clearly echoes why do you think most a I researchers folks who are really close to the metal
01:07:11 they really are not concerned about it and they don't think about it whether they don't want to think about
01:07:17 it it's but what are the yeah why do you think that is what are the echoes of the nuclear situation to the current
01:07:27 situation and what can we do about it I think there is a you know a kinda modak motivated cognition which is a term in
01:07:37 psychology means that you believe what you would like to be true rather than what is true and you know it's it's
01:07:47 unsettling to think that what you're working on might be the end of the human race obviously so you would rather
01:07:55 instantly deny it and come up with some reason why it couldn't be true and the you know I have
01:08:04 I collected a long list of reasons that extremely intelligent competent AI scientists have come up with for why we
01:08:13 shouldn't worry about this you know for example calculators are super human at arithmetic and they haven't taken over
01:08:19 the world so there's nothing to worry about well okay my five-year-old you know could have figured out why that was
01:08:28 an unreasonable and and really quite weak argument you know another one was you know you while it's theoretically
01:08:40 possible that you could have superhuman AI destroy the world you know it's also theoretically possible that a black hole
01:08:47 could materialize right next to the earth and destroy humanity I mean yes it's theoretically possible quantum
01:08:55 theoretically extremely unlikely that it would just materialize right there but that's a completely bogus analogy
01:09:03 because you know if the whole physics community on earth was working to materialize a black hole in near Earth
01:09:10 orbit right wouldn't you ask them is that a good idea is that gonna be safe you know what if you succeed all right
01:09:18 right and that's the thing right the AI is sort of refused to ask itself what if you succeed and initially I think that
01:09:27 was because it was too hard but you know Alan Turing asked himself that and he said we'd be toast right if we were
01:09:36 lucky we might be able to switch off the power but probably we'd be toast but there's also an aspect that because
01:09:45 we're not exactly sure what the future holds it's not clear exactly so technically what to worry about sort of
01:09:56 how things go wrong and so there is something it feels like maybe you can correct me if I'm wrong but there's
01:10:03 something paralyzing about worrying about something that logically is inevitable but you don't really know
01:10:11 what that will look like yeah I think that's that's it's a reasonable point and you know the you know it's certainly
01:10:19 in terms of existential risks it's different from you know asteroid collides with the earth right right
01:10:26 which again is quite possible you know it's happened in the past it'll probably happen again we don't right we don't
01:10:33 know right now but if we did detect an asteroid that was going to hit the earth in 75 years time we'd certainly be doing
01:10:40 something about it well it's clear there's got big rocks we'll probably have a meeting you see what do we do
01:10:45 about the big rock will they I write with a I I mean the very few people who think it's not gonna
01:10:52 happen within the next 75 years I know rod Brooks doesn't think it's gonna happen maybe and ruing doesn't
01:10:58 think it's happened but you know a lot of the people who work day-to-day you know as you say at the rock face
01:11:06 they think it's gonna happen I think the median estimate from AI researchers is somewhere in forty to fifty years from
01:11:14 from now or maybe a little you know I think in Asia they think it's gonna be even faster than that I am I'm a little
01:11:23 bit more conservative I think probably take longer than that but I think it's you know as happened with nuclear weapons
01:11:30 well I went overnight it can happen overnight that you have these breakthroughs and we need more than one
01:11:36 breakthrough but you know the it's on the order of half a dozen this is a very rough scale but so half a dozen
01:11:46 breakthroughs of that nature it would have to happen for us to reach the superhuman AI but the you know the AI
01:11:54 research community is vast now the massive investments from governments from corporations tons of really really
01:12:02 smart people you know you just have to look at the rate of progress in different areas of AI to see that things
01:12:09 are moving pretty fast so to say oh it's just gonna be thousands of years I don't see any basis for that you know I see
01:12:20 you know for example the the Stanford hundred year AI project right which is supposed to be sort of you know the
01:12:30 serious establishment view their most recent report actually said it's probably not even possible
01:12:37 Wow right which if you want a perfect example of people in denial that's it because you know for the whole
01:12:47 history of AI we've been saying to philosophers who said it wasn't possible well you have no idea what you're
01:12:51 talking about of course it's possible right give me an give me an argument for why it couldn't happen and there isn't
01:12:59 one all right and now because people are worried that maybe a oh it might get a bad name or or I just don't want to
01:13:06 think about this they're saying okay well of course it's not really possible you know and we imagine right imagine if
01:13:12 you know the the leaders of the cancer biology community got up and said well you know of course curing cancer it's
01:13:22 not really possible complete outrage and dismay and you know I I find this really a strange phenomenon so okay so if you
01:13:35 accept it as possible and if you accept that it's probably going to happen
01:13:43 the point that you're making that you know how does it go wrong a valid question without that without an answer
01:13:51 to that question then you stuck with what I call the gorilla problem which is you know the problem that the gorillas
01:13:57 face right they made something more intelligent than them namely us a few million years ago and now now they're in
01:14:05 deep doo-doo yeah so there's really nothing they can do they've lost the control theater they failed to solve the
01:14:11 control problem of controlling humans and so they've lost so we don't want to be in that situation and if the gorillas
01:14:20 problem is is the only formulation you have there's not a lot you can do right other than to say okay we should try to
01:14:27 stop you know we should just not make the humans or right in this case not make the AI and I think that's really
01:14:34 hard to do - I'm not actually proposing that that's a feasible course of action I also think
01:14:44 that you know if properly control a I could be incredibly beneficial so the but it seems to me that there's a
01:14:54 there's a consensus that one of the major failure modes is this loss of control that we create AI systems that
01:15:03 are pursuing incorrect objectives and because the AI system believes it knows what the objective is it has no
01:15:12 incentive to listen to us anymore so to speak right it it's just carrying out the the strategy that it it has
01:15:23 computed as being the optimal solution and you know it may be that in the process it needs to acquire more
01:15:32 resources to increase the possibility of success or prevent various failure modes by defending itself against interference
01:15:41 and so that collection of problems I think is something we can address yes that the other problems are roughly
01:15:54 speaking you know misuse right so even if we solve the control problem we make perfectly safe controllable AI systems
01:16:01 well why you know why does dr. evil going to use those right he wants to just take over the world and he'll make
01:16:06 unsafe AI system said but then get out of control so that's one problem which is sort of a you know a partly a
01:16:13 policing problem partly a-- a sort of a cultural problem for the profession of how we teach
01:16:23 people what kinds of AI systems are safe you talk about autonomous weapon system and how pretty much everybody agrees
01:16:29 there's too many ways that that can go horribly wrong if this great slaughter BOTS movie that kind of illustrates that
01:16:36 beautifully I want to talk that's another there's another topic I I'm happy talking about the I just want to
01:16:42 mention that what I see is the third major failure mode which is overuse not so much misuse but overuse of AI
01:16:52 that we become overly dependent so I call this the wooly problems if you seen wall-e the movie all right all the
01:16:59 humans are on the spaceship and the machines look after everything for them and they just watch TV and drink big
01:17:07 gulps and they're all sort of obese and stupid and they sort of totally lost any you know so a in effect right this would
01:17:21 happen like the slow boiling frog right we would gradually turn over more and more of the management of our
01:17:27 civilization to machines as we are already doing in this you know this if this process continues you know we sort
01:17:36 of gradually switch from sort of being the Masters of Technology to just being the guests right so so we become guests
01:17:44 on a cruise ship you know which is fine for a week but not not further the rest of eternity right you know and it's
01:17:54 almost irreversible right once you once you lose the incentive to for example you know learn to be an engineer or a
01:18:03 doctor or a sanitation operative or or any other of the the infinitely many ways that we maintain and propagate our civilization
01:18:13 you know if you if you don't have the incentive to do any of that you won't and then it's really hard to recover and
01:18:20 of course there's just one of the technologies that could that third failure mode result in that there's
01:18:26 probably other technology in general detaches us from it does a bit but the the the difference is that in terms of
01:18:35 the knowledge to to run our civilization you know up to now we've had no alternative but to put it into people's
01:18:42 heads right and if you oh it's not we're with Google I mean so software in general so I probably if computers in
01:18:49 general but but the you know the knowledge of how you know how a sanitation system works you know that's
01:18:56 an the AI has to understand that it's no good putting it into Google so I mean we we've always put knowledge in on paper
01:19:03 but paper doesn't run our civilization it only runs when it goes from the paper into people's heads again right so we've
01:19:10 always propagated civilization through human minds and we've spent about a trillion person years doing that
01:19:18 literature right you you can work it out yeah but right is about just over a hundred billion people who've ever lived
01:19:25 and each of them has spent about ten years learning stuff and to keep their civilization going and so that's a
01:19:31 trillion person years we put into this effort beautiful way to describe all of civilization and now we're you know
01:19:37 we're danger of throwing that away so this is a problem that AI console it's not a technical problem it's a you know
01:19:45 if we do our job right the AI systems will say you know the human race doesn't in the long run want to be passengers in
01:19:53 a cruise ship the human race wants autonomy this is part of human preferences so we the AI systems are not
01:20:01 going to do this stuff for you you've got to do it for yourself right I'm not going to carry you to the top of Everest
01:20:08 in an autonomous helicopter you have to climb it if you want to get the benefit and so on so
01:20:15 but I'm afraid that because we are short-sighted and lazy we're gonna override the AI systems and and there's
01:20:24 an amazing short story that I recommend to everyone that I talk to about this called the machine stops
01:20:34 written in 1909 by Ian Foster who you know wrote novels about the British Empire and sort of things that became
01:20:40 costume dramas on the BBC but he wrote this one science fiction story which is an amazing vision of the future it has
01:20:51 it has basically iPads it has video conferencing it has MOOCs it has computer and computer induced
01:20:59 obesity I mean literally the whole thing it's what people spend their time doing is giving online courses or listening to
01:21:05 online courses and talking about ideas but they never get out there in the real world that they don't really have a lot
01:21:12 of face-to-face contact everything is done online you know so all the things we're
01:21:18 worrying about now were described in this story and and then the human race becomes more and more dependent on the
01:21:25 Machine loses knowledge of how things really run and then becomes vulnerable to collapse and so it's a it's a pretty
01:21:36 unbelievably amazing story for someone writing in 1909 to imagine all this loss yeah so there's very few people that
01:21:44 represent artificial intelligence more right you're often brought up as the person well Stuart Russell like the AI
01:22:03 person is worried about this that's why you should be worried about it do you feel the burden of that I don't know if
01:22:11 you feel that at all but when I talk to people like from you talk about set people outside of computer science when
01:22:19 they think about this still Russell is worried about AI safety you should be worried too do you feel the burden of
01:22:26 that I mean in a practical sense yeah because I'd yet you know a dozen sometimes 25 invitations a day
01:22:37 to talk about it to give interviews to write press articles and so on so in that very practical sense I'm seeing
01:22:46 that people are concerned and really interested about this are you worried that you could be wrong as all good
01:22:53 scientists are of course I worry about that all the time I mean that's that's always been the way that I I've worked
01:23:01 you know is like I have an argument in my head with myself right so I have some idea and then I think okay how could
01:23:10 that be wrong or did someone else already have that idea so I'll go and you know search and as much literature
01:23:16 as I can't to see whether someone else already thought of that or or even refuted it so you know I right now I'm
01:23:26 I'm reading a lot of philosophy because you know in in the form of the debate so V over utilitarianism and other kinds of
01:23:40 moral moral formulas shall we say people have already thought through some of these issues but you know what one of
01:23:47 the things I'm I'm not seeing in a lot of these debates is this specific idea about the importance of uncertainty in
01:23:58 the objective that this is the way we should think about machines that are beneficial to humans so this idea of
01:24:06 provably beneficial machines based on explicit uncertainty in the objective you know it seems to be you know my gut
01:24:17 feeling is this is the core of it it's gonna have to be elaborated in a lot of different directions and there are a lot
01:24:24 of lis beneficial yeah but they're I mean it has to be right we can't afford you know hand-wavy beneficial yeah
01:24:31 because there are you know whenever we do hand wavy stuff there are loopholes and the thing about super intelligent
01:24:38 machines is they find the loopholes you know just like you know tax evaders if you don't write your tax law properly
01:24:46 that people will find loopholes and end up paying no taxes and and so you should think of it this way
01:24:55 and in getting those definitions right you know it is really a long process you know so you can you can define
01:25:05 mathematical frameworks and within that framework you can prove mathematical theorems that yes this will you know
01:25:11 this this theoretical entity will be proven beneficial to that theoretical entity but that framework may not match
01:25:19 the real world in some crucial way so long process thinking through it of iterating and so on the last question
01:25:26 yep you have ten seconds to answer it what is your favorite sci-fi movie about AI I would say interstellar has my
01:25:36 favorite robots or beat it Space Odyssey yeah yeah yeah so so tars the robots one of the robots in interstellar is the way
01:25:45 a robot should behave and I would say ex machina is in some ways the one like the one that makes you
