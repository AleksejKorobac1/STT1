00:00:01 the following is a conversation with William McCaskill he's a philosopher ethicists and one of the originators of
00:00:09 the effective altruism movement his research focuses on the fundamentals of effective altruism are the use of
00:00:16 evidence and reason to help others but as much as possible with our time and money with a particular concentration on
00:00:22 how to act given moral uncertainty he's the author of doing good
00:00:28 better effective altruism and a radical new way to make a difference he is a co-founder and the president of the
00:00:37 center of effective altruism CEA that encourages people to commit to donate at least 10 percent of their income to the
00:00:44 most effective charities he co-founded 80,000 dollars which is a nonprofit that provides research and advice on how you
00:00:51 can best make a difference through your career this conversation was recorded before the outbreak of the coronavirus 
00:00:58 pandemic for everyone feeling the medical psychological and financial burden of this crisis I'm sending love
00:01:05 your way stay strong or in this together we'll beat this thing this is the artificial intelligence
00:01:12 podcast if you enjoy it subscribe on YouTube review it with five stars an apple podcast support on
00:01:18 patreon or simply connect with me on Twitter Alex Friedman spelled Fri D ma n as usual I'll do one or two minutes of
00:01:26 ads now and never in the ads in the middle they can break the flow or the conversation I hope that works for you
00:01:32 and doesn't hurt the listening experience this show is presented by cap the number one finance I up in the App
00:01:40 Store when you get it you just collects podcast cash your app lets you send money to friends buy Bitcoin and invest
00:01:47 in the stock market with as little as one dollar since cash app allows you to send and receive money digitally
00:01:54 peer-to-peer and security in all digital transactions is very important let me mention the PCI data security standard
00:02:01 that cash app is compliant with I'm a big fan of standards for safety and security PCI DSS is a good example of
00:02:08 that where a bunch of competitors got together and agreed that there needs to be a global standard around the security
00:02:15 of transactions now we just need to do the same for autonomous vehicles and AI systems in general so again if you get
00:02:21 cash app from the App Store or Google Play and use the code Lex podcast you get ten dollars in cash wrap will also
00:02:28 donate ten dollars to first an organization that is helping to advance robotics and STEM education for young
00:02:35 people around the world and now here's my conversation with William McCaskill what is utopia for humans and all life
00:02:43 on Earth look like for you that's a great question what I want to say is that we don't know
00:02:52 and the Utopia we want to get to is an indirect one that I call the long reflection so period of post-scarcity no
00:03:00 longer have the kind of urgent problems we have today but instead can spend perhaps it's tens of thousands of years
00:03:08 debating engaging ethical reflection in order before we take any kind of drastic lock in actions like spreading to the
00:03:18 Stars and then we can figure out what is like what is of kind of moral value a long reflection as a that's a really
00:03:26 beautiful term so if we look at Twitter for just a second do you think human beings are able to reflect in a
00:03:38 productive way I don't mean to make you sound bad because there is a lot of fights and politics and division in our
00:03:46 discourse maybe if you zoom out it actually is civilized discourse it might not feel like it but when you zoom out
00:03:53 but so I don't want to say that Twitter is not civilized discourse I actually believe it it's more civilized than
00:03:59 people give it credit for but do you think the long reflection can actually be stable where we as human beings with
00:04:08 our descendants of a brains would be able to sort of rationally discuss things together and arrive at ideas I
00:04:17 think overall we're pretty good at discussing things rationally and you know at least in the earlier stages of
00:04:27 our mind be other lives being open to many different ideas and being able to be convinced and change our views I
00:04:36 think that Twitter's designed almost to bring out all the worst tendencies so if the long reflection were conducted on
00:04:43 Twitter you know maybe would be better just not even not even to bother but I think the challenge really is getting to
00:04:52 a stage where we have a society that is as conducive as possible to rational reflection to deliberation I think we're
00:05:03 actually very lucky to be in a liberal society where people are able to discuss a lot of ideas and so on I think when we
00:05:08 look to the future that's not at all guaranteed that society would be like that rather than less I rather than a
00:05:16 society where there's a fixed Canon of values that are being imposed on all society and where you aren't able to
00:05:23 question that that would be very bad for my perspective because it means we wouldn't be able to figure out what the
00:05:29 truth is I can already sense we're gonna go down yeah a million tangents but what do you think is the if Twitter is not
00:05:40 optimal what kind of mechanism this modern age of technology can we design where the exchange of ideas could be
00:05:49 both civilized and productive and yet not to be not be too constrained where there's rules of what you can say and
00:05:56 can't say which is as you say is not desirable but yet not have some limits as what can be said or not and so on do
00:06:04 you have any ideas thoughts on a possible future of course nobody knows how to do it but do you have thoughts of
00:06:10 what a better Twitter might look like I think that text-based media are intrinsically going to be very hard to
00:06:19 be conducive to professional discussion because if you think about it from a non informational perspective if I just send
00:06:27 you a text of less than what is it now two hundred and forty characters doing than 80 characters I think that's a tiny
00:06:34 amount of information compared to say you and I talking now where you have access to the words I say
00:06:40 which is the same as in text but also my tone also my body language and we're very poorly designed to be able to
00:06:48 assess you know I have to read all this context into anything you say so you know I say you may be your partner
00:06:55 sends you a text and as a full-stop at the end are they mad at you right you don't know you have to infer everything
00:07:02 about this person's mental state from whether they put a full staff at the end of a text or know where the flip side of that
00:07:08 is it truly text that's the problem here because there's a there's a viral aspect to the text where it's you could just
00:07:18 post text non-stop it's very immediate you know in the times before Twitter before the internet you know the way you
00:07:27 would exchange texts is you write books yeah and that while it doesn't get body language it doesn't get tone it doesn't
00:07:35 so on but it does actually boil down after some time thinking some editing so on boil down ideas so so yeah it's is
00:07:45 the immediacy and the viral nature the out that which produces the outrage mobs and so on the potential problem I think
00:07:51 that is a big issue I think there's going to be the strong selection effect where something provokes outrage well
00:07:59 that's high arousal you're more likely to to retweet that where there's kind of so bill analysis is not as sexy not as
00:08:10 viral I do agree that long-form content is much better to productive discussion in terms of the media that very popular
00:08:20 at the moment I think the podcasting is great where like your podcast for two hours long so they're much more than
00:08:31 depth than Twitter are and you are able to convey so much more nuance so much more caveat because it's an actual
00:08:38 conversation it's more like the sort of communication that we've evolved to do rather than kind of these very small
00:08:45 little snippets of ideas that when also combined with bad incentives just clearly aren't designed for helping us
00:08:50 get to the truth it's kind of interesting there it's not just the length of the podcast medium but it's
00:08:56 the fact that was started by people that don't give a damn about quote unquote the man yet of that it's there's a
00:09:06 relaxed sort of the style like that Joe Rogan does there's a freedom took to to express ideas in a unconstrained way
00:09:16 that's very real it's kind of funny in that it feels so refreshing Lee real to us today and I wonder what the
00:09:25 future looks like it's a little bit sad now that quite a lot of sort of more popular people are getting into
00:09:32 podcasting mm-hmm and then like me you know and like and they they tried to sort of create they try to control it
00:09:39 they try to constrain it in different kinds of ways people I love like Conan O'Brien and so on different comedians
00:09:47 and I'd love to see where the the real aspects of this podcast and medium persists maybe in TV maybe in YouTube
00:09:55 maybe Netflix is pushing those kind of ideas and it's kind of it's a really exciting word that kind of sharing of
00:10:01 knowledge yeah I mean I think it's a double-edged sword as it becomes more popular and more profitable where on the
00:10:06 one hand you'll get a lot more creativity people doing more interesting things with the medium but also perhaps
00:10:12 you get this place to the bottom where suddenly maybe pod will be hard to find good content on podcasts because it'll
00:10:22 be so overwhelmed by you know the latest bit of viral outrage so speaking of that jumping on effective attrition for a
00:10:33 second so so much of that internet content is funded by advertisements just in your in the context of effective
00:10:42 altruism we're talking about Inc the the the richest companies in the world they're funded by advertisements
00:10:46 essentially Google that's their primary source William do you see that as do you have any criticism of that source of
00:10:56 income do you see that source of money as a potentially powerful source of money that could be used well certainly
00:11:03 can be used for good but is there something bad about that source of money I think the significant worries with it
00:11:12 where it means that the incentives of the company might be quite misaligned with you know making people's lives better
00:11:25 where again perhaps the incentives are towards increasing drama and debate on your social news social media feed in
00:11:33 order the more people are going to be engaged perhaps kind of compulsively involved with the
00:11:43 platform whereas there are other business models like having an opt-in subscription service where perhaps they
00:11:53 have other issues but there's much more of a incentive to provide a product that its users are just like really wanting
00:12:02 because you know now I'm paying for this product I'm paying for this thing I want to buy rather than I'm trying to use
00:12:10 this thing and it's gonna get a profit mechanism that is somewhat orthogonal to me actually just wanting to use the use
00:12:21 the product and so I mean in some cases it will work better than others I can imagine a I can in theory you imagine
00:12:28 Facebook movie having a subscription service but I think it's you know unlikely to happen anytime soon
00:12:34 well it's interesting and it's weird now that you bring it up that it's unlikely this example I pay I think 10 bucks a
00:12:41 month for YouTube's bread mm-hmm and that's another thing I get it much for that except just so no ads yeah but in
00:12:51 general it's just a slightly better experience and I would gladly now I'm not wealthy in fact I'm
00:12:59 operating very close to zero dollars but I would pay 10 bucks a month to Facebook content mok's a month to Twitter yeah
00:13:07 for some kind of more control yeah in terms of advertisements and so on but the other aspect of that is data
00:13:14 yeah personal data people are really sensitive about this and I'm I as one who hopes to one day create a company
00:13:25 that may may use people's data to do good for the world wonder about this for one the psychology of why people are so
00:13:33 paranoid well I understand why but they seem to be more paranoid than it's justified at times and the other is how
00:13:39 do they do it right so it seems that Facebook is it seems that Facebook is doing it wrong
00:13:49 that's certainly the part of the popular narrative it's unclear to me actually how wrong
00:13:54 like I tend to give him more benefit of the doubt because there you know it's a really hard thing to do right mmm and
00:14:02 people don't listen realize it but how do we respect in your view people's privacy yeah I mean in the case of how
00:14:11 worried are people about using the data I mean there's a lot of public debate and criticism about it when we look at
00:14:21 people's revealed preferences yeah you know people's continuing massive use of these sorts of services it's not clear
00:14:29 to me how much people really do care perhaps they care a bit but they're happy to in effect kind of sell their
00:14:36 data in order to be able to get you a certain service that's a great term reveal preferences so these aren't
00:14:42 preferences your self-report in a survey this is like your actions speak yeah exactly so you might say oh yeah I hate
00:14:51 the idea of Facebook having my data but then when it comes to it you actually are willing to give that data in
00:14:58 exchange for being able to use the service and if that's the case then I think unless we have some explanation
00:15:09 about why why there's some negative externality from that or why there's or if there's something that consumers
00:15:20 are just really misled about where they don't realize why giving away data like this is a really bad thing to do then
00:15:31 ultimately I kind of want to you know respect people's preferences they can give away their data if they want I
00:15:37 think there's a big difference between companies use of data and governments having data where you know looking at
00:15:45 the particular code of history at governments knowing a lot about their people can be very bad if the government
00:15:55 chooses to do bad things with it and that's more wedding I think so let's jump into it a little bit most people
00:16:04 know but actually I two years ago I had no idea what effective altruism was until I saw there's a cool-looking event
00:16:11 in an MIT group here mm-hmm they I think the it's called the effective altruism Club or a group
00:16:18 that's like what the heck is that yeah and one of my friends said I mean they he said that they're just eccentric
00:16:31 characters I was like hell yes I'm in so I went to one of their events and looked up what's it about it's quite a
00:16:37 fascinating philosophical and just a movement of ideas so can you tell me what is effective altruism great so the
00:16:45 core of effective altruism is about to answer this question which is how can I do as much good as possible with my
00:16:52 scarce resources my time and with my money and then once we have our best guess answers to that trying to take
00:16:59 those ideas and put that into practice and do those things that we believe will do the most good and we're now a
00:17:06 community of people many thousands of us around the world who really are Tanna answer that question as best we can and
00:17:13 then use our time and money to make the world better so what's the difference between sort of classical general idea
00:17:25 of altruism and effective altruism so normally when people will try to do good they often
00:17:31 often just aren't so reflective about those attempts so someone might approach you in the street asking you to give to
00:17:41 charity and you'll you know if you're feeling out a stick you'll give to the person on the street or if you think oh
00:17:48 I want to do some good in my life you might volunteer at a local place or perhaps you'll decide you know pursue a
00:17:56 career where you're working in a field that's kind of more obviously beneficial like being a doctor or a nurse or health
00:18:04 care professional but it's very rare that people apply the same level of rigor and analytical thinking to lots of
00:18:14 other areas we think about so take the case of someone approaching you in the street imagine if that person instead
00:18:19 was saying hey I've got this amazing company do you want to invest in it it would be insane and no one would ever
00:18:26 think oh of course I'm just a company like you'd think it was a scam but somehow we don't have that same
00:18:31 level of rigor when it comes to doing good even though the stakes are more important when it comes to trying to
00:18:37 help others then kind of make money for ourselves well first of all so there is a psychology at the individual level of
00:18:47 doing good just feels good yes and so in some sense and that's pure psychological part it doesn't matter in fact you don't
00:18:55 want to know if it does good or not because most of the time it won't yeah so like in a certain sense it's
00:19:07 understandable why altruism without the effective part is so appealing to a certain population by the way let's zoom
00:19:16 off for a second do you think most people to questions do you think most people are good and question number two
00:19:23 is do you think most people want to do good so are most people good I think it's just super dependent on the
00:19:32 circumstances that someone is in I think that the actions people take and their moral worth is just much more dependent
00:19:40 on circumstance than it is on someone's intrinsic character so there's an evil within it seems like like with the better
00:19:47 angels of our nature there's a tendency of us as a society to tend towards good less war I mean with
00:19:57 all these metrics what is that us becoming who we want to be or is that some kind of societal force what's the
00:20:05 nature versus nurture thing here yes so in that case I just think yeah so violence has massively declined over
00:20:12 time I think that's a slow process of cultural evolution institutional evolution such that now the incentives
00:20:20 for you and I to be violent a very very small indeed in contrast when we were hunter-gatherers the incentives were
00:20:28 quite large if there was someone who was you know potentially disturbing the social order and hunter-gatherer setting
00:20:36 there was a very strong incentive to kill that person and people did and it was just the guarded you know 10% of
00:20:42 deaths among hunter-gatherers all arguably were murders after a hunter gatherers when you have actual societies
00:20:51 is when bars can probably go up because there's more incentive to do mass violence right to take over all their
00:20:59 conquer other people's lands and murder everybody in place and so on yeah I mean I think total death leaf the mother heap
00:21:07 from human causes does go down but you're right that if you're in a hunter-gatherer situation you're you
00:21:14 kind of group that you're part of is very small then you know you can't have massive wars that just massive
00:21:20 communities does exist but anyway there the second question eating most people want to do good yeah and then I think
00:21:27 that is true for most people I think you see that with the fact that you know most people donate a large proportion of
00:21:34 people volunteer if you give people opportunities to easily help other people they will take it but at the same
00:21:42 time we're a product of our circumstances and if it were more socially rewarded to be doing more good
00:21:48 if it were more socially the water to do good effectively about them not effectively then we would see that
00:21:56 behavior a lot more why why should we do good yeah my answer to this is there's no kind of deeper level
00:22:06 of explanation so my answer to kind of why should you good is well there is someone whose life is on the line for
00:22:15 example whose life you can save via donating just actually a few thousand dollars to an effective nonprofit and
00:22:22 like the against malaria foundation that is a sufficient reason to do good and then if you ask well why or I do that
00:22:29 and like I just show you the same facts again those it's that fact that is the reason to do good there's nothing more
00:22:34 fundamental than that I'd like to sort of make more concrete the thing we're trying to make better so
00:22:43 you just mentioned malaria so there's a huge amount of suffering in the world are we trying to remove so is ultimately
00:22:54 the goal not ultimately but the first step is to remove the worst of the suffering so there's some kind of
00:23:02 threshold of suffering they want to make sure it does not exist in the world or do we really naturally want to take a
00:23:12 much further step and look at things like income inequality so not just getting everybody above a certain
00:23:18 threshold but making sure that there is some that that is broadly speaking there's less injustice in the world
00:23:29 unfairness in some definition of course very difficult to define a fairness yeah so the metric I use is how many people
00:23:36 do we affect and by how much do we affect them and so that can you know often that means eliminating suffering
00:23:45 but it doesn't have to could be helping promote a flourishing life instead and so if I was comparing you know reducing
00:23:55 income inequality or a limb you know getting people from the very pits of suffering to a higher level the app the
00:24:02 question I would ask is just a quantitative one of just if I do this first thing of a second thing how many
00:24:09 people am I going to benefit and by how much am I going to benefit am I going to move that one person from kind of 10% 0%
00:24:17 well-being to 10% well-being perhaps that's just not as good as moving a hundred people from 10% well-being to
00:24:24 50% well-being and the idea is the diminishing returns is the idea of one year the when you're in terrible poverty
00:24:36 then the one dollar that you give goes much further than if you were in the middle class in the United States for
00:24:43 example absolutely and this this fact is really striking so if you take even just quite a conservative estimate
00:24:55 of how we are able to turn money into well-being The Economist put it as like a log curve that's the or steeper but
00:25:03 that means that any proportional increase in your income has the same impact on your well-being and so someone
00:25:11 moving from thousand dollars a year to two thousand dollars a year has the same impact to someone moving from a hundred
00:25:18 thousand dollars a year to two hundred thousand dollars a year and then when you combine that with the fact that we
00:25:25 in middle class members of rich countries are a hundred times richer than financial terms in the global poor
00:25:32 that means we can do a hundred times to benefit the poorest people in the world as we can to benefit people of our
00:25:38 income level and that's this astonishing fact yeah it's it's quite incredible a lot of these facts and ideas are just
00:25:51 difficult to think about because there's an overwhelming amount of suffering in the world
00:25:59 and even acknowledging it is difficult not exactly sure why that is I mean I mean it's difficult because you have to
00:26:08 bring to mind you know it's an unpleasant experience thinking about other people suffering it's unpleasant
00:26:14 to be empathizing with it firstly and then secondly thinking about it means that maybe we'd have to change our
00:26:21 lifestyles and if you're very attached to the income that you've got perhaps you don't want to be confronting ideas
00:26:29 or arguments that might cause you to use some of that money to help others so it's quite understandable in the
00:26:35 psychological terms even if it's not the right thing that we ought to be doing so how can we do better how can we be more
00:26:44 effective how does data help yeah in general how can how can we do better it's definitely hard and we have spent
00:26:52 the last ten years engaged in kind of some deeper search projects so today an answer can two questions one is of all the many
00:27:02 problems the world is facing what are the problems we ought to be focused on and then within those problems that we
00:27:08 judge to become the most pressing where we use this idea of focusing on problems that are the biggest in scale there are
00:27:17 the most fact able where we can do have the kind of make the most progress on that problem and there are the most
00:27:25 neglected within them what the things that are have the kind of best evidence or we have the best guest will do the
00:27:33 most good and so we have a bunch of organizations so I give well for example is focused on global health and
00:27:40 development and has a list of seven top recommended charities so the idea in general and sorry to interrupt is so
00:27:47 what doc was sort of poverty and Emma will flare an existential risk this is all fascinating topics but in general
00:27:55 the idea is there should be a group not sorry there's a lot of groups that seek to convert money into good and then you
00:28:13 accounting of how good they actually did perform that conversion how well they did in converting money to good
00:28:20 so ranking of these different groups ranking this the charities so that does that apply across basically all aspects
00:28:30 of effective altruism so there should be a group of people and they should be they should report on certain metrics on
00:28:36 how well they've done and you should only give your money to groups that do a good job that's the core idea I'd make
00:28:44 two comments one is just it's not just about money so we're also trying to encourage people
00:28:50 to work in areas where they'll have the biggest impact absolutely and in some areas you know they're really people
00:28:57 heavy but money poor other areas of the money which and people poor and so whether it's better to focus time or
00:29:05 money depends on the cause area and then the second is that you mentioned metrics and while that's the ideal and
00:29:11 in some areas we do we are able to get somewhat quantitative information about how much
00:29:19 impact an area is having that's not always true for some of the issues like you mentioned existential risks well
00:29:29 we're not able to measure in any sort of precise way like how much progress we're making and so you have to instead fall
00:29:37 back on just a regular sgt and evaluation even in the absence of data so as let's first sort of linger on your
00:29:47 own story for a second how do you yourself practice effective altruism in your own life because I think that's
00:29:54 really interesting place to start so I've tried to build effective altruism into at least many components of my life
00:30:03 so on the donation side my plan is to give away most of my income over the course of my life I've got a set bar I
00:30:11 feel happy with and I just donate above that bar so at the moment I donate about twenty percent of my income then on the
00:30:21 career side I've also shifted kind of what I do where I was initially planning to work on very esoteric topics in the
00:30:30 philosophy of logic for us whose language things that have intellectually extremely interesting but the path by
00:30:35 which they really make a difference to the world is let's just say it's very unclear that at best and so I switched
00:30:43 and said to researching ethics to actually just working on this question of how we can do as much good as
00:30:50 possible and then I've also spent a very large chunk of my life over the last ten years creating a number of nonprofits
00:30:57 who again in different ways are tackling this question of how we can do the most good and helping them to grow over time
00:31:03 - yeah we'll mention a few of them of the career selection 80,000 80,000 hours 80,000 hours and a really interesting
00:31:15 group so maybe also just a quick pause on the origins of effective altruism because you paint a picture who the key
00:31:24 figures are including yourself in the effect of ultras movement today yeah there are two main
00:31:31 strands that can I came together to form the effect of altruism movement so one was two philosophers myself and Toby
00:31:41 awed at Oxford and we had been very influenced by the work of Peter Singer and Australian moral philosopher who had
00:31:49 argued for many decades that because one can do so much good at such little cost to oneself we have an obligation to give
00:31:56 away most of our income to benefit those and actually in poverty just in the same way that we have an obligation to run in
00:32:03 and save a child from Adamic finish planning in a shallow pond if it would just tell you in your suit to the
00:32:10 coaster a few thousand dollars and we set up given what we can in 2009 which is encouraging people to give at least
00:32:16 10 percent of our income to the most effective charities and the second main strand was the formation of giveWell
00:32:22 which was originally based in New York and started in about 2007 and that was set up by Holden canossian le Hassenfeld
00:32:34 who were two hedge fund dudes who you know making making good money and thinking well where should I donate and
00:32:40 in the same way as if they wanted to buy a product for themselves they would look at Amazon reviews they were like well
00:32:47 what are the best charities fans they just weren't really good answers to that question certainly not they were satisfied with
00:32:53 and so they formed giveWell in order to try and work out what are those charities where they can have the
00:33:00 biggest impact and then from there and some other influences can a community glue and spread can we explore the
00:33:08 philosophical and political space that effective altruism occupies a little bit so from the little and distant in my own
00:33:18 life time that I've read of Iran's work Iran's philosophy of Objectivism espouses and it's interesting to put her
00:33:26 philosophy in contrast yeah with effective altruism so it espouses selfishness as the best thing you can do
00:33:35 yeah and if you but it's not actually against altruism its if it's just you have that choice but you should be
00:33:44 selfish in it right or not maybe you can disagree here but so it can be viewed as the complete opposite of effective
00:33:50 altruism or it can be viewed as similar because the word effective is really interesting because if you want to do
00:34:00 good then you should be damn good at doing good right that's the I think that would fit within the the morality that's
00:34:09 defined by objectivism so do you see a connection between these two philosophies and other perhaps other in
00:34:16 this in this complicated space of beliefs that effective altruism is positioned as opposing or
00:34:25 lined with oh definitely say that objectivism England's philosophy is a philosophy that's you know quite
00:34:32 fundamentally opposed to effective altruism in we can waive insofar as England's philosophy is about
00:34:40 championing egoism and saying that are never quite sure whether the philosophy is meant to say that just you ought to
00:34:46 do whatever will best benefit yourself as ethical egoism no matter what the consequences are or second if there's
00:34:55 this alternative view which is well you ought to attain benefit yourself because that's actually the best way of
00:35:03 benefiting society certainly in you know Atlas Shrugged she is presenting her philosophy as a way that's you know
00:35:10 actually going to bring about a flourishing society and if it's the former then well effective altruism is
00:35:16 all about promoting the idea of altruism so it's and saying in fact we ought really be trying to help others as much
00:35:23 as possible so it's opposed there and then on the second side I would just dispute the empirical premise it would
00:35:30 seem given the major problems in the world today it would seem like this remarkable coincidence quite suspicious
00:35:37 one might say if benefiting myself was actually the best way to bring about a better world so in that point and I
00:35:44 think that connects also with career selection that we'll talk about but let's consider not objectives but
00:35:55 capitalism so and the idea that you focusing on the thing that you damn I are damn good at whatever that is yeah
00:36:05 maybe the best thing for the world sort of part of it is also mindset right sort of like I the thing I love is robots
00:36:15 yeah so maybe I should focus on building robots and never even think about the idea of effective altruism it which is
00:36:25 kind of the capitalist notion yeah is there any value in that idea and just finding anything you're good at and
00:36:30 maximizing your productivity in this world and thereby sort of lifting the ball boats and benefiting society
00:36:39 and as a result yeah I think there's two things I'd want to say on that so one is what your comparative advantages what
00:36:45 your strengths are when it comes to career there's obviously super important because you know there's lots of career
00:36:51 paths I would be terrible at if I was thought being an artist was the best thing one could do well I'd be doomed
00:36:58 just really it quite astonishingly bad and so I do think at least within the realm of things that could plausibly be
00:37:05 very high impact yes choose the thing that you're gonna that you think you're going to be able to
00:37:12 like really be passionate at and excel at kind of over the long term then on this question of like should one just do
00:37:19 that in an unrestricted way and not even think about what the most important problems are I do think that in a kind
00:37:27 of perfectly designed society that might well be the case that would be a society where we've collected all market
00:37:32 failures we've internalized all externalities and then we've managed to set up incentives such that people just
00:37:42 pursuing their own strengths is the best way of doing good but we're very far from that Society so if one did that
00:37:50 then you know it would be very unlikely that you would focus on improving the lives of non-human animals that you know
00:37:58 aren't participating in markets or ensuring the long-run future goes well we're future people certainly aren't
00:38:04 participating in markets our benefiting the global poor who do participate but have so much less can a power from a
00:38:13 starting perspective that their views on accurately kind of represented in by market forces to guarded so yeah the in
00:38:22 serves pure pure definition capitalism and just seem may very well ignore the people that are suffering the most the
00:38:30 wide swath of them so if you could allow me this line of thinking here so I've listened to a lot of your conversations
00:38:43 Yellin I find if I can compliment you is there very interesting conversation is your conversation on Rogan and Joe
00:38:49 Rogan's was really interesting with sam harris and so on whatever there's a lot of stuff that's really
00:38:57 good out there and yet when I look at the internet and look at YouTube which has certain mobs
00:39:07 certain swaths of right-leaning folks yeah whom I dearly love I love all people all especially people with ideas they
00:39:23 seem to not like you very much so I don't understand why exactly so what my own sort of hypothesis is there is a
00:39:33 right left divide that absurdly so caricatured in politics at least in the United States and maybe you're somehow
00:39:42 pigeon-holed into one of those sides and there did you know maybe that that's what it is this may be your message is
00:39:50 somehow politicized yeah I mean well how how do you make sense of that because you're extremely interesting like you
00:39:57 got the the comments I see on Joe Rogan there's a bunch of negative stuff and yet if you listen to it the conversation
00:40:04 is fascinating I'm not speaking I'm not some kind of lefty yeah extremist but just this a fascinating conversation so
00:40:12 why are you getting some wine like nollamara hate so I'm actually pretty glad that effective altruism has managed
00:40:21 to stay on relatively unpolitical because I think the core message just use some of your time and money to do as
00:40:27 much good as possible to fight some of the problems in the world can be you know appealing across the political
00:40:32 spectrum and we do have a diversity of political viewpoints among people who have engaged and effective altruism we
00:40:40 do however do get some criticism from the left and the light was to Chris both Oh be interesting to hear yeah so
00:40:48 criticism from the left is that we're not focused enough on dismantling the capitalist system that they see as the
00:40:55 loot of most of the problems that we're talking about and there I kind of disagree on partly the premise
00:41:09 where I don't think relevant alternative systems would say to the animals or to the the global poor which the future
00:41:15 generations can a much better and then also the tactics where I think for the particular ways we can change society
00:41:22 that would massively benefit you know be massively beneficial on those things that don't go via dismantling like the
00:41:29 entire system which is perhaps a million times harder to do then criticism on the right there's definitely in like in
00:41:37 response to the Joe Rogan podcast they definitely were a number of inland fans who weren't keen on the idea of
00:41:44 promoting altruism there was a remarkable set of ideas just the idea of it affected unmanly I think was driving
00:41:56 a lot of criticism let me okay so I love fighting I've been in street fights my whole life
00:42:03 I'm  I'm as alpha and everything I do as he gets and the fact that and Joe Rogan said that I thought scent of a
00:42:11 woman is a better movie than John wick put me into this beta category among people core like basically saying this
00:42:21 yeah unmanly or it's not tough it's not it's not some yeah principled view of strength that is represented by as
00:42:30 possible and so actually so how do you think about this because to me altruism especially effective altruism is I don't
00:42:42 I don't know what the female version of that is but on the male side manliest if I may say so so what how do you
00:42:51 think about that kind of criticism I think people who would make that criticism are just occupying a like
00:42:57 state of mind that I think is just so different from my state of mind that I kind of struggle to maybe even
00:43:05 understand it where if something's manly or unmanly or feminine or unfeminine I'm like I don't care like is it the right
00:43:11 thing to do or the wrong thing to do so yeah I let me put in that in terms of man a woman yeah I don't think that's
00:43:17 useful but I think there's notion of acting out of fear yeah that or as opposed to out of principle and
00:43:28 strength yeah so okay yeah here's something that I do you know feel as an intuition and that I think drives some
00:43:36 people who do find kind of a and land attractive and and so on as a philosophy which is a kind of taking power of
00:43:43 you're taking control of your own life and having power over how you're stealing your life and not kind of
00:43:54 cowering to others you know really thinking things through I find like that set of ideas just very compelling and
00:44:00 inspirational but I actually think of effective altruism has really Xenu that side of my personality it's like
00:44:10 scratch that itch where you and just not taking the kind of priorities that society is giving you is granted instead
00:44:17 you're choosing to act in accordance with the priorities that you think are most important in the world and often
00:44:28 that involves then doing quite on quite unusual things from a societal perspective like donating a large chunk
00:44:35 of your earnings or working on these weird issues about AI and so on that other people might not understand yeah I
00:44:41 think that's a really gutsy thing to do that is taking control that's at least at this stage I mean that that's that's
00:44:52 you taking ownership not of just yourself but your presence in this world that's full of suffering and saying as
00:45:00 opposed to being paralyzed by that notion yeah it's taking control and saying I could do something yeah I mean
00:45:06 that's really powerful but I mean sort of the the one thing I personally hate too about the left currently that I
00:45:14 think those folks to detect is the social signaling mmm-hmm do you see when you look at yourself
00:45:20 yeah sort of late at night would you do everything you're doing in terms of effective
00:45:27 altruism if your name because you're quite popular but if your name was totally unattached to it
00:45:33 if it was in secret yeah I mean I think I would to be honest I think the kind of popularity is like you know it's mixed
00:45:43 bag but there are serious costs and I don't particularly I don't like love it like it means you get all these people
00:45:50 calling you a on Joey Logan it's like not the most fun thing like how do you also get a lot of sort of brownie
00:45:57 points for doing good for the world yeah you do but I think I think my ideal life I would be like in some library solving
00:46:04 logic puzzles all day and I'd like really be like learning maths and so on so you're and how you know have a like
00:46:11 good buddy offends and so on so your instinct for effective altruism is something deep it's it's not it's not a
00:46:18 it's not one that you know it's communicating socially it's more in your heart you want to do good for the world
00:46:25 yeah I mean so we can look back to early giving what we can so you know we're setting this up me and giving me and
00:46:35 Toby and I really thought that doing this would be a big hit to my academic career because I was now spending you
00:46:40 know at that time more than half my time setting up this nonprofit at the crucial time when you should be like producing
00:46:47 their best academic work and so on and it was also the case that at the time it was kind of like the Toby Order
00:46:54 Club you know he was he was the most popular this personal interest story around him and his plans donate and
00:47:02 sorry to interrupt but Toby was donating a large amount can you tell just briefly what he was doing
00:47:07 yeah so he made this public commitment to give everything here and above 20,000 pounds per year to the most effective
00:47:16 causes and even as a graduate student he was still donating about 15-20 percent of his income which so quite significant
00:47:23 given that graduate students are not known for being super wealthy that's right and when we launched giving what
00:47:30 we can the media just loved this as like a a personal into their story so the story about him and his pledge was the
00:47:39 most yeah it was actually the most popular news story of the day and we kind of ran the same story a year later
00:47:44 and it was the most popular and used to a year later - and so it really was kind of several years before then I was also
00:47:54 kind of giving more talks I'm starting to do more writing and then especially with you know whether it was book doing
00:48:00 good better that then they started to be kind of attention and so on but deep inside your own relationship with
00:48:11 effective altruism was I mean it had nothing to do with the publicity that did it did you see yourself how did the
00:48:17 publicity connect with it yeah I mean that's kind of what I'm saying is I think the publicity came like several
00:48:23 years after afterwards I mean at the early stage when we set up giving what we can it was really just every person
00:48:30 we get pledged 10% is you know something like $100,000 over their lifetime that's huge so it was just we had started with
00:48:40 23 members every single person was just this like kind of huge accomplishment and at the time I just really thought
00:48:47 you know maybe over time we'll have a hundred members and they'll be like amazing whereas now we have you know
00:48:52 over four thousand and one and a half billion dollars pledged that's just unimaginable to me at a time when I was
00:49:00 first kind of getting this you know gang with stuff off the ground so can we talk about poverty and the other the biggest
00:49:12 problems that you think in the near term effective altruism can can attack in each one so poverty obviously is a huge
00:49:22 one yeah how can we help okay yeah so poverty absolutely this huge problem seven hundred million people and
00:49:28 actually in poverty living in less than $2 per day where that's what that means is what $2 would buy in the US so think
00:49:38 about that it's like some rice maybe some beans it's very you know really not much and at the same time we can do an
00:49:46 enormous amount to improve the lives of people in extreme poverty so the things that we tend to focus on are
00:49:53 interventions in global health and that's for a couple few reasons one is that Global Health just has this amazing
00:49:58 fact they're called life expectancy globally is up 50% relative to 60 or 70 years ago we've ever
00:50:06 allocated smallpox that's which killed 2 million lives every year almost eradicated polio second is that we just
00:50:13 have great data on what works when it comes to global health so we just know that bed nets protects children from
00:50:21 prevent them from dying from malaria and then the third is just that's extremely cost effective so it costs $5 to buy one
00:50:30 bed net protects two children for two years against malaria if you spend about $3,000 on bed nets then statistically
00:50:36 speaking you're going to save a child's life and there are other interventions too and so given people in such
00:50:45 suffering and we have this opportunity to you know do such huge good for such low cost well yeah why not so the
00:50:55 individuals of for me today if I wanted to to look at poverty how would I help and I wanted to say I think
00:51:04 donating 10% of your income is a very interesting idea or some percentage or some setting a bar instead of sticking
00:51:12 to it how do we then take the step towards the effective part so you've conveyed some notions but who do you
00:51:20 give the money to yeah so give well this organization I mentioned is well it makes charity
00:51:28 recommendations and some of its top recommendations so against malaria foundation is this organization that
00:51:36 buys and distributes these insecticides they did bed nets and then it has a total of seven like charities that it
00:51:41 recommends very highly so that recommendation is it is almost like a star of approval or is there some
00:51:50 metrics so what are what are the ways that give well conveys that this is a great charity organization yeah so give
00:52:00 well is looking at metrics and it's trying to compare Travie T is ultimately in the number of lives that you can save
00:52:08 or an equivalent benefit so one of the charities that recommends is give directly which
00:52:14 just transfers cash to the poorest families where poor family will get a cash transfer $1,000 and they kind of
00:52:22 regard that's the baseline intervention because it's it's so simple and people you know they know what to do with how
00:52:29 to benefit themselves that's quite powerful by the way so before give war before the effect of autism movement was
00:52:36 there I imagine as a huge amount of corruption mm-hmm funny enough in charity organizations or
00:52:44 misuse of money yeah so that now there was nothing like give up before that no I mean there was some so I mean the
00:52:49 charity corruption I mean obviously there's some I don't think it's a huge issue if I were to just focusing him
00:52:57 along things prior to give well there were some organizations like Charity Navigator which were more aimed at
00:53:04 worrying about corruption and so on so they weren't saying these are the charities where you're going to do the
00:53:10 most good instead it was like how good are the charity's financials how good is this held are they transparent and yeah
00:53:17 so that would be more useful for weeding out some of those worst charities so give well just taking a step further
00:53:23 sort of in this 21st century yeah of data it's actually looking at the effective part yeah so it's like you
00:53:32 know if you know the wire cutter if you want to buy a pair of headphones they will just look over headphones behind
00:53:36 these the best headphones you can buy that's the idea with gift well okay so do you think there's a a bar of what
00:53:45 suffering is and do you think one day we can eradicate suffering in our world yeah amongst humans let's talk humans
00:53:53 for now talk point humans but in general yeah actually so there's a colleague of mine kind of turn abolitionism for the
00:54:02 idea that we should just be kind of a bowl of suffering and in the long run I mean I don't expect anytime soon but I
00:54:09 think we can I think that would require you know quite change quite drastic changes to the way society is structured
00:54:20 and perhaps even the you know that he in fact even changes to human nature but I do think that suffering whenever it occurs
00:54:28 that and we should want it to not cur so there's a there's a line there's a gray area between suffering now I'm Russian
00:54:36 so I romanticize some aspects of suffering there's a gray line between struggle gray area between struggle and
00:54:47 suffering so one do we want to eradicate all struggle in the world there's a this there's an idea
00:54:59 you know that the human condition inherently has suffering in it and it's a creative force it's this it's a
00:55:08 struggle of our lives and we somehow grow from that how do you think about how do you think about that I agree
00:55:19 that's true so you know often you know clay artists can be also suffering from you know major health conditions or the
00:55:25 player and still want to come from abusive parents yeah most I'm great artists they think come from abusive
00:55:31 parents yeah that seems to be he's commonly the case but I want to distinguish between suffering as being
00:55:38 instrumentally good you know it causes people to produce good things and whether it's intrinsically good and I
00:55:44 think in physically it's always bad and so if we can produce these you know great achievements via some other means
00:55:52 where you know if we look at the scientific enterprise we've produced incredible things often from people who
00:55:59 aren't suffering have you know the good lives they're just they're driven instead of you know being pushed by
00:56:04 sense or languish they're being driven by intellectual curiosity if we can instead produce a society where it's all
00:56:12 carrot and no stick that's better from my perspective yeah but unless disagree with the notion that
00:56:17 that's possible but I would say most of the suffering in the world is not productive so I would a
00:56:27 dream of effective altruism curing that suffering yeah but then I would say that there is some suffering that is
00:56:34 productive that we want to keep that because but that's not even the focus of because most of the suffering is just
00:56:44 absurd yet I mean used to be eliminated so let's not even romanticize this usual notion I have but nevertheless struggle
00:56:52 has some kind of an inherent value that to me at least you're right there's some elements of human nature that also have
00:57:01 to be modified in order to cure all suffering yeah I mean there's an interesting question of whether it's
00:57:06 possible so at the moment you know most of the time we're kind of neutral and then we burn ourselves and that's
00:57:11 negative and that's really good that we get that negative signal because it means we won't burn ourselves again
00:57:17 there's a question like could you design agents humans such that you're not hovering around the zero level you're
00:57:25 having it like bliss yeah and then you touch the flame in your own oh you just slightly worse bliss yeah but that's
00:57:31 really bad compared to the bliss you were normally in so that so you can have like a gradient of bliss instead of like
00:57:37 pain and pleasure well on that point I think it's a really important point on the experience of suffering the relative
00:57:47 nature of it maybe having grown up in the Soviet Union were quite poor by any measure in and when I when I was my
00:57:59 childhood but it we didn't feel like you were poor because everybody around you were poor as  and then in America I
00:58:08 feel I for the first time begin to feel poor yeah because of the road there's different there's some cultural aspects
00:58:15 to it that really emphasize that it's good to be rich and then there's just the notion that there's a lot of income
00:58:22 inequality and therefore you experience that inequality that's where it's often called do you so what do you think about
00:58:30 the inequality of suffering that that we have to think about do you that do you think we have to think about that as
00:58:37 part of effective altruism yeah I think we're just things vary in terms of whether you get benefits or cost from
00:58:45 them just in relative terms or in absolute terms so a lot of the time yeah there's this hedonic treadmill where if
00:58:52 you get you know there's money is useful because it helps you buy things or good for you
00:59:00 because it helps you buy things but there's also a status component too and that status component is kind of
00:59:07 zero-sum if you were saying like in Russia you know no one else felt poor because everyone around you is poor
00:59:15 whereas now you've got this these other people who are you know super rich and maybe that makes you feel you know less
00:59:25 good about yourself there are some other things however which are just in tins eclis good or bad so commuting for
00:59:33 example is just people hate it it doesn't really change knowing that other people are commuting to doesn't make it
00:59:41 any any kind of less bad but to push back on that for a second I mean yes but also if some people were you know on
00:59:51 horseback your commute and the train might feel a lot better yeah you know the there is a relative nickname yeah
00:59:57 everybody's complaining about society today forgetting it's forgetting how much better it is the better angels of
01:00:06 our nature how the technology is improved fundamentally improving most of the world's lives yes and actually
01:00:13 there's some psychological research on the well-being benefits of volunteering where people who volunteer tend to just
01:00:21 feel happier about their lives and one of the suggested explanations is it because it extends your reference class
01:00:27 so no longer you comparing yourself to the Joneses who have they're slightly in that better car because realize that you
01:00:32 know people with much worse conditions than you and so now you know your life doesn't seem so bad that's actually an a
01:00:40 psychological level one of the fundamental benefits of effective altruism yeah is is I mean I guess it's
01:00:48 the altruism part of effective altruism is exposing yourself to the suffering in the world allows you to be more yea
01:00:59 happier and actually allows you in the sort of meditative introspective way I realize that you don't need most of
01:01:07 wealth you have to to be happy absolutely I mean I think effect that also has been this huge benefit for me
01:01:13 and I really don't think that if I had more money that I was living on that that would change my level of well-being
01:01:17 at all well as engaging in something that I think is meaningful that I think is
01:01:24 stealing humanity in a positive direction that's extremely rewarding and so yeah I mean despite my best attempts
01:01:32 at sacrifice I don't know I think I've actually ended up happier as a result of engaging
01:01:38 in effective altruism than it I would have done that's such an interesting idea yeah so let's let's talk about
01:01:45 animal welfare sure easy question what is consciousness okay especially as it has to do with the capacity to suffer I
01:01:52 think there seems to be a connection between how conscious something is the amount of consciousness and its ability
01:02:00 to suffer and that all comes into play of us us thinking how much suffering there's in the world with regard to
01:02:06 animals so how do you think about animal welfare and consciousness okay well consciousness easy question okay yeah I
01:02:13 mean I think we don't have a good understanding of consciousness my best guess is it's gone and by consciousness
01:02:19 I'm meaning what it is feels like to be you a subjective experience this seems to be different from everything else we
01:02:26 know about in the world yeah I think it's Clio is very poorly understood at the moment I think it has something to
01:02:32 do with information processing so the fact that the brain is a computer or something like a computer so that would
01:02:39 mean that very advanced AI could be conscious information processors in general could be conscious with some
01:02:48 suitable complexity but that also some suitable complexity it's a question whether greater complexity creates some
01:02:53 kind of greater consciousness which relates to animals yeah right is there if it's an information processing system
01:03:01 and it's smaller and smaller is an ant less conscious than a cow less conscious than an am monkey yeah and again this
01:03:11 super hard question but I think my best guess is yes like if you if I think more consciousness it's not some magical
01:03:17 thing that appears out of nowhere it's not you know Descartes thought it was just comes in from this other realm
01:03:24 and then enters through the pineal gland in your vein and that's you kind of soul and it's conscious so it's got something
01:03:31 to do with what's going on in your brain a chicken has one 300th of the size of the brain that you have aunts I don't
01:03:41 know how small it is maybe it's a million for size my best guess which I may well be wrong
01:03:47 about cuz this is so hard is that in some relevant sense the chicken is experiencing consciousness to a lesser
01:03:54 degree than the human and the ant significantly less again I don't think it's as little as three hundredths as
01:04:01 much I think there's evolutionary reasons for thinking that like the ability to feel pain comes on the scene
01:04:06 if I was heavily early on and we have lots of our brain that's dedicated stuff that doesn't seem to have to do and
01:04:11 anything to do with consciousness language processing and so on so it seems like the easy so there's a lot of
01:04:18 complicated questions there that we can't ask the animals about but it seems that there's easy questions in terms of
01:04:26 suffering which is things like factory farming that could be addressed yeah is that is that the lowest hanging fruit if
01:04:35 I may use crude terms here of animal welfare absolutely I think that's the lowest hanging fruit so at the moment we
01:04:43 kill we raise and kill about 50 billion animals every year so how many 50 billion in yeah so for every human on
01:04:52 the planet several times that number are being killed and the vast majority of them are raised in factory farms where
01:04:59 basically whatever your view on animals I think you should agree even if you think well maybe it's not bad to kill an
01:05:05 animal maybe if the animal was lays in good conditions that's just not the empirical reality the empirical reality
01:05:12 is that there kept an incredible cage confinement there dabiq tore detailed without anaesthetic you know chickens
01:05:20 often peck each other to death other like otherwise because in such stress it's really you know I think when a
01:05:27 chicken gets killed that's the best thing that happened to the chicken in the course of its life and it's also completely
01:05:33 necessary this is in order to save you know a few pence for a place of meat or place of eggs and we have indeed found
01:05:42 it's also just inconsistent with consumer preference as well people who buy the products if they could they or
01:05:50 they when you do surveys are extremely against suffering in factory farms it's just they don't appreciate how bad it is
01:05:56 and you know just tend to go with the easy options and so then the best the most effective programs I know of at the
01:06:03 moment are nonprofits that go to companies and work with companies to get them to take a pledge to cut certain
01:06:11 sorts of animal products like eggs and cage confinement out of their supply chain and it's now the case that the top
01:06:21 50 food retailers and fast food companies have all made these Kanna cage for the pledges and when you do the
01:06:27 numbers you get the conclusion that every dollar you're giving to these nonprofits result in hundreds of
01:06:32 chickens being spared from cage cage confinement and then they're working to other other types of animals other
01:06:40 products too so is that the most effective way to do in have a ripple effect essentially it's supposed to
01:06:49 directly having regulation from on top that says you can't do this so I would be more open to the regulation
01:06:57 approach but at least in the u.s. there's quite intense regulate to the capture from the agricultural industry
01:07:04 and so attempt that we've seen to try and change regulation have it's been a real uphill struggle there are some
01:07:13 examples of ballot initiatives where the people have been able to vote in a ballot to say we want to ban eggs from
01:07:19 caged conditions and that's been huge that's been really good but beyond that it's much more limited so I've been
01:07:28 really interested an idea of hunting in general wild animals and seeing nature as a form of Cruelty that I am ethically
01:07:43 more okay with okay just from our perspective and then I about wild animals suffering they I'm
01:07:49 just I'm just giving you the kind of yeah notion of how I felt because animal  because animal factory farming is so
01:07:58 bad yeah then living in the woods seemed good yeah and yet when you actually start to think about it all I mean all
01:08:08 of the animals in the animal world the living in like terrible poverty right yeah yeah so you have all the medical
01:08:15 conditions all of that I mean they're living horrible eyes it could be improved that's a really interesting
01:08:21 notion that I think may not even be useful to talk about because factory farming is such a big thing to focus on
01:08:28 but it's nevertheless an interesting notion to think of all the animals in the wild is suffering in the same way
01:08:35 the humans in poverty are suffering yeah I mean and often even worse so many animals we produced by our selection so
01:08:42 you have a very large number of children in the expectation that only small number survive and so for those animals
01:08:50 almost all of them just live short lives where they starve to death so yeah there's huge amounts of suffering in
01:08:57 nature I don't think we should you know pretend that it's this kind of wonderful paradise for most animals yeah their
01:09:08 life is filled with hunger and fear and disease I could agree with you entirely that when it comes to focusing on animal
01:09:15 welfare we should focus on factory farming but we also yeah should be aware to the reality of what life for most
01:09:24 animals is like so let's talk about a topic I've talked a lot about and you've actually quite eloquently talked about
01:09:32 which is the third priority that effective altruism considers is really important is there's no existential
01:09:40 risks yeah when you think about the existential risks that are facing our civilization what's what's before us
01:09:47 what concerns you what should we be thinking about from an especially from an effective altruism perspective okay
01:09:53 so the reason I started getting concerned about this was thinking about future generations where
01:10:01 the key idea is just well feature people matter Marlee there are vast numbers of future people if we don't cause of own
01:10:07 extinction there's no reason why civilization might not last a million years I mean we last as long as typical
01:10:15 mammalian species or a billion years is when the earth is no longer habitable or if we can take to the stars then perhaps
01:10:23 it's trillions of years beyond that so the future could be very big indeed and it seems like we're potentially very
01:10:30 early on in civilization then the second idea is just well maybe there are things that are going to really derail that
01:10:34 things that actually could prevent us from having this long wonderful civilization and instead could cause
01:10:45 although an acausal own extinction or otherwise perhaps like lock ourselves into a very bad state and what ways
01:10:54 could that happen well causing our own extinction development of nuclear weapons in the 20th century at least put on the table
01:11:02 that we now had weapons that were powerful enough that you could vary significantly despite society perhaps
01:11:09 and all that nuclear war would kill it cause a nuclear winter perhaps that would be enough for the human race to go
01:11:16 extinct when you think we haven't done it sorry general why do you think we haven't done it yet
01:11:24 this is surprising to you that having a you know always for the past few decades several thousand of active ready to
01:11:34 launch nuclear weapons warheads and yet we have not launched them yeah ever since the initial launch on Hiroshima
01:11:45 and Nagasaki I think it's a mix of luck so I think it's definitely not inevitable that we haven't used them so
01:11:51 John F Kennedy Jim Cuban Missile Crisis put the estimate of nuclear exchange between the US and USSR that's somewhere
01:11:58 between one and three and even so you know we really did come close at the same time I do think mutually assured
01:12:08 destruction is a reason why people don't go to war it would be mad you know why nuclear powers
01:12:13 don't go to war do you think that holds if you can link around there for a second like my dad's a physicist amongst
01:12:22 other things and he believes that nuclear weapons are actually just really hard to build mm-hmm which is one of the
01:12:31 really big benefits of them currently so that you don't have it's very hard if you're crazy to build to acquire a
01:12:38 nuclear weapon so the mutually assured destruction really works when you talk it seems to
01:12:46 work better when it's nation states when it's serious people even if they're a little bit you know dictatorial and so
01:12:54 on do you think this mutual assured destruction idea will Caray how far will it carry us in terms of different kinds
01:13:04 of weapons oh yeah I think it's your point that nuclear weapons are very hard to build and relatively easy to control
01:13:11 because you can control fissile material is a really important one and future technology that's equally destructive
01:13:18 might not have those properties so for example if in the future people are able to design viruses perhaps using a DNA
01:13:30 printing kit that's on that you know one can just buy in fact there were companies in the process of creating
01:13:42 home DNA printing kits well then perhaps that's just totally democratized perhaps the power to reap huge destructive
01:13:50 potential is in the hands of most people in the world or certainly most people with effort and then yeah I no longer
01:13:55 trust mutually assured destruction because some for some people the idea that they would die is just not a
01:14:04 disincentive there was a Japanese cult for example Shinrikyo in the 90s that had they what they believed was the
01:14:13 Armageddon was coming if you died before Armageddon you would get good karma you wouldn't go to hell if you died during
01:14:22 Armageddon maybe you would go to hell and they had a bio weapons program chemical weapons broke
01:14:29 them when they were finally apprehended they hadn't stocks of sarin gas that were sufficient to kill four million
01:14:36 people engaged in multiple terrorist acts if they had had the ability to print a virus at home that would have
01:14:43 been very scary so it's not impossible to imagine groups of people that hold that kind of belief of a death as in
01:14:56 suicide as a is a good thing for passage into the next world and so on and then you connect them with some weapons then
01:15:05 ideology and weaponry is make recreate serious problems for us we ask a quick question on what do you think is the
01:15:12 line between killing most humans and killing all humans how hard is it to kill everybody
01:15:19 yeah I'm thought about this he I've thought about it a bit I think is very hard to kill everybody so in the case of
01:15:26 let's say and all that nuclear exchange and let's say that leads to nuclear winter we don't really know but we you
01:15:33 know might well happen that would I think there's all in billions of deaths would it kill
01:15:41 everybody it's quite it's quite hard to see how that how it would kill everybody a few reasons one is just there were so
01:15:48 many people yes you know seven and a half billion people so this bad event has to kill all you know all almost all
01:15:56 of them secondly live in such a diversity of locations so in nuclear exchange of a virus that has to kill
01:16:03 people who live in the coast of New Zealand which is going to be climatically much more stable than other
01:16:10 areas in the world or people who are on submarines or who have access to bunkers so this is very like there's just like
01:16:19 I'm sure there's like two guys and Siberia just badass there's the just human nature somehow yes perseverance
01:16:27 yeah and then the second thing is just if there's some catastrophic event people really don't want to die so
01:16:33 there's gonna be like you know huge amounts of effort to ensure that it doesn't affect everyone
01:16:41 if you thought about what it it takes to rebuild a society or smaller smaller numbers like how big of a setback oh
01:16:48 these kinds of things are yeah so then that's something where there's real uncertainty I think where at some point
01:16:55 you just lose genetic sufficient genetic diversity such that you can't come back there's a it's unclear how small that
01:17:05 population is but if you've only got say a thousand people a few of the thousands and maybe that small enough what about
01:17:12 human knowledge and then there's human knowledge I mean it's striking how short on geological timescales or evolutionary
01:17:22 timescales the progress in or how quickly the progress in human knowledge has been like agriculture we only have
01:17:31 entered in 10,000 BC cities will only have you know 3,000 BC whereas typical mammal species is half a million years
01:17:37 to a million years do you think it's inevitable in some sense the agriculture everything that came the Industrial
01:17:47 Revolution cars planes the internet that level of innovation you think is inevitable I think given how quickly it
01:17:57 arose so in the case of agriculture I think that was dependent on climate so it was the kind of glacial period was
01:18:08 over the earth warmed up a bit that made it much more likely that humans would develop agriculture when it comes to the
01:18:17 Industrial Revolution it's just you know again only took a few thousand years from cities to Industrial Revolution if
01:18:25 we think ok we've gone back to this even let's say agricultural era but there's no reason why we wouldn't go extinct in
01:18:31 the coming tens of thousands of years or hundreds of thousands of years it seems just vet it would be very surprising if
01:18:37 we didn't rebound unless there's some special reason that makes things different yes so perhaps we just have a
01:18:45 much greater like disease burden now so HIV exists it didn't exist before and perhaps that's kind of latent in
01:18:53 being suppressed by modern medicine and sanitation and so on but would be a much bigger problem for some you know utterly
01:19:03 destroyed the society that was trying to rebound all this just maybe there's something we don't know about
01:19:11 so another existential risk comes from the mysterious the beautiful artificial intelligence yeah so what what's the
01:19:22 shape of your concerns about AI I think that quite a lot of concerns about AI and sometimes the different risks don't
01:19:32 get distinguished enough so the kind of classic worry most is closely associated with Nick Bostrom and eliezer yudkowsky
01:19:40 is that we at some point move from having now a way our systems to artificial general intelligence you get
01:19:47 this very fast feedback effect where a GI is able to build health you know artificial intelligence helps you to
01:19:54 build greater artificial intelligence we have this one system that's suddenly very powerful far more powerful than
01:20:00 others then perhaps far more powerful than you know the rest of the world combined and then secondly it has goals
01:20:10 that are misaligned with human goals and so it pursues its own goals it realize hey there's this competition namely from
01:20:17 humans it would better if we eliminated them in just the same way as Homo sapiens eradicated the Neanderthals in
01:20:27 fact it in fact killed off most large animals on the planet let walk the planet so that's kind of one set of
01:20:37 worries I think that's not my I think these shouldn't be dismissed to science fiction I think it's something we should
01:20:45 be taking very seriously but it's not the thing you visualize when you're concerned about them what the biggest
01:20:52 near-term yeah I think yes I think it's like one possible scenario that would be astronomically bad I think that other
01:20:57 scenarios that would also be extremely bad compatibly bad lot more likely to occur so one is just we are able to
01:21:06 they I so we're able to get it to do what we want it to do and perhaps does not like this fast takeoff of AI
01:21:14 capabilities within a single system it's just limited across many systems that do some different things but you do get
01:21:23 very rapid economic and technological progress as a result that concentrates power into the hands of a very small
01:21:29 number of individuals perhaps a single dictator and secondly that single individual is a small group of
01:21:37 individuals or single country is then able to like lock in their values indefinitely via transmitting those
01:21:45 values to artificial systems that have no reason to die like you know their code is copyable perhaps you know Donald
01:21:54 Trump or jumping create their kind of AI progeny in an image and once you have a system that's content once you have a
01:22:03 society that's controlled by AI you no longer have one of the main drivers of change historically which is the fact
01:22:11 that human life spans you know only a hundred years give or take that's really interesting so as opposed to sort of
01:22:19 killing off all humans is locking in and like creating a hell on earth basically a set of sense that set of principles
01:22:27 under which the society operates that's extremely undesirable so everybody is suffering indefinitely oh it doesn't I
01:22:33 mean it also doesn't need to be hell on earth it could just be the wrong values so we talked at the very beginning about
01:22:41 how I want to see this kind of diversity of different values and exploration so that we can just work out what is kind
01:22:47 of morally like what is good what is bad and then pursue the thing that's bad so actually so the idea of wrong values is
01:22:58 actually probably so the beautiful thing is there's no such thing as right and wrong values because we don't know the
01:23:02 right answer we just kind of have a sense of which value is more right which is more wrong so any kind of lock-in
01:23:11 makes a value wrong because it prevents exploration of this kind yeah and just you know imagine if fascist Val
01:23:19 you know imagine if there was Hitler's utopia or Stalin's utopia or Donald thumps or using pings forever yeah you
01:23:28 know how how good or bad would that be compared to the best possible future we could create and my suggestion is it
01:23:35 really suck compared to the best possible future when you're just one individual there are some individuals
01:23:44 for home Donald Trump is perhaps the best possible future and so that's the whole point of us individuals exploring
01:23:51 the space together exactly yeah and which trying to figure out which is the path that will make America great again
01:24:00 yeah exactly so how can effective altruism help I mean this is a really interesting notion they actually
01:24:07 describing of artificial intelligence being used as extremely powerful technology in the hands of very few
01:24:15 potentially one person yeah to create some very undesirable effect as opposed to AI and again the source of the
01:24:23 undesirable dust there is the human you know AI is just a really powerful tool so whether it's that or where their AI
01:24:30 is a GI just runs away from us completely how as individuals as as people and the effective altruism
01:24:40 movement how can we think about something like this understand poverty and their welfare but this is a far-out
01:24:46 incredibly mysterious and difficult problem okay well I think there's three paths as an individual so if you're
01:24:55 thinking about you know career paths you can pursue so one is going down the line of technical AI safety so this is most
01:25:05 relevant to the kind of AI winning AI taking over scenarios where this is just technical work on current machine
01:25:12 learning systems often sometimes going more theoretical to on how we can ensure that an AI is able to learn human values
01:25:20 and able to act in the way that you want it to act and that's a pretty mainstream issue and approach in machine learning
01:25:30 today so you know we definitely need more people doing that second is on the policy side of things
01:25:35 which i think is even more important at the moment which is how should developments in AI be managed on a
01:25:44 political level how can you ensure that the benefits of AI are very distributed it's not being powered isn't being
01:25:53 concentrated in the hands of a small set of individuals how do you ensure that on arms races between different AI
01:26:05 companies that might result in them you know cutting corners with respect to safety and so there the input as
01:26:11 individuals who can have is this we're not talking about money we're talking about effort we're talking about career
01:26:16 choice here we're talking about career choice yeah but then is the case that supposing you know you're like I've
01:26:22 already decided my career I'm doing something quite different you can contribute with money to where at the
01:26:28 center of effective altruism we set up the long term future fund so if you go on to effective altruism dog you can
01:26:37 donate we're a group of individuals will then work out what's the highest value place they can donate to work on
01:26:45 accidental risk issues with a particular focus on AI and what's bad number three this would path number three this was
01:26:51 that donated donations were the third option I was thinking of okay and then yeah there are you can also donate
01:26:58 directly to organizations working on this like center for human compatible AI at Berkeley future of humanity Institute
01:27:08 at Oxford or other organizations to does AI keep you up at night this kind of concern yeah it's kind of a mix where I
01:27:17 think it's very likely things are gonna go well I think we're gonna be able to solve these problems
01:27:23 I think that's by far the most likely outcome at least overnight far the most likely so if you look at all the
01:27:30 trajectories running away from our current moment in the next hundred years you see AI creating destructive
01:27:40 consequences as a small subset of those possible futures well at least yeah it kind of eternal the Selective consequence
01:27:47 I think that being a small subset at the same time it still freaks me out I mean when we're talking about the entire
01:27:53 future of civilization then small probabilities you know 1% probability that's terrifying what do you think
01:28:02 about Elon Musk's strong worry that we should be really concerned about existential risk so they I yeah I mean I
01:28:08 think you know broadly speaking I think he's right I think if we took if we talked we would probably have very
01:28:15 different probabilities on how likely it is that we're doomed but again when it comes to talking about the entire future
01:28:21 of civilization it doesn't really matter if it's 1% or if it's 50% we ought to be taking every possible safeguard we can
01:28:28 to ensure that things go well loved and poorly last question if you yourself could radical one problem from the world
01:28:36 or would that problem be that's a great question I don't know if I'm cheating in saying this but I think the thing I
01:28:44 would most want to change is just the fact that people don't actually care about insuring the long-run future go as
01:28:51 well people don't really care about future generations they don't think about it it's not part of their aims
01:28:56 well in some sense you're not cheating at all because in speaking the way you do in writing the things you're writing
01:29:03 you're doing you're addressing exactly this aspect exactly that that is your input into the into the effective
01:29:12 altruism movement so for that well thank you so much so now I'm gonna talk to you that really enjoyed it thanks so much
01:29:17 for having me thanks for listening to this conversation with William McCaskill
01:29:22 thank you to our presenting sponsor cash app please consider supporting the podcast by downloading cash app and
01:29:30 using code Lex podcast if you enjoy this podcast subscribe on youtube review it with five stars sign up a podcast
01:29:36 supported on patreon or simply connect with me on Twitter at Lex Friedman and now let me leave you with some words
01:29:45 from William McCaskill one additional unit of income can do a hundred times as much to benefit the extreme poor as it
01:29:53 can to benefit you or I earning the typical us wage of twenty eight thousand dollars a year
01:29:58 it's not often that you have two options one of which is a hundred times better than the other imagine a happy hour
01:30:05 where you can either buy yourself a beer for five dollars or by someone else a beer for five cents if that were the
01:30:12 case would probably be pretty generous next rounds on me but that's effectively the situation
01:30:20 were in all the time it's like a ninety nine percent off sale or buy one get ninety-nine free it might be the most
