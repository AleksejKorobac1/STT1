00:00:01 the following is a conversation with Jana kun he's considered to be one of the fathers of deep learning which if
00:00:09 you've been hiding under a rock is the recent revolution in AI that's captivated the world with the
00:00:14 possibility of what machines can learn from data he's a professor in New York University a vice president and chief AI
00:00:23 scientist a Facebook & Co recipient of the Turing Award for his work on deep learning he's probably best known as the
00:00:29 founding father of convolutional neural networks in particular their application to optical character recognition and the
00:00:38 famed M NIST data set he is also an outspoken personality unafraid to speak his mind in a distinctive French accent
00:00:46 and explore provocative ideas both in the rigorous medium of academic research and the somewhat less rigorous medium of
00:00:53 Twitter and Facebook this is the artificial intelligence podcast if you enjoy it subscribe on YouTube give it
00:01:00 five stars on iTunes support and on patreon we're simply gonna equip me on Twitter Alex Friedman spelled the Fri D
00:01:08 ma N and now here's my conversation with Yann Laocoon you said that 2001 Space Odyssey is one of your favorite movies
00:01:19 Hal 9000 decides to get rid of the astronauts for people haven't seen the movie spoiler alert because he it she
00:01:29 believes that the astronauts they will interfere with the mission do you see how is flawed in some fundamental way or
00:01:37 even evil or did he do the right thing neither there's no notion of evil in that in that context other than the fact
00:01:46 that people die but it was an example of what people call value misalignment right you give an objective to a machine
00:01:54 and the Machine strives to achieve this objective and if you don't put any constraints on this objective like don't
00:02:00 kill people and don't do things like this the Machine given the power will do stupid things just to achieve this dis
00:02:09 objective or damaging things to achieve its objective it's a little bit like we are used to this in the context of human
00:02:20 society we we put in place laws to prevent people from doing bad things because fantasy did we do those bad
00:02:26 things right so we have to shave their cost function the objective function if you want through laws to kind of correct
00:02:33 an education obviously to sort of correct for for those so maybe just pushing a little further on on that
00:02:44 point how you know there's a mission there's a this fuzziness around the ambiguity around what the actual mission
00:02:53 is but you know do you think that there will be a time from a utilitarian perspective or an AI system where it is
00:03:00 not misalignment where it is alignment for the greater good of society that kneei system will make decisions that
00:03:06 are difficult well that's the trick I mean eventually we'll have to figure out how to do this and again we're not
00:03:13 starting from scratch because we've been doing this with humans for four millennia so designing objective functions for
00:03:20 people is something that we know how to do and we don't do it by you know programming things although the legal
00:03:30 code is called code so that tells you something and it's actually the design of an object you function that's really
00:03:35 what legal code is right it tells you you can do it what you can't do if you do it you pay that much that's that's an
00:03:43 objective function so there is this idea somehow that it's a new thing for people to try to design objective functions are
00:03:48 aligned with the common good but no we've been writing laws for millennia and that's exactly what it is
00:03:54 so this that's where you know the science of lawmaking and and computer science will come together will come
00:04:04 together so it's nothing there's nothing special about how or a I systems is just the continuation of tools used to make
00:04:10 some of these difficult ethical judgments that laws make yeah and we and we have systems like this already that
00:04:18 you know make many decisions for ourselves in society that you know need to be designed in a way that they like
00:04:24 you know rules about things that sometimes sometimes have bad side effects and we have to be flexible
00:04:30 enough about those rules so that they can be broken when it's obvious that they shouldn't be applied so you don't
00:04:36 see this on the camera here but all the decorations in this room is all pictures from 2001 a Space Odyssey Wow
00:04:43 and by accident or is there a lot about accident it's by design Wow so if you were if you were to build hell 10,000 so
00:04:55 an improvement of Hal 9000 what would you improve well first of all I wouldn't ask you to hold secrets and tell lies
00:05:03 because that's really what breaks it in the end that's the the fact that it's asking itself questions about the
00:05:09 purpose of the mission and it's you know pieces things together that it's heard you know all the secrecy of the
00:05:14 preparation of the mission and the fact that it was discovery and on the lunar surface that really was kept secret and
00:05:22 and one part of Hal's memory knows this and the other part is does not know it and it's supposed to not tell anyone and
00:05:28 that creates a internal conflict do you think there's never should be a set of things that night AI system should not
00:05:38 be allowed like a set of facts that should not be shared with the human operators well I think no I think the I
00:05:46 think it should be a bit like in the design of autonomous AI systems there should be the equivalent of you know the
00:05:58 the the oath that hypocrite Oh calm yourself yeah that doctors sign up to right so the certain thing certain rule
00:06:05 said that that you have to abide by and we can sort of hardwire this into into our into our machines to kind of make
00:06:11 sure they don't go so I'm not you know advocate of the the 303 dollars of Robotics you know the as you move kind
00:06:18 of thing because I don't think it's practical but but you know some some level of of limits but but to be clear
00:06:28 this is not these are not questions that are kind of really worth asking today because we just don't have the
00:06:35 technology to do this we don't we don't have a ton of missing teller machines we have intelligent machines so my
00:06:38 intelligent machines that are very specialized but they don't they don't really sort of satisfy an objective
00:06:45 they're just you know kind of trained to do one thing so until we have some idea for design of a full-fledged autonomous
00:06:54 intelligent system asking the question of how we design use objective I think is a little a little too abstract it's a
00:07:00 little tough rat there's useful elements to it in that it helps us understand our own ethical codes humans so even just as
00:07:11 a thought experiment if you imagine that in a GI system is here today how would we program it is a kind of nice thought
00:07:19 experiment of constructing how should we have a law have a system of laws far as humans it's just a nice practical tool and I
00:07:29 think there's echoes of that idea too in the AI systems left today it don't have to be that intelligent
00:07:35 yeah like autonomous vehicles there's these things start creeping in that were thinking about but certainly they
00:07:43 shouldn't be framed as as hell yeah looking back what is the most I'm sorry if it's a silly question but what is the
00:07:52 most beautiful or surprising idea and deep learning or AI in general that you've ever come across sort of
00:08:00 personally well you said back and and just had this kind of wow that's pretty cool moment that's nice well surprising I
00:08:07 don't know if it's an idea rather than a sort of empirical fact the fact that you gigantic neural nets trying to train
00:08:20 them on you know relatively small amounts of data relatively with the caste grid in the center that it
00:08:27 actually works breaks everything you read in every textbook right every pre deep learning textbook that told you you
00:08:34 need to have fewer parameters and you have data samples you know if you have non-convex objective function you have
00:08:40 no guarantee of convergence you know all the things that you read in textbook and they tell you stay away from this and
00:08:45 they were all wrong huge number of parameters non-convex and somehow which is very relative to the number of
00:08:54 parameters data it's able to learn anything right does that surprise you today well it it was kind of obvious to
00:09:02 me before I knew anything that that's that this is a good idea and then it became surprising that it worked
00:09:07 because I started reading those text books okay so okay you talk to the intuition of why was obviously if you
00:09:15 remember well okay so the intuition was it's it's sort of like you know those people in the late 19th century who
00:09:23 proved that heavier than than air flight was impossible right and of course you have birds right they do fly and so on
00:09:31 the face of it it it's obviously wrong as an empirical question right and so we have the same kind of thing that you
00:09:38 know the we know that the brain works we don't know how but we know it works and we know it's a large network of neurons
00:09:43 and interaction and the learning takes place by changing the connection so kind of getting this level of inspiration
00:09:49 without copying the details but sort of you know that kind of gives you a clue as to which direction to go there's also
00:10:00 the idea somehow that I've been convinced of since I was an undergrad that even before that intelligence is
00:10:08 inseparable from running so you the idea somehow that you can create an intelligent machine by basically
00:10:16 programming for me was a non-starter you know from the start every intelligent entity that we know about arrives at
00:10:25 this intelligence to learning so learning you know machine learning was completely obvious path also because I'm
00:10:34 lazy so you know it's automate basically everything and learning is the automation of intelligence right so do
00:10:43 you think so what is learning then what what falls under learning because do you think of reasoning is learning where
00:10:52 reasoning is certainly a consequence of learning as well just like other functions of of the brain
00:10:59 the big question about reasoning is how do you make reasoning compatible with gradient based learning do you think
00:11:05 neural networks can be made to reason yes that there's no question about that again we have a good example right the
00:11:12 question is is how so the question is how much prior structure you have to put in the neural net so that something like
00:11:19 human reasoning will emerge from it you know from running another question is all of our kind of model of what
00:11:27 reasoning is that are based on logic are discrete and and and are therefore incompatible with gradient based
00:11:34 learning and I was very strong believer in this idea Grandin baserunning I don't believe that other types of learning
00:11:41 that don't use kind of gradient information if you want so you don't like discrete mathematics you don't like
00:11:45 anything discrete well that's it's not that I don't like it it's just that it's it's incompatible
00:11:51 with learning and I'm a big fan of running right so in fact that's perhaps one reason why deep learning has been
00:11:58 kind of looked at with suspicion by a lot of computer scientists because the math is very different the method you
00:12:03 use for deep running you know we kind of as more to do with you know cybernetics the kind of math you do in electrical
00:12:11 engineering then the kind of math you doing computer science and and you know nothing in in machine learning is exact
00:12:18 right computer science is all about sort of you know obviously compulsive attention to details of like you know
00:12:24 every index has to be right and you can prove that an algorithm is correct right machine learning is the science of
00:12:34 sloppiness really that's beautiful so okay maybe let's feel around in the dark of what is a neural network that reasons
00:12:45 or a system that is works with continuous functions that's able to do build knowledge however we think about
00:12:55 reasoning builds on previous knowledge build on extra knowledge create new knowledge generalized outside of any
00:13:03 training set ever built what does that look like if yeah maybe do you have Inklings of thoughts of what
00:13:11 that might look like well yeah I mean yes or no if I had precise ideas about this I think you know we'd be building
00:13:17 it right now but and there are people working on this or whose main research interest is actually exactly that right
00:13:24 so what you need to have is a working memory so you need to have some device if you want some subsystem they can
00:13:34 store a relatively large number of factual episodic information for you know a reasonable amount of time so you
00:13:43 you know in the in the brain for example it kind of three main types of memory one is the sort of memory of the the
00:13:54 state of your cortex and that sort of disappears within 20 seconds you can't remember things for more than about 20
00:14:00 seconds or a minute if if you don't have any other form of memory the second type of memory which is longer term is short
00:14:07 term is the hippocampus so you can you know you came into this building you remember whether where the the exit is
00:14:14 where the elevators are you have some map of that building that's stored in your hippocampus you might remember
00:14:20 something about what I said you know if you minutes ago and forgot all our stars being raised but you know but that does
00:14:27 not work in your hippocampus and then the the longer term memory is in the synapse the synapses right so what you
00:14:34 need if you want for a system that's capable reasoning is that you want the hippocampus like thing right and that's
00:14:42 what people have tried to do with memory networks and you know no Turing machines and stuff like that right and and now
00:14:48 with transformers which have sort of a memory in their kind of self attention system you can you can think of it this
00:14:56 way so so that's one element you need another thing you need is some sort of network that can access this memory get
00:15:07 an information back and then kind of crunch on it and then do this iteratively multiple times because a
00:15:16 chain of reasoning is a process by which you you you can you update your knowledge about the state of the world
00:15:22 about you know what's gonna happen etc and that there has to be this sort of recurrent operation basically and you
00:15:29 think that kind of if we think about a transformer so that seems to be too small to contain the knowledge that's
00:15:37 that's to represent the knowledge as containing Wikipedia for example but transformer doesn't have this idea of
00:15:42 recurrence it's got a fixed number of layers and that's number of steps that you know limits basically it's a
00:15:48 representation but recurrence would build on the knowledge somehow I mean yeah it would evolve the knowledge and
00:15:57 expand the amount of information perhaps or useful information within that knowledge yeah but is this something
00:16:04 that just can emerge with size because it seems like everything we have now is just no it's not it's not it's not clear
00:16:11 how you access and right into an associative memory in efficient way I mean sort of the original memory network
00:16:16 maybe had something like the right architecture but if you try to scale up a memory network so that the memory
00:16:23 contains all we keep here it doesn't quite work right so so this is a need for new ideas there okay but it's not
00:16:30 the only form of reasoning so there's another form of reasoning which is true which is very classical so in
00:16:39 some types of AI and it's based on let's call it energy minimization okay so you have some sort of objective some energy
00:16:50 function that represents the the the quality or the negative quality okay energy goes up when things get bad and
00:16:57 they get low when things get good so let's say you you want to figure out you know what gestures do I need to to do to
00:17:08 grab an object or walk out the door if you have a good model of your own body a good model of the environment using this
00:17:14 kind of energy minimization you can make a you can make you can do planning and it's in optimal control it's called it's
00:17:21 called Marie put model predictive control you have a model of what's gonna happen in the world as consequence for
00:17:27 your actions and that allows you to buy energy minimization figure out the sequence of action that optimizes a
00:17:32 particular objective function which measures you know minimize the number of times you're gonna hit something and the
00:17:37 energy gonna spend doing the gesture and etc so so that's performer reasoning planning is a form of reasoning and
00:17:46 perhaps what led to the ability of humans to reason is the fact that or you know species you know that appear before
00:17:55 us had to do some sort of planning to be able to hunt and survive and survive the winter in particular and so you know
00:18:02 it's the same capacity that you need to have so in your intuition is if you look at expert systems in encoding knowledge
00:18:15 as logic systems as graphs in this kind of way is not a useful way to think about knowledge graphs are your brittle
00:18:24 or logic representation so basically you know variables that that have values and constraint between them that are
00:18:32 represented by rules as well too rigid and too brittle right so one of the you know some of the early efforts in that
00:18:40 respect were were to put probabilities on them so a rule you know you know if you have this in that symptom you know
00:18:46 you have this disease with that probability and you should describe that antibiotic with that
00:18:51 probability right this my sin system from the for the 70s and that that's what that branch of AI led to you know
00:19:00 busy networks in graphical models and causal inference and vibrational you know method so so there there is I mean
00:19:10 certainly a lot of interesting work going on in this area the main issue with this is is knowledge acquisition
00:19:17 how do you reduce a bunch of data to graph of this type near relies on the expert and a human being to encode at
00:19:26 add knowledge and that's essentially impractical yeah the question the second question is do you want to represent
00:19:35 knowledge symbols and you want to manipulate them with logic and again that's incomparable we're learning so
00:19:43 one suggestion with geoff hinton has been advocating for many decades is replace symbols by vectors think of it
00:19:51 as pattern of activities in a bunch of neurons or units or whatever you wanna call them and replace logic by
00:19:59 continuous functions okay and that becomes now compatible there's a very good set of ideas by region in a
00:20:08 paper about 10 years ago by leon go to on who is here at face book the title of the paper is for machine learning to
00:20:17 machine reasoning and his idea is that learning learning system should be able to manipulate objects that are in the
00:20:23 same space in a space and then put the result back in the same space so is this idea of working memory basically and
00:20:31 it's a very enlightening and in the sense that might learn something like the simple expert systems
00:20:40 I mean it's with you can learn basic logic operations there yeah quite possibly yeah this is a big debate on
00:20:46 sort of how much prior structure you have to put in for this kind of stuff to emerge that's the debate I have with
00:20:51 Gary Marcus and people like that yeah yeah so and the other person so I just talked to judea pearl mm-hmm well you
00:21:00 mentioned causal inference world his worry is that the current knew all networks are not able to learn what
00:21:11 causes what causal inference between things so I think I think he's right and wrong about this if he's talking about
00:21:20 the sort of classic type of neural nets people also didn't worry too much about this but there's a lot of people now
00:21:26 working on causal inference and there's a paper that just came out last week by Leon Mbutu among others develop his path
00:21:32 and push for other people exactly on that problem of how do you kind of you know get a neural net to sort of pay
00:21:40 attention to real causal relationships which may also solve issues of bias in data and things like this so I'd like to
00:21:51 read that paper because that ultimately the challenges also seems to fall back on the human expert to ultimately decide
00:22:02 causality between things people are not very good at its direction causality first of all so first of all you talk to
00:22:07 a physicist and physicists actually don't believe in causality because look at the all the busy clause or
00:22:14 microphysics are time reversible so there is no causality the arrow of time is not right yeah it's it's as soon as
00:22:20 you start looking at macroscopic systems where there is unpredictable randomness where there is clearly an arrow of time
00:22:26 but it's a big mystery in physics actually well how that emerges is that emergent or is it part of the
00:22:34 fundamental fabric of reality yeah or is it bias of intelligent systems that you know because of the second law of
00:22:40 thermodynamics we perceive a particular arrow of time but in fact it's kind of arbitrary right so yeah physicists
00:22:47 mathematicians they don't care about I mean the math doesn't care about the flow of time well certainly certainly
00:22:54 macro physics doesn't people themselves are not very good at establishing causal causal relationships if you ask is I
00:23:02 think it was in one of Seymour Papert spoken on like children learning you know he studied with Jean Piaget you
00:23:10 know he's the guy who co-authored the book perceptron with Marvin Minsky that kind of killed the first wave
00:23:16 but but he was actually a learning person he in the sense of studying learning in humans and machines that's
00:23:23 what he got interested in for scepter on and he wrote that if you ask a little kid about what is the cause of the wind
00:23:35 a lot of kids will say they will think for a while and they'll say oh it's the the branches in the trees they move and
00:23:40 that creates wind right so they get the causal relationship backwards and it's because their understanding of the world
00:23:45 and intuitive physics is not that great right I mean these are like you know four or five year old kids you know it
00:23:51 gets better and then you understand that this it can't be right but there are many things which we can because of our
00:24:00 common sense understanding of things what people call common sense yeah and we understanding of physics we can
00:24:08 there's a lot of stuff that we can figure out causality even with diseases we can figure out what's not causing
00:24:15 what often there's a lot of mystery of course but the idea is that you should be able to encode that into systems it
00:24:21 seems unlikely to be able to figure that out themselves well whenever we can do intervention but you know all of
00:24:27 humanity has been completely deluded for millennia probably since existence about a very very wrong causal relationship
00:24:34 where whatever you can explain you attributed to you know some deity some divinity right and that's a cop-out
00:24:41 that's the way of saying like I don't know the cause so you know God did it right so you mentioned Marvin Minsky and
00:24:52 the irony of you know maybe causing the first day I winter you were there in the 90s you're there in the 80s of course in
00:24:59 the 90s what do you think people lost faith and deep learning in the 90s and found it again a decade later over a
00:25:07 decade later yeah it wasn't called dethroning yeah it was just called neural nets you know
00:25:14 yeah they lost interests I mean I think I would put that around 1995 at least the machine learning community there was
00:25:19 always a neural net community but it became disconnected from sort of ministry machine owning if you want
00:25:30 there were it was basically electrical engineering that kept at it and computer science just gave up give up on neural
00:25:39 nets I don't I don't know you know I was too close to it to really sort of analyze it with sort of a unbiased eye
00:25:49 if you want but I would I would I would would make a few guesses so the first one is at the time neural nets were it
00:25:58 was very hard to make them work in the sense that you would you know implement back prop in your favorite language and
00:26:07 that favorite language was not Python it was not MATLAB it was not any of those things cuz they didn't exist right you
00:26:13 had to write it in Fortran or C or something like this right so you would experiment with it you would probably
00:26:21 make some very basic mistakes like you know badly initialize your weights make the network too small because you read
00:26:25 in the textbook you know you don't want too many parameters right and of course you know and you would train on x4
00:26:30 because you didn't have any other data set to try it on and of course you know it works half the time so we'd say you
00:26:37 give up also 22 the batch gradient which you know isn't it sufficient so there's a lot of bag of tricks that you had to
00:26:45 know to make those things work or you had to reinvent and a lot of people just didn't and they just couldn't make it
00:26:53 work so that's one thing the investment in software platform to be able to kind of you know display things figure out
00:27:00 why things don't work and I get a good intuition for how to get them to work have enough flexibility so you can
00:27:05 create you know network architectures well completion ads and stuff like that it was hard yeah when you had to write
00:27:10 everything from scratch and again you didn't have any Python or MATLAB or anything right so what I read that sorry
00:27:17 to interrupt but I read he wrote in in Lisp the first versions of Lynette accomplished in your networks which by
00:27:24 the way one of my favorite languages that's how I knew you were legit the Turing Award whatever this would be
00:27:31 programmed and list that's still my favorite language but it's not that we programmed in Lisp it's that we had to
00:27:38 write or this printer printer okay cuz it's not that's right that's one that existed so
00:27:42 we wrote a lisp interpreter that we hooked up to you know back in library that we wrote also for neural net
00:27:50 competition and then after a few years around 1991 we invented this idea of basically having modules that know how
00:27:56 to forward propagate and back propagate gradients and then interconnecting those modules in a graph loom but who had made
00:28:04 proposals on this about this in the late 80s and were able to implement this using all this system eventually we
00:28:12 wanted to use that system to make build production code for character recognition at Bell Labs so we actually
00:28:16 wrote a compiler for that disp interpreter so that Christy Martin who is now Microsoft kind of did the bulk of
00:28:23 it with Leone and me and and so we could write our system in lisp and then compiled to seee and then we'll have a
00:28:29 self-contained complete system that could kind of do the entire thing neither Python or turn pro can do this
00:28:40 today yeah okay it's coming yeah I mean there's something like that in Whitehorse called you know tor script
00:28:46 and so you know we had to write or Lisp interpreter which retinol is compiler way to invest a huge amount of effort to
00:28:53 do this and not everybody if you don't completely believe in the concept you're not going to invest the time to
00:28:58 do this right now at the time also you know it were today this would turn into torture by torture and so for whatever
00:29:05 we put it in open-source everybody would use it and you know realize it's good back before 1995 working at AT&T there's
00:29:14 no way the lawyers would let you release anything in open source of this nature and so we could not distribute our code
00:29:22 really and at that point and sorry to go on a million tangents but on that point I also read that there was some almost
00:29:29 pad like a patent on convolution your network yes it was labs so that first of all I mean just to actually that ran out
00:29:45 the thankfully 8007 in 2007 that what look can we can we just talk about that first I know you're a facebook but
00:29:52 you're also done why you and and what does it mean patent ideas like these software ideas
00:30:01 essentially or what are mathematical ideas or what are they okay so they're not mathematical idea so there are you
00:30:08 know algorithms and there was a period where the US Patent Office would allow the patent of software as long as it was
00:30:17 embodied the Europeans are very different they don't they don't quite accept that they have a different
00:30:24 concept but you know I don't I know no I mean I never actually strongly believed in this but I don't believe in this kind
00:30:29 of patent Facebook basically doesn't Google Files patterns because they've been burned with Apple and so now they
00:30:41 do this for defensive purpose but usually they say we're not going to see you if you infringe Facebook has a
00:30:48 similar policy they say you know we file pattern on certain things for defensive purpose we're not going to see you if
00:30:53 you infringe unless you sue us so the the industry does not believe in in patterns they are there because of
00:31:02 you know the legal landscape and and and various things but but I don't really believe in patterns for this kind of
00:31:09 stuff yes so that's that's a great thing so I tell you a war story yeah you so what happens was the the first the first
00:31:15 pattern of a condition that was about kind of the early version Congress on that that didn't have separate pudding
00:31:22 layers it had the conditional layers which tried more than one if you want right and then there was a second one on
00:31:29 commercial nets with separate pudding layers train with back probably in 89 and 1992 something like this at the time the life
00:31:40 life of a pattern was 17 years so here's what happened over the next few years is that we started developing character
00:31:47 recognition technology around commercial Nets and in 1994 a check reading system was deployed in ATM machines in 1995 it was
00:31:59 for a large check reading machines in back offices etc and those systems were developed by an engineering group that
00:32:07 we were collaborating with AT&T and they were commercialized by NCR which at the time was a subsidiary of AT&T now it
00:32:15 ain't he split up in 1996 99 in 1996 and the lawyers just looked at all the patterns and they distributed
00:32:22 the patterns among the various companies they gave the the commercial net pattern to NCR because they were actually
00:32:29 selling products that used it but nobody I didn't see are at any idea where they come from that was yeah okay so between
00:32:38 1996 and 2007 there's a whole period until 2002 I didn't actually work on machine on your
00:32:43 couch on that I resumed working on this around 2002 and between 2002 and 2007 I was working on them crossing my finger
00:32:51 that nobody and NCR would notice nobody noticed yeah and I and I hope that this kind of somewhat as you said lawyers
00:33:00 decide relative openness of the community now will continue it accelerates the entire progress of
00:33:09 the industry and you know the problems that Facebook and Google and others are facing today is not whether Facebook or
00:33:16 Google or Microsoft or IBM or whoever is ahead of the other it's that we don't have the technology to build the things
00:33:21 we want to build we only build intelligent virtual systems that have common sense we don't have a monopoly on
00:33:27 good ideas for this we don't believe with you maybe others do believe they do but we don't okay if a start-up tells
00:33:34 you they have the secret to you know human level intelligence and common sense don't believe them they don't and
00:33:42 it's going to take the entire work of the world research community for a while to get to the point where you can go off
00:33:49 and in each of the company is going to start to build things on this we're not there yet
00:33:53 it's absolutely in this this calls to the the gap between the space of ideas and the rigorous testing of those ideas
00:34:02 of practical application that you often speak to you've written advice saying don't get fooled by people who claim to
00:34:09 have a solution to artificial general intelligence who claim to have an AI system that work just like the human
00:34:15 brain or who claim to have figured out how the brain works ask them what the error rate they get on em 'no store
00:34:24 imagenet this is a little dated by the way that mean five years who's counting okay but i think your opinion it's the
00:34:34 Amna stand imagenet yes may be data there may be new benchmarks right but i think that philosophy is one you still
00:34:43 and and somewhat hold that benchmarks and the practical testing the practical application is where you really get to
00:34:48 test the ideas well it may not be completely practical like for example you know it could be a toy data set
00:34:54 but it has to be some sort of task that the community as a whole has accepted as some sort of standard you know kind of
00:35:01 benchmark if you want it doesn't need to be real so for example many years ago here at fair people you know chosen
00:35:07 Western art one born and a few others proposed the the babbitt asks which were kind of a toy problem to test the
00:35:14 ability of machines to reason actually to access working memory and things like this and it was very useful even though
00:35:20 it wasn't a real task amnesties kind of halfway a real task so you know toy problems can be very useful it's just
00:35:28 that i was really struck by the fact that a lot of people particularly our people with money to invest would be
00:35:34 fooled by people telling them oh we have you know the algorithm of the cortex and you should give us 50 million yes
00:35:42 absolutely so there's a lot of people who who tried to take advantage of the hype for business reasons and so on but
00:35:52 let me sort of talk to this idea that new ideas the ideas that push the field forward may not yet have a benchmark or it may
00:36:00 be very difficult to establish a benchmark I agree that's part of the process establishing benchmarks is part
00:36:06 of the process so what are your thoughts about so we have these benchmarks on around stuff we can do with images from
00:36:15 classification to captioning to just every kind of information can pull off from images and the surface level
00:36:20 there's audio datasets there's some video what can we start natural language what kind of stuff what kind of
00:36:29 benchmarks do you see they start creeping on to more something like intelligence like reasoning like maybe
00:36:38 you don't like the term but AGI echoes of that kind of yeah sort of elation a lot of people are working on interactive
00:36:45 environments in which you can you can train and test intelligent systems so so there for example you know it's the
00:36:56 classical paradigm of supervised running is that you you have a data set you partition it into a training site
00:37:01 validation set test set and there's a clear protocol right but what if the that assumes that this
00:37:08 apples are statistically independent you can exchange them the order in which you see them doesn't shouldn't matter you
00:37:14 know things like that but what if the answer you give determines the next sample you see which is the case for
00:37:20 example in robotics right you robot does something and then it gets exposed to a new room and depending on where it goes
00:37:26 the room would be different so that's the decrease the exploration problem the what if the samples so that creates
00:37:35 also a dependency between samples right you you if you move if you can only move it in in space the next sample you're
00:37:41 gonna see is going to be probably in the same building most likely so so so the all the assumptions about the validity
00:37:50 of this training set test set a potus's break whatever a machine can take an action that has an influence in the in
00:37:56 the world and it's what is going to see so people are setting up artificial environments where what that takes place
00:38:05 right the robot runs around a 3d model of a house and can interact with objects and things like this how you do robotics
00:38:12 by simulation you have those you know opening a gym type thing or mu Joko kind of simulated robots and you have games
00:38:21 you know things like that so that that's where the field is going really this kind of environment now back to the
00:38:29 question of a GI like I don't like the term a GI because it implies that human intelligence is general and human
00:38:38 intelligence is nothing like general it's very very specialized we think it's general we'd like to think of ourselves
00:38:44 as having your own science we don't we're very specialized we're only slightly more general than why does it
00:38:50 feel general so you kind of the term general I think what's impressive about humans is ability to learn as we were
00:38:59 talking about learning to learn in just so many different domains is perhaps not arbitrarily general but just you can
00:39:07 learn in many domains and integrate that knowledge somehow okay that knowledge persists so let me take a very specific
00:39:13 example yes it's not an example it's more like a a quasi mathematical demonstration so you have about 1
00:39:19 million fibers coming out of one of your eyes okay two million total but let's let's talk about just one of
00:39:25 them it's 1 million nerve fibers your optical nerve let's imagine that they are binary so they can be active or
00:39:31 inactive right so the input to your now they connected to your brain in a particular way on your brain has
00:39:43 connections that are kind of a little bit like accomplish on that they're kind of local you know in space and things
00:39:48 like this I imagine I play a trick on you it's a pretty nasty trick I admit I I cut your optical nerve and I put a
00:39:57 device that makes a random perturbation of a permutation of all the nerve fibers so now what comes to your to your brain
00:40:07 is a fixed but random permutation of all the pixels there's no way in hell that your visual cortex even if I do this to
00:40:16 you in infancy will actually learn vision to the same level of quality that you can got it and you're saying there's
00:40:23 no way you ever learn that no because now two pixels that on your body in the world will end up in very different
00:40:29 places in your visual cortex and your neurons there have no connections with each other because they only connect it
00:40:35 locally so this whole our entire the hardware is built in many ways to support the locality of the real world
00:40:42 yeah yes that's specialization yep okay it's still now really damn impressive so it's not perfect generalization I even
00:40:49 closed no no it's it's it's it's not that it's not even close it's not at all yes it's socialize so how many boolean
00:40:55 functions so let's imagine you want to train your visual system to you know recognize particular patterns of those 1
00:41:04 million bits ok so that's a boolean function right either the pattern is here or not here this is a to to a
00:41:10 classification with 1 million binary inputs how many such boolean functions are there okay if you have 2 to the 1
00:41:21 million combinations of inputs for each of those you have an output bit and so you have 2 to the 2 to the 1 million
00:41:29 boolean functions of this type okay which is an unimaginably large number how many of those functions can actually
00:41:36 be computed by your visual cortex and the answer is a tiny tiny tiny tiny tiny tiny sliver like an enormous little tiny
00:41:46 sliver yeah yeah so we are ridiculously specialized you know okay but okay that's an argument against the word
00:41:56 general I think there's there's a I there's I agree with your intuition but I'm not sure it's it seems the breath
00:42:07 the the brain is impressively capable of adjusting to things so it's because we can't imagine tasks that are outside of
00:42:17 our comprehension right we think we think we are general because we're general of all the things that we can
00:42:22 apprehend so yeah but there is a huge world out there of things that we have no idea
00:42:27 we call that heat by the way heat heat so at least physicists call that heat or they call it entropy which is kokkonen
00:42:39 you have a thing full of gas right call system for gas right goes on a coast it has you know pressure it has temperature
00:42:50 has you know and you can write the equations PV equal NRT you know things like that right when you reduce a volume
00:42:57 the temperature goes up the pressure goes up you know things like that right for perfect gas at least those are the
00:43:05 things you can know about that system and it's a tiny tiny number of bits compared to the complete information of
00:43:10 the state of the entire system because the state when HR system will give you the position and momentum of every every
00:43:19 molecule of the gas and what you don't know about it is the entropy and you interpret it as heat the energy
00:43:27 containing that thing is is what we call heat now it's very possible that in fact there is some very strong structure in
00:43:35 how those molecules are moving is just that they are in a way that we are just not wired to perceive they are ignorant
00:43:42 to it and there's in your infinite amount of things we're not wired to perceive any right that's a nice way to
00:43:47 put it well general to all the things we can imagine which is a very tiny a subset of
00:43:55 all things that are possible it was like coma growth complexity or the coma was charged in some one of complexity you
00:44:03 know every bit string or every integer is random except for all the ones that you can actually write down yeah okay so
00:44:13 beautifully put but you know so we can just call it artificial intelligence we don't need to have a general whatever novel
00:44:20 human of all Nutella transmissible oh you know you'll start anytime you touch human it gets it gets interesting
00:44:32 because you know it's just because we attach ourselves to human and it's difficult to define with human
00:44:36 intelligences yeah nevertheless my definition is maybe damn impressive intelligence ok damn
00:44:45 impressive demonstration of intelligence whatever and so on that topic most successes in deep learning have been in
00:44:56 supervised learning what is your view on unsupervised learning is there a hope to reduce involvement of human input and
00:45:05 still have successful systems that are have practically used yeah I mean there's definitely a hope is it's more
00:45:12 than a hope actually it's it's you know mounting evidence for it and that's basically or I do like the only thing
00:45:19 I'm interested in at the moment is I call it self supervised running not unsupervised cuz unsupervised running is
00:45:26 a loaded term people who know something about machine learning you know tell us how you doing clustering or PCA yeah
00:45:32 she's nice and the way public we know when you say enterprise only oh my god you know machines are gonna learn by
00:45:37 themselves and without supervision you know there's the parents yeah so so I could sell supervised learning because
00:45:45 in fact the underlying algorithms that I use are the same algorithms as the supervised learning algorithms except
00:45:52 that what we trained them to do is not predict a particular set of variables like the category of an image and and
00:46:02 not to predict a set of variables that have been provided by human labelers but what you're trying to machine to do is
00:46:09 basically reconstruct a piece of its input that it's being this being masked masked out essentially you can think of
00:46:17 it this way right so show a piece of a video to a machine and ask it to predict what's gonna happen next and of course
00:46:23 after a while you can show what what happens and the machine will kind of train itself to do better at that task
00:46:31 you can do like all the latest most successful models the natural language processing use cell supervised running
00:46:38 you know sort of bird style systems for example right you show it a window of a thousand words on a test corpus you take
00:46:47 out 15% of the words and then you train a machine to predict the words that are missing that's out supervised running
00:46:54 it's not predicting the future it's just you know predicting things in middle but you could have you predict the future
00:46:59 that's what language models do so you construct it so in an unsupervised way you construct a model of language do you
00:47:07 think or video or the physical world or whatever right how far do you think that can take us do you think very far it
00:47:18 understands anything to some level it has you know a shallow understanding of of text but it needs to I mean to have
00:47:26 kind of true human level intelligence I think you need to ground language in reality so some people are attempting to
00:47:34 do this right having systems that can I have some visual representation of what what is being talked about which is one
00:47:39 reason you need interactive environments actually this is like a huge technical problem that is not solved and that
00:47:47 explains why such super versioning works in the context of natural language that does not work in the context on at least
00:47:53 not well in the context of image recognition and video although it's making progress quickly and the reason
00:48:02 that reason is the fact that it's much easier to represent uncertainty in the prediction you know context of natural
00:48:08 language than it is in the context of things like video and images so for example if I ask you to predict what
00:48:14 words are missing you know 15 percent of the words that I've taken out the possibility is small that means small
00:48:22 right there is 100,000 words in the in the lexicon and what the Machine spits out is a big probability vector right
00:48:29 it's a bunch of numbers between 0 & 1 that's 1 to 1 and we know how to do how to do this with computers so they are
00:48:36 representing uncertainty in the prediction is relatively easy and that's in my opinion why those techniques work
00:48:45 for NLP for images if you ask if you block a piece of an image and you as a system reconstruct that piece of the image
00:48:51 there are many possible answers there are all perfectly legit right and how do you represent that the set of possible answers
00:49:00 you can't train a system to make one prediction you can train a neural net to say here it is that's the image because
00:49:06 it's there's a whole set of things that are compatible with it so how do you get the machine to represent not a single
00:49:14 output but all set of outputs and you know similarly with video prediction there's a lot of things that can happen
00:49:20 in the future video you're looking at me right now I'm not moving my head very much but you know I might you know what
00:49:25 turn my my head to the left or to the right right if you don't have a system that can predict this and you train it
00:49:32 with least Square to kind of minimize the error with the prediction and what I'm doing
00:49:36 what you get is a blurry image of myself in all possible future positions that I might be in which is not a good
00:49:42 prediction but so there might be other ways to do the self supervision right for visual scenes like what if i I mean
00:49:52 if I knew I wouldn't tell you publish it first I don't know I know there might be so I mean these are kind
00:50:01 of there might be artificial ways of like self play in games the way you can simulate part of the environment you can
00:50:06 oh that doesn't solve the problem it's just a way of generating data but because you have more of a country might
00:50:15 mean you can control yeah it's a way to generate data and that's right and because you can do huge amounts of data
00:50:21 generation that doesn't you write this well it's it's a creeps up on the problem from the side of data and you
00:50:27 don't think that's the right way to it doesn't solve this problem of handling uncertainty in the world right so if you
00:50:34 if you have a machine learn a predictive model of the world in a game that is deterministic or quasi deterministic
00:50:43 it's easy right just you know give a few frames of the game to a combat put a bunch of layers and then half the game
00:50:50 generates the next few frames and and if the game is deterministic it works fine and that includes you know feeding the
00:51:00 system with the action that your little character is going to take the problem comes from the fact that the
00:51:08 real world and certain most games are not entirely predictable that's what they're you get those blurry predictions
00:51:12 and you can't do planning with very predictions all right so if you have a perfect model of the world you can in
00:51:21 your head run this model with a hypothesis for a sequence of actions and you're going to predict the outcome of
00:51:28 that sequence of actions but if your model is imperfect how can you plan yeah it quickly explodes what are your
00:51:37 thoughts on the extension of this which topic I'm super excited about it's connected to something you're talking
00:51:43 about in terms of robotics is active learning so as opposed to sort of unemployed and supervisors self
00:51:52 supervised learning you ask the system for human help right for selecting parts you want annotated next so if you talk
00:52:00 about a robot exploring a space or a baby exploring a space or a system exploring a data set every once in a
00:52:08 while asking for human input you see value in that kind of work I don't see transformative value it's going to make
00:52:18 things that we can already do more efficient or they will learn slightly more efficiently but it's not going to
00:52:23 make machines sort of significantly more intelligent I think and I and by the way there is no opposition there is no
00:52:34 conflict between self supervisor on reinforcement learning and supervisor on your imitation learning or active learning
00:52:41 I see sub super wrestling as a as a preliminary to all of the above yes so the example I use very often is how is
00:52:55 enforcement running deep enforcement running if you want the best methods today was so-called model free
00:53:03 enforcement training to learn to play Atari games take about 80 hours of training to reach the level that any
00:53:12 human can reach in about 15 minutes they get better than humans but it takes a long time alpha star okay the you know
00:53:23 are your videos and his team's the system to play to to play Starcraft plays you know a single map a single
00:53:34 type of player and which better than human level is about the equivalent of 200 years of training
00:53:45 playing against itself it's 200 years right it's not something that no no human can could every I'm not sure what
00:53:52 it doesn't take away from that okay now take those algorithms the best our algorithms we have today to train a car
00:54:01 to drive itself it would probably have to drive millions of hours you will have to kill thousands of pedestrians it will
00:54:07 have to run into thousands of trees it will have to run off cliffs and you had to run the cliff multiple times before
00:54:14 it figures out it's a bad idea first of all yeah and second of all the figures that had not to do it and so I mean this type
00:54:21 of running obviously does not reflect the kind of running that animals and humans do there is something missing
00:54:26 that's really really important there and my apart is is which have been advocating for like five years now is
00:54:34 that we have predictive models of the world that include the ability to predict under uncertainty and what
00:54:44 allows us to not run off a cliff when we learn to drive most of us can learn to drive in about 20 or 30 hours of
00:54:50 training without ever crashing causing any accident if we drive next to a cliff we know that if we turn the wheel to the
00:54:57 right the car is going to run off the cliff and nothing good is gonna come out of this because we have a pretty good
00:55:02 model of intuitive physics that tells us you know the car is gonna fall we know we know about gravity babies run this
00:55:07 around the age of eight or nine months that objects don't float they fall and you know we have a pretty good idea of
00:55:15 the effect of turning the wheel of the car and you know we know we need to stay on the road so there is a lot of things
00:55:20 that we bring to the table which is basically or predictive model of the world and that model allows us to not do
00:55:28 stupid things and to basically stay within the context of things we need to do we still face you know unpredictable
00:55:35 situations and that's how we learn but that allows us to learn really really really quickly so that's called
00:55:42 model-based reinforcement running there's some imitation and supervised running because we have a driving
00:55:47 instructor that tells us occasionally what to do but most of the learning is Mauro bass is learning the model yeah
00:55:55 running physics that we've done since we were babies that's where all almost all are learning and the physics is somewhat
00:56:02 transferable from is transferable from scene to scene stupid things are the same everywhere yeah I mean if you you
00:56:08 know you have experience of the world you don't need to be particularly from a particularly intelligent species to know
00:56:16 that if you spill water from a container you know the rest is gonna get wet and you might get wet so you know cats know
00:56:26 this right yeah so the main problem we need to solve is how do we learn models of the world that's and that's what I'm
00:56:32 interesting that's what's a supervised learning is all about if you were to try to construct a benchmark for let's let's
00:56:41 look at happiness I'd love that dataset but if you do you think it's useful interesting / possible to perform well
00:56:52 on eminence with just one example of each digit and how would we solve that problem yeah so it's probably yes the question
00:57:02 is what other type of running are you allowed to do so if what you like to do is train on some gigantic data set of
00:57:07 labelled digit that's called transfer running and we know that works okay we do this at Facebook like in
00:57:15 production right we we train large commercial nets to predict hashtags that people type on Instagram and we train on
00:57:20 billions of images literally billions and and then we chop off the last layer and fine-tune on whatever task we want
00:57:27 that works really well you can be you know the image net record with we actually open source the whole thing
00:57:32 like a few weeks ago yeah that's still pretty cool but yeah so what in yet won't be impressive and what's useful an
00:57:38 impressive what kind of transfer learning would be useful impressive is it Wikipedia that kind of thing no no I
00:57:45 don't think transfer learning is really where we should focus we should try to do you know have a kind of scenario for
00:57:54 benchmark where you have only ball data and you can and it's very large number of enabled data it could be video clips
00:58:04 it could be what you do you know frame prediction it could be images you could choose to you know mask a piece of it it
00:58:13 could be whatever but they're only bold and you're not allowed to label them so you do some training on this and then
00:58:23 you train on a particular supervised task imagenet or nist and you measure how your test our decrease or variation
00:58:32 error decreases as you increase the number of label training samples okay and and what what you would like to see
00:58:43 is is that you know your your error decreases much faster than if you trained from scratch from random weights
00:58:49 so that to reach the same level of performance and a completely supervised purely supervised system would reach you
00:58:56 would need way fewer samples so that's the crucial question because it will answer the question to like you know
00:59:01 people are interested in medical image analysis okay you know if I want to get to a particular level of error rate for
00:59:11 this task I know I need a million samples can I do you know soft supervised pre-training to reduce this
00:59:18 to about 100 or something anything the answer there is soft supervised retraining yep some form some form of it
00:59:27 telling you active learning but you disagree you know it's not useless it's just not gonna lead to a quantum leap
00:59:33 it's just gonna make things that we already do so you're way smarter than me I just disagree with you but I don't
00:59:39 have anything to back that it's just intuition so I've worked a lot of large-scale data sets and there's
00:59:45 something there might be magic and active learning but okay at least I said it publicly at least some being an idea
00:59:54 publicly okay it's not bigoted yet it's you know working with the data you have I mean I mean certainly people are doing
00:59:59 things like okay I have three thousand hours of you know imitation running for in car but most of those are incredibly
01:00:07 boring what I like is select you know 10% of them that are kind of the most informative and with just that I would
01:00:13 probably reach the same so it's a weak form of of active running if you want yes but there might be a much stronger
01:00:21 version yeah that's right that's what another notion question is the question is how much talking yet Elon Musk is
01:00:30 confident talk to him recently he's confident that large-scale data and deep learning can solve the autonomous
01:00:36 driving problem what are your thoughts on the limitless possibilities of deep learning in this space I was it's
01:00:43 obviously part of the solution I mean I don't think we'll ever have a set driving system or it is not in the
01:00:48 foreseeable future that does not use deep running you put it this way now how much of it so in the history of sort of
01:00:58 engineering particularly is sort of sort of a I like systems is generally your first phase where everything is built by
01:01:04 hand and it was the second phase and that was the case for autonomous driving you know 23 years ago there's a phase
01:01:10 where this a little bit of running is used but there's a lot of engineering that's involved in kind of you know
01:01:16 taking care of corner cases and and putting limits etc because the learning system is not perfect and then I as
01:01:25 technology progresses we end up relying more and more on learning that's the history of character recognition is a
01:01:29 history of speech recognition now computer vision that ronnie was processing and I think the same is going
01:01:35 to happen with with the term is driving that currently the the the methods that are closest to providing some level of
01:01:44 autonomy some you know a decent level of autonomy where you don't expect a driver to kind of do anything is where you
01:01:51 constrain the world so you only run within you know 100 square kilometers or square miles in Phoenix but the weather
01:01:58 is nice and the roads are wide it wishes what Weimer is doing you completely over engineer the car with tons of light
01:02:07 hours and sophisticated sensors that are too expensive for consumer cars but and you engineer the thing the hell out
01:02:17 of the everything else you you map the entire world so you have complete 3d model of everything so the only thing
01:02:22 that the perception system has to take care of is moving objects and and and construction and sort of you know things
01:02:31 that that weren't in your map and you can engineer a good you know slam system or eye stuff right so so that's kind of
01:02:36 the current approach that's closest to some level of autonomy but I think eventually the long term solution is
01:02:43 going to rely more and more on learning and possibly using a combination of supervised learning and model-based
01:02:50 reinforcement or something like that but ultimately learning will be at not just at the core but really the fundamental
01:02:58 part of the system yeah it already is but it'll become more and more what do you think it takes to build a system
01:03:04 with human level intelligence you talked about the AI system and then we her being way out of reach our current reach
01:03:12 this might be outdated as well but this is still way out of reach what would it take to build her do you think so I can
01:03:21 tell you the first two obstacles that we have to clear but I don't know how many obstacles they are after this so the
01:03:26 image I usually use is that there is a bunch of mountains that we have to climb and we can see the first one but we
01:03:31 don't know if there are 50 mountains behind it or not and this might be a good sort of metaphor for why AI
01:03:38 researchers in the past I've been overly optimistic about the result of AI you know for example New Orleans Simon
01:03:48 Wright wrote the general problem solver and they call it the general problems you have problems okay and of course if
01:03:54 it's you realize is that all the problems you want to solve is financial and so you can't actually use it for
01:04:00 anything useful but you know yes oh yeah all you see is the first peak so in general what are the first couple of
01:04:06 peaks for her so the first peak which is precisely what I'm working on is self supervisor running high how do we get
01:04:12 machines to learn models of the world by observation kind of like babies and like young animals
01:04:21 so I we've been working with you know cognitive scientists so this Amanda depuis who is at fair and in Paris is
01:04:31 half-time is also a researcher and French University and he he has his chart that shows that which how many
01:04:40 months of life baby humans kind of learned different concepts and you can met you can measure this various ways so
01:04:50 things like distinguishing animate objects from animate inanimate object you can you can tell the difference at
01:04:58 age to three months whether an object is going to stay stable is gonna fall you know about four months you can tell you
01:05:06 know things like this and then things like gravity the fact that objects are not supposed to float in the air but as
01:05:11 opposed to fall you run this around the age of eight or nine months if you look at a lot of you know eight month old
01:05:18 babies you give them a bunch of toys on the highchair first thing they do is it's why I'm on the ground that you look
01:05:22 at them it's because you know they're learning about actively learning about gravity gravity yeah okay so they're not
01:05:30 trying to know you but they you know they need to do the experiment right yeah so you know how do we get machines
01:05:36 to learn like babies mostly by observation with a little bit of interaction and learning those those
01:05:41 those models of the world because I think that's really a crucial piece of an intelligent autonomous system so if
01:05:47 you think about the architecture of an intelligent autonomous system it needs to have a predictive model of the world
01:05:53 so something that says here is a wall that time T here is a stable world at time T plus one if I take this action
01:05:59 and it's not a single answer it can be education yeah yeah well but we don't know how to represent distributions in
01:06:05 high dimension continuous basis so it's got to be something we care that data Hey but with some summer presentation
01:06:12 with certainty if you have that then you can do what optimal control theory is called model predictive control which
01:06:17 means that you can run your model with the hypothesis for a sequence of action and then see the result now what you
01:06:23 need the other thing you need is some sort of objective that you want to optimize am i reaching the goal of
01:06:29 grabbing the subject about minimizing energy am I whatever right so there is some sort of
01:06:34 objectives that you have to minimize and so in your head if you had this model you can figure out the sequence of
01:06:40 action that will optimize your objective that objective is something that ultimately is rooted in your basal
01:06:46 ganglia at least in the human brain that's that's what is available Gambia computes your level of contentment or
01:06:53 miss contentment oh no noise that's a word unhappiness okay yeah this contentment this contentment and so your
01:07:03 entire behavior is driven towards kind of minimizing that objective which is maximizing your contentment computed by
01:07:11 your your basal ganglia and what you have is an objective function which is basically a predictor of what your basal
01:07:17 ganglia is going to tell you so you're not going to put your hand on fire because you know it's gonna you know
01:07:23 it's gonna burn and you're gonna get hurt and you're predicting this because of your model of the world and your your
01:07:30 predictor of this objective right so you if you have those you have those three components you have four components you
01:07:38 have the the hard-wired contentment objective good computer if you want calculator and then you have the three
01:07:46 components one is the objective predictor which basically predicts your level of contact and one is the model of
01:07:53 the world and there's a third module I didn't mention which is a module that will figure out the best course of
01:08:00 action to optimize an objective given your model okay yeah cool it's a policy policy network or something like that
01:08:11 right now you need those three components to act autonomously intelligently and you can be stupid in
01:08:16 three different ways you can be stupid because your model of the world is wrong you can be stupid because your objective
01:08:23 is not aligned with what you actually want to achieve okay and in humans that would be a psychopath right and then the
01:08:32 the third thing you the third way you can be stupid is that you have the right model you have the right objective but
01:08:38 you're unable to figure out a course of action to optimize your objective given some people who are in charge of big
01:08:46 countries actually have all three that are wrong all right which countries I don't know okay so if we think about
01:08:56 this this agent if you think about the movie her you've criticized the art project that is Sophia the robot and
01:09:07 what that project essentially does is uses our natural inclination to anthropomorphize things that look like
01:09:15 human and given more do you think that could be used by AI systems like in the movie her
01:09:23 so do you think that body is needed to create a feeling of intelligence well if Sophia was just an art piece I
01:09:30 would have no problem with it but it's presented as something else let me add that comics real quick if creators of
01:09:39 Sofia could change something about their marketing or behavior in general what would it be what what's just about
01:09:48 everything I mean don't you think here's a tough question I mean so I agree with you so Sofia is not in the general
01:09:57 public feels that Sofia can do way more than she actually can that's right and the people will create a Sofia are not
01:10:08 honestly publicly communicating trying to teach the public right but here's a tough question don't you think this the
01:10:21 same thing is scientists in industry and research are taking advantage of the sameness misunderstanding in the public
01:10:27 when they create AI companies or published stuff some companies yes I mean there is no sense of there's no
01:10:35 desire to delude there's no desire to kind of over claim what something is done right you know you should paper on
01:10:41 AI that you know has this result on image net you know it's pretty clear I mean it's not even not even interesting
01:10:47 anymore but you know I I don't think there is that I mean the reviewers are generally not very forgiving of of you
01:10:56 know unsupported claims of this type and but there are certainly quite a few startups that have had a huge amount of
01:11:04 hype around this that I find extremely damaging and I've been calling it out when I've seen it so yeah but to go back
01:11:11 to your original question like the necessity of embodiment I think I don't think embodiment is necessary I think
01:11:18 grounding is necessary so I don't think we're gonna get machines that I really understand language without some level
01:11:22 of grounding in the world world and it's not clear to me that language is a kind of bandwidth medium to communicate how
01:11:30 the real world works I think what this doctor ground our grounding means so running me he's that
01:11:35 so there is this classic problem of common sense reasoning you know the the Winograd Winograd schema right and so I
01:11:44 tell you the the trophy doesn't fit in the suitcase because this tool is too big what the trophy doesn't fit in the
01:11:49 suitcase because it's too small and the it in the first case refers to the trophy in the second case to the
01:11:54 suitcase and the reason you can figure this out is because you know what the trophy in the suitcase are you know one
01:11:58 is supposed to fit in the other one and you know the notion of size and the big object doesn't fit in a small object and
01:12:04 this is a TARDIS you know it things like that right so you have this got this knowledge of how the world works of
01:12:12 geometry and things like that I don't believe you can learn everything about the world by just being told in language
01:12:18 how the world works I think you need some low-level perception of the world you know be a visual touch you know
01:12:25 whatever but some higher bandwidth perceptions of the world but by reading all the world's text you still may not
01:12:31 have enough information that's right there's a lot of things that just will never appear in text and that you can't
01:12:39 really infer so I think common sense will emerge from you know certainly a lot of language interaction but also
01:12:46 with watching videos or perhaps even interacting in the in virtual environments and possibly you know robot
01:12:52 interacting in the real world but I don't actually believe necessarily that this last one is absolutely necessary
01:12:57 but I think there's a need for some grounding but the final product doesn't necessarily need to be embodied you know
01:13:05 who say no it just needs to have an awareness a grounding right but it needs to know how the world works to have you
01:13:13 know to not be frustrated frustrating to talk to and you talked about emotions being important that's that's a whole
01:13:23 nother topic well so you know I talked about this the the basal ganglia ganglia as the you know this thing that could
01:13:31 you know calculates your level of miss contentment contentment and then there is this other module that sort of tries
01:13:37 to do a prediction of whether you're going to be content or not that's the source of some emotion so here for
01:13:44 example is an anticipation of bad things that can happen to you right you have this inkling that there is some
01:13:50 chance that something really bad is gonna happen to you and that creates here when you know for sure that
01:13:54 something bad is gonna happen to you you cannot give up right it's not bad anymore it's uncertainty it creates fear
01:14:01 so so the punchline is yes we're not gonna have a ton of intelligence without emotions whatever the heck emotions are
01:14:10 so you mentioned very practical things of fear but there's a lot of other mess around but there are kind of the results
01:14:16 of you know drives yeah there's deeper biological stuff going on and I've talked a few folks on
01:14:23 this there's a fascinating stuff that ultimately connects to our joy to our brain if we create an AGI system sorry
01:14:32 interminable human level intelligence system and you get to ask her one question what would that question be you
01:14:41 know I think the the first one we'll create would probably not be that smart did you like a four-year-old okay so you
01:14:50 would have to ask her a question - no she's not that smart yeah well what's a good question to ask
01:15:01 you know to be responsive wind and if she answers oh it's because the leaves of the tree are moving in that creates
01:15:09 wind she's on to something and if she says yeah that's a stupid question she's really obtuse no and then you tell
01:15:16 her actually you know here is the the real thing and she says oh yeah that makes sense
01:15:24 so questions that that reveal the ability to do common-sense reasoning about the physical world yeah and you
01:15:30 know someone will call 20 ferns causal evidence well it was a huge honor congratulations returning award you know
