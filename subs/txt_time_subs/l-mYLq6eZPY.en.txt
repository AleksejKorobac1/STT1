00:00:01 the following is a conversation with Petera Beal he's a professor UC Berkeley and the director of the Berkeley
00:00:08 robotics learning lab he's one of the top researchers in the world working on how we make robots understand and
00:00:14 interact with the world around them especially using imitation and deeper enforcement learning this conversation
00:00:23 is part of the MIT course and artificial general intelligence and the artificial intelligence podcast if you enjoy it
00:00:30 please subscribe on YouTube iTunes where your podcast provider of choice or simply connect with me on Twitter at Lex
00:00:38 Friedman spelled Fri D and now here's my conversation with Peter a Biel you've mentioned that if there was one person
00:00:45 you could meet you'll be Roger Federer so let me ask when do you think we will have a robot that fully autonomously can
00:00:55 beat Roger Federer at tennis Roger Federer level player at tennis huh well first if you can make it happen for me
00:01:02 to meet Roger let me know terms of getting a robot to beat him at tennis it's kind of an interesting question
00:01:12 because for a lot of the challenges we think about in AI the software is really the missing piece but for something like
00:01:21 this the hardware is nowhere near either like to really have a robot that can physically run around the Boston
00:01:28 Dynamics robots are starting to get there but still not really human level ability to to run around and then swing
00:01:38 a racket that's a hardware problem I don't think it's a harder problem only I think it's a hardware and a software
00:01:43 problem I think it's both and I think they'll they'll have independent progress so I'd say the the hardware
00:01:52 maybe in 10-15 years I'm just late not grass I've dressed with a sliding yeah oh plague I'm not sure what's Carter
00:02:00 grass or clay the clay involves sliding which might be harder to master actually yeah but you're not limited to bipedal I
00:02:10 mean I'm sure there's I can build a machine it's a whole different question of course you know
00:02:14 you can if you can say okay this robot can be on wheels they can move around on wheels and can be designed differently
00:02:22 then I think that that can be done sooner probably than a full humanoid type of setup what do you think is swing
00:02:29 a racket so you've worked at basic manipulation how hard do you think is the task of swinging or racket would be
00:02:36 able to hit a nice backhand or a forehand okay let's say let's say we just set up stationary a nice robot arm let's say
00:02:46 you know a standard industrial arm and it can wash the ball come and then swing the racket it's a good question I'm not
00:02:56 sure it would be super hard to do I mean I'm sure it would require a lot if we do it breed with reinforced Maleny would
00:03:01 require a lot of trial and error it's not gonna swing it right the first time around but yeah I don't I don't see why
00:03:09 I couldn't see the right way I think it's learn about I think if you set up a ball machine let's say on one side and
00:03:16 then a robot with a tennis racket on the other side I think it's learn about and maybe a
00:03:21 little bit of pre training and simulation yeah I think that's I think that's feasible I think I think the
00:03:27 swinging the racket is feasible I'd be very interesting to see how much precision it can get
00:03:35 listen I mean that's that's where I mean some of the human players can hit it on the lines which is very high precision
00:03:41 with spin this win is it is an interesting whether RL can learn to put a spin on the ball well you got me
00:03:47 interested maybe someday we'll set this is your answer is basically okay for this problem it sounds fascinating but
00:03:56 for the general problem of a tennis player we might be a little bit farther away what's the most impressive thing
00:04:01 you've seen a robot do in the physical world so physically for me it's the Boston Dynamics videos always just ring
00:04:15 home and just super impressed recently the robot running up the stairs doing the parkour type thing
00:04:21 I mean yes we don't know what's underneath they don't really write a lot of detail but even if it's hard coded
00:04:28 underneath which you might or might not be just the physical abilities of doing that parkour that's a very impressive so
00:04:34 a lot right there have you met spot many or any of those robots in person might spot mini last hearing in April at the
00:04:42 Mars event that Jeff Bezos organizes they brought it out there and it was nicely falling around Jeff when Jeff
00:04:49 left the room they had it follow him along which is pretty impressive so I think there's some
00:04:56 confidence to know that there's no learning going on in those robots the psychology of it so while knowing that
00:05:01 while knowing there's not if there's any learning going on it's very limited I met spot Minnie earlier this year and
00:05:08 knowing everything that's going on having one-on-one interaction so I got to spend some time alone and there's a
00:05:17 immediately a deep connection on the psychological level even though you know the fundamentals how it works there's
00:05:25 something magical so do you think about the psychology of interacting with robots in the physical world even you
00:05:33 just showed me the pr2 the the robot and and there was a little bit something like a face head a little bit something
00:05:39 like a face there's something that immediately draws you to it do you think about that aspect of
00:05:45 of the robotics problem well it's very hard with bread here we'll give him a name Berkeley robot for the elimination
00:05:52 of tedious tasks is very hard to not think of the robot as a person and it seems like everybody calls him a he for
00:06:00 whatever reason but that also makes it more a person than if it was a it and it's it seems pretty natural to think of
00:06:08 it that way this past weekend really struck me I've seen pepper many times on on videos but then I was at an event
00:06:16 organized by this was by fidelity and they had scripted pepper to help moderate some sessions and yet scripted
00:06:24 pepper to have the personality of a child a little bit and it was very hard to not think of it as its own person in
00:06:32 some sense because it was just kind of jumping it would just jump into conversation making it very interactive
00:06:37 moderate will be saying pepper just jump in hold on how about me can I participate in this doing it just like I
00:06:43 heard this is like like a person and I was 100% scripted and even then it was hard not to have that sense of somehow
00:06:51 there is something there so as we have robots interact in this physical world is that a signal that can be used in
00:06:58 reinforcement learning you've you've worked a little bit in this direction but do you think that's that psychology
00:07:04 can be somehow pulled in now so that's a question I would say a lot a lot of people ask and I think part of why they ask it is
00:07:15 they're thinking about how unique are we really still ask people like after they see some results they see a computer
00:07:21 play go to say computer do this that they're like ok but can it really have emotion can it really interact with us
00:07:28 in that way and then once you're around robots you already start feeling it and I think that kind of maybe
00:07:34 mythologically the way that I think of it is if you run something like reinforce some Linux about optimizing
00:07:42 some objective and there's no reason that D object couldn't be tied into how much there's a person like interacting
00:07:52 with this system and why could not the reinforcement learning system optimized for their robot being fun to be around and
00:07:58 why wouldn't it then naturally become more and more interactive and more and more maybe like a person or like a pet I
00:08:04 don't know what it would exactly be but more more have those features and acquire them automatically as long as
00:08:11 you can formalize an objective of what it means to like something what how you exhibit what's the ground truth how do
00:08:19 you how do you get the reward from human cause you have to somehow collect that information within you human
00:08:24 but you you're saying if you can formulate as an objective it can be learned there is no reason it couldn't
00:08:30 emergent through learning and maybe one way to formulate has an objective you wouldn't have to necessarily score it
00:08:35 explicitly so standard rewards are numbers and numbers are hard to come by this is a 1.5 or 0.7 on some scale it's
00:08:43 very hard to do for a person but much easier is for a person to say okay what you did the last five minutes was much
00:08:50 nicer than we did the previous five minutes and that now gives a comparison compare and in fact there have been some
00:08:55 results in that for example Paul Christiana and collaborators at open e I had the hopper madoka Hopper one legged
00:09:03 robot the Batman's little back flips yeah purely from feedback I like this better than that that's kind of equally
00:09:10 good and after a bunch of interactions it figured out what it was the person was asking for it namely a back flip and
00:09:15 so I think the same thing od wasn't trying to do a back flip it was just getting a score from the
00:09:21 comparison score from the person based on hers and having a mind in their own mind what I wanted to do a back flip but
00:09:29 the robot didn't know what it was supposed to be doing it just knew that sometimes the person said this is better
00:09:35 this is worse and then the robot figure it out what the person was actually after was a back flip and I'd imagine
00:09:41 the same would be true for things like more interactive robots that the robot would figure out over time oh this kind
00:09:47 of thing apparently has appreciated more than this other kind of thing so when I first picked up
00:09:55 Sutton's Richard Sutton's reinforcement learning book before sort of this deep learning before the re-emergence of
00:10:04 neural networks is a powerful mechanism for machine learning IRL seemed to me like magic as a as beautiful so
00:10:13 that seemed like what intelligence is RL reinforcement learning so how do you think we can possibly learn anything
00:10:22 about the world when the reward for the actions is delayed is so sparse like where is why do you think RL works why
00:10:32 do you think you can learn anything under such sparse awards whether it's regular reinforcement learning a deeper
00:10:38 enforcement learning what's your intuition the kind of part of that is why is RL why does it need so many
00:10:48 samples so many experiences to learn from because really what's happening is when you have a sparse reward you do
00:10:54 something maybe for like I don't know you take a hundred actions and then you get a reward and maybe get like a score
00:11:01 of three and I'm like okay three not sure what that means you go again and now I get to and now you know that that
00:11:07 sequence of hundred actions that you did the second time around somehow was worse than the sequence of hundred actions you
00:11:12 did the first time around but that's tough to now know which one of those were better or worse some might have
00:11:17 been good and bad in either one and so that's why I need so many experience but once you have enough experiences
00:11:23 effectively rlist easing that apart it's time to say okay when what is consistently there when you get a higher
00:11:28 reward and what's consistently there when you get a lower reward and then kind of the magic of sums is the policy
00:11:34 grant update is to say now let's update the neural network to make the actions that were kind of
00:11:41 present when things are good more likely and make the actions that are present when things are not as good less likely
00:11:47 so that's that is the counterpoint but it seems like you would need to run it a lot more than you do even though right
00:11:53 now people could say that RL is very inefficient but it seems to be way more efficient than one would imagine on
00:12:00 paper that the the simple updates to the policy the policy gradient that that's somehow you can learn is exactly users
00:12:07 said what are the common actions that seem to produce some good results that that somehow can learn anything it seems
00:12:15 counterintuitive at least did is there some intuition behind yeah so I think there's a few ways to think about this
00:12:24 the way I Tennant about it mostly originally when so when we started working on deep reinforcement
00:12:30 learning here at Berkeley which was maybe two thousand eleven twelve thirteen around that time
00:12:35 challenge Schulman was a PhD student initially kind of driving it too forward here and did it the way we thought about
00:12:44 it at the time was if you think about rectified linear units or kind of break the fire type neural networks what do
00:12:51 you get you get something that's piecewise linear feedback control and if you look at the literature linear
00:12:58 feedback control is extremely successful can solve many many problems surprisingly well
00:13:04 I remember for example when we did helicopter flight if you're in a stationary flight regime not a non
00:13:10 station by the stationary flight regime like hover you can use linear feedback control to stabilize a helicopter a very
00:13:15 complex dynamical system but the controller is relatively simple and so I think that's a big part of is that if
00:13:22 you do feedback control even though the system you control can be very very complex often relatively simple control
00:13:30 architectures can already do a lot but then also just linear is not good enough and so one way you can think of these
00:13:35 neural networks is that in sometimes they tile the space which people were already trying to do more by hand or
00:13:41 with finite state machines say this linear controller here this leaner controller here you'll network learns
00:13:46 that alva spins a linear controller here another linear controller here but it's more subtle than that yeah and so it's
00:13:51 benefiting from this linear control aspect is benefiting from the tiling but it's somehow tiling it one dimension at
00:13:58 a time because if let's say you have a two layer network even the hidden layer you make a transition from active to
00:14:06 inactive or the other way around that is essentially one axis but not acts as a line but one direction that you change
00:14:13 and so you have this kind of very gradual tiling of the space we have a lot of sharing between the linear
00:14:19 controllers that tile the space and that was always my intuition s of why to expect that this might work pretty well
00:14:25 it's essentially leveraging the fact that linear feedback control is so good but of course not enough and this is a
00:14:32 gradual tiling of the space with linear feedback controls that share a lot of expertise across them so that that's
00:14:39 that's really nice intuition do you think that scales to the more and more general problems of when you start going
00:14:47 up the number of controllers dimensions when you start going down in terms of how often you get a clean reward signal
00:14:57 does that intuition carry forward to those crazy or weird or worlds that we think of as the real world so I think
00:15:08 where things get really tricky in the real world compared to the things we've looked at so far with great success in
00:15:18 the time skills which takes us to an extreme so when you think about the real world I mean I don't know maybe some
00:15:26 student decided to do a a PhD here right okay that's that's the decision that's a very high-level decision but if you
00:15:33 think about their lives I mean any person's life it's a sequence of muscle fiber contractions and relaxations and
00:15:39 that's how you interact with the world and that's a very high frequency control thing but it's ultimately what you do
00:15:46 and how you affect the world until I guess we have brain readings and you can maybe do it slightly differently but
00:15:51 typically that's how you affect the world and the decision of doing a PhD is like so abstract relative to what you're
00:15:59 actually doing in the world and I think that's where credit assignment becomes just completely beyond what any current
00:16:07 RL algorithm can do and we need hierarchical reasoning at a level that is just not available at all yet where
00:16:14 do you think we can pick up hierarchical reasoning by which mechanisms yeah so maybe let me highlight what I think the
00:16:23 limitations are of what already was done 20-30 years ago in fact you'll find reasoning systems that reason over
00:16:31 relatively long horizons but the problems that they were not grounded in the real world so people would have to
00:16:41 hand design some kind of logical dynamical descriptions of the world and that didn't tie into perception and so
00:16:49 then time to real objects and so forth and so that that was a big gap now with deep learning we start having the
00:16:58 ability to really see with sensors process that and understand what's in the world and so it's a good time to try
00:17:05 to bring these things together one I see a few ways of getting there one way to get there would be to say deep learning
00:17:10 can get bolted on somehow to some of these more traditional approaches now bolted on would probably mean you need
00:17:15 to do some kind of end-to-end training where you say my deep learning processing somehow leads to a
00:17:23 representation that in Perm uses some kind of traditional underlying dynamical systems that can be used for planning
00:17:31 and that's for example the direction Aviv Tamar and the North Korea touch here have been pushing with causal info
00:17:36 gone and of course other people to that that's that's one way can we somehow force it into the form factor that is
00:17:44 amenable to reasoning another direction we've been thinking about for a long time and they didn't
00:17:51 make any progress on was more information theoretic approaches so the idea there was that what it means to
00:18:00 take high-level action is to take and choose a latent variable now that tells you a lot about what's gonna be the case
00:18:05 in the future because that's what it means to to take a high-level action I say what I decide I'm gonna navigate to
00:18:14 the gas station because need to get gas for my car well that'll now take five minutes to get there but the fact that I
00:18:20 get there I could already tell that from the high-level action it took much earlier that we had a very hard time
00:18:29 getting success with not saying it's a dead-end necessarily but we had a lot of trouble getting that to work and then we
00:18:35 start revisiting the notion of what are we really trying to achieve what we're trying to achieve is non ously hierarchy
00:18:41 per se but you could think about what does hierarchy give us what it's we hope it would give us is better credit
00:18:49 assignment kind of what is better credit ominous is given is giving us it gives us faster learning right and so faster
00:18:59 learning is ultimately maybe what we're after and so that's what we ended up with the RL squared paper on learning -
00:19:06 reinforcement learn which at a time rocky duan LED and that's exactly the meta learning approach or is say okay we
00:19:14 don't know how to design hierarchy we know what we want to get from it let's just enter an optimize for what want to
00:19:20 get from it and see if it might emerging we saw things emerge the maze navigation had consistent motion down hallways
00:19:27 which is what you want a hierarchical control should say I want to go down this hallway and then when there is an
00:19:32 option to take a turn I can this art will take a turn or not and repeat even had the notion of where have you been
00:19:38 before or not do not revisit places you've been before it still didn't scale yet to the real world kind of scenarios
00:19:46 I think you had in mind but it was some sign of life that maybe you can meta learn these hierarchal concepts I mean
00:19:54 it seems like through these meta learning concepts get at the what I think is one of the
00:20:01 hardest and most important problems of AI which is transfer learning so it's generalization how far along this
00:20:09 journey towards building general systems are we being able to do transfer learning well so there's some signs that
00:20:18 you can generalize a little bit but do you think we're on the right path or it's totally different breakthroughs are
00:20:26 needed to be able to transfer knowledge between different learned models yeah I'm I'm pretty tired on this and then I
00:20:37 think there are some very many there there's just some very impressive results already right I mean yes I would
00:20:46 say when even with the initial and a big breakthrough in 2012 with Aleks net right the initial the initial thing is
00:20:53 okay great this does better on imagenet hands image recognition but then immediately thereafter that was of
00:21:01 course the notion that Wow what was learned on image net and you now want to solve a new task you can
00:21:08 fine-tune Aleks net for new tasks and that was often found to be the even bigger deal that you learned something
00:21:15 that was reusable which was not often the case before usually machine learning you learned something for one scenario
00:21:19 and that was it and that's really exciting I mean that's just a huge application that's probably the biggest
00:21:24 success of transfer learning today in terms of scope and impact that was huge breakthrough and then recently I feel
00:21:35 like similar kind of but by scaling things up it seems like this has been expanded upon like people training even
00:21:40 bigger networks they might transfer even better if you looked at for example some of the opening eye results on language
00:21:46 models and some of the recent Google results on language models they are learned for just prediction and then
00:21:57 they get reused for other tasks and so I think there is something there where somehow if you train a big enough model
00:22:03 on enough things it seems to transfer some deepmind results I thought were very impressive unreal results where it
00:22:12 was learned to navigate mazes in ways where it wasn't just reinforcement learning going to have
00:22:16 other objectives was optimizing for so I think there's a lot of interesting results already I think maybe words hard
00:22:25 to wrap my head around this to which extend or when do we call something generalization right or the levels of
00:22:32 generalization involved in these different tasks alright so you draw this by the way just to frame things you've
00:22:41 heard you say somewhere it's the difference between learning to master versus learning to generalize that it's
00:22:48 a nice line to think about and it guess you're saying that's a gray area of what learning to master and learning to
00:22:55 generalize where once think I might have heard this I might have heard it somewhere else and I think it might have
00:23:00 been one of one of your interviews and maybe the one with yo show Benjamin on hundred percent sure but I like the
00:23:08 example I'm gonna act not sure who it was but the example was essentially if you use current deep learning techniques
00:23:16 what we're doing to predict let's say the relative motion of our planets it would do pretty well but then now if a
00:23:27 massive new mass enters our solar system it would prompt predict what will happen right and that's a different kind of
00:23:34 journal is a Shahnaz a generalization that relies on the ultimate simplest simplest explanation that we have
00:23:41 available today to explain the motion of planets where I was just pattern recognition could predict our current
00:23:46 solar system motion pretty well no problem and so I think that's an example of a kind of generalization that is a
00:23:52 little different from what we've achieved so far and it's not clear if just you know regularizing more I'm
00:24:01 forcing it to come up with a simpler simpler simple experience but it's not simple but that's what physics
00:24:06 researchers do right to say can I make this even simpler how simple can I get this what's a simplest equation I can
00:24:12 explain everything right yeah the master equation for the entire dynamics of the universe we haven't really pushed that
00:24:18 direction as hard in in deep learning I would say not sure if it should be pushed but it seems a kind of
00:24:24 generalization you get from that that you don't get in our current methods so far so I just talked to vladimir
00:24:31 vapnik for example who was a statistician the statistical learning and he kind of dreams of creating these
00:24:39 are the a equals e equals mc-squared for learning right the general theory of learning do you think that's a fruitless
00:24:49 pursuit in the near term in within the next several decades I think that's a really interesting pursuit and in the
00:24:57 following sense and that there is a lot of evidence that the brain is pretty modular and so I wouldn't maybe think of
00:25:06 it as the theory maybe the the underlying theory but more kind of the principle where there have been findings
00:25:15 where people who are blind will use the part of the brain usually used for vision for other functions and even
00:25:25 after some kind of if people will get rewired in some way they might I'm able to reuse parts of their brain for other
00:25:31 functions and so what that suggests is some kind of modularity and I think it is a pretty natural thing to strive
00:25:40 forward to see can we find that modularity can we find this thing of course it's not every part of the brain
00:25:46 is not exactly the same not everything can be rewired arbitrarily but if you think of things like the neocortex which
00:25:52 is pretty big part of the brain that seems fairly modular from what the findings so far can you design something
00:26:00 equally modular and if you can just grow it it becomes more capable probably I think that would be the kind of
00:26:05 interesting underlying principle to shoot for that is not unrealistic do you think you prefer math or empirical trial
00:26:16 and error for the discovery of the essence of what it means to do something intelligent so reinforcement learning embodies both
00:26:23 groups right then prove that something converges prove the bounds and then at the same time a lot of those successes
00:26:30 are well let's try this and see if it works so which do you gravitate towards how do you think of those two parts of
00:26:43 maybe I would prefer we could make the progress with mathematics and the reason maybe I would prefer that is because
00:26:49 because often if you have something you can mathematically formalise you can leapfrog a lot of experimentation and
00:26:57 experimentation takes a long time to get through and a lot of trial and error kind of reinforcement learning your
00:27:04 research process but you need to do a lot of trial and error before you get to a success so if we can leapfrog doubt in
00:27:09 my mind that's what the math is about and hopefully once you do a bunch of experiments you start seeing a pattern
00:27:16 you can do some derivations that leapfrog some experiments but I agree with you I mean in practice a lot of the
00:27:21 progress has been such that we have not been able to find the math that allows it to leapfrog ahead and we are kind of
00:27:28 making gradual progress one step at a time a new experiment here a new experiment there that gives us new
00:27:34 insights and gradually building up but not getting to something yet where we're just okay here's an equation that now
00:27:40 explains how you know that would be have been two years of experimentation to get there but this tells us what the results
00:27:46 going to be unfortunately not so much yes not so much yeah but your hope is there in trying to teach robots or
00:27:57 systems to do everyday tasks or even in simulation what what do you think you're more excited about imitation learning or
00:28:04 self play so letting robots learn from humans or letting robots plan their own to try to
00:28:13 figure out in their own way and eventually play eventually interact with humans or to solve whatever problem is
00:28:22 what's the more exciting to you what's more promising you think as a research direction so when we look at self play
00:28:34 what's so beautiful about it is goes back to kind of the challenges in reinforcement learning so the challenge
00:28:39 of reinforced learning is getting signal and if you don't never succeed you don't get any signal in self play you're on
00:28:48 both sides so one of you succeeds and the beauty is also one of you fails and so you see the contrast you see the one
00:28:53 version of me that it better the other version and so every time you play yourself you get signal and so
00:28:58 whenever you can turn something into self play you're in a beautiful situation where you can naturally learn
00:29:05 much more quickly than in most other reinforced learning environments so I think I think if somehow we can turn
00:29:13 more reinforcement learning problems into self play formulations that would go real really far so far south play has
00:29:21 been largely around games where there is natural opponents but if we could do self play if for other things and let's
00:29:26 say I don't know a robot learns to build a house I mean that's a pretty advanced thing to try to do for a robot but maybe
00:29:31 it tries to build a hut or something if that can be done through self play it would learn a lot more quickly if
00:29:36 somebody can figure that out and I think that would be something where it goes closer to kind of the mathematical leap
00:29:42 frogging where somebody figures out a formalism to it's okay any RL problem by playing this and this
00:29:48 idea you can turn it into a self play problem where you get signal a lot more easily reality is many problems we don't know
00:29:56 how to turn the self lay and so either we need to provide detailed reward that doesn't just reward for achieving a goal
00:30:03 but rewards for making progress and that becomes time-consuming and once you're starting to do that let's say you want a
00:30:07 robot to do something you need to give all this detailed reward well why not just give a demonstration right because
00:30:15 why not just show the robot and now the question is how do you show the robot one way to show is to tally operate the
00:30:19 robot and then the robot really experiences things and that's nice because that's really high
00:30:24 signal-to-noise ratio data and we've done a lot of that and you teach your robot skills in just 10 minutes you can
00:30:29 teach your robot a new basic skill like okay pick up the bottle place it somewhere else that's a skill no matter
00:30:34 where the bottle starts maybe it always goes on to a target or something that's fairly is a teacher about with
00:30:41 tally up now what's even more interesting if you can now teach robot through third person learning where the
00:30:47 robot watches you do something and doesn't experience it but just watches it and says okay well if you're showing
00:30:53 me that that means I should be doing this and I'm not gonna be using your hand because I don't get to control your
00:30:58 hand but I'm gonna use my hand I'd do that mapping and so that's where I think one of the big breakthroughs has
00:31:04 happened this year this was led by Chelsea Finn here it's almost like machine translation for demonstrations
00:31:11 were you have a human demonstration and the robot learns to translated into what it means for the robot to do it and that
00:31:17 was a meta learning for a Malaysian learn from one to get the other and that I think opens up a lot of opportunities
00:31:25 to learn a lot more quickly so my focus is on autonomous vehicles do you think this approach of third-person watching
00:31:31 is about the autonomous driving is amenable to this a kind of approach so for autonomous driving I would say it's
00:31:41 third-person is slightly easier and the reason I'm gonna say slightly easier to do a third-person is because the hard
00:31:50 dynamics are very well understood so the easier than of first-person you mean or easier so I think the distinction
00:31:58 between third-person and first-person is not a very important distinction for autonomous driving they're very similar
00:32:05 because the distinction is really about who turns the steering wheel and or maybe I'll let me put it differently how
00:32:14 to get from a point where you are now to a point let's say a couple meters in front of you and that's a problem that's
00:32:19 very well understood and that's the only distinction being third and first-person there whereas with the robot
00:32:24 manipulation interaction forces are very complex and it's still a very different thing for autonomous driving I think
00:32:32 there is still the question imitation versus RL so imitation gives you a lot more signal I think where imitation is
00:32:41 lacking and needs some extra machinery is it doesn't in its normal format doesn't think about goals or objectives
00:32:50 and of course there are versions of imitation learning inverse reinforce learning type imitation which also
00:32:55 thinks about goals I think then we're getting much closer but I think it's very hard to think of a fully reactive
00:33:04 car generalizing well if it really doesn't have a notion of objectives to generalize well to the kind of general
00:33:10 that you would want you'd want more than just that reactivity that you get from just behavioral cloning / supervised
00:33:19 learning so a lot of the work whether its self play imitation learning would benefit
00:33:24 significantly from simulation from effective simulation and you're doing a lot of stuff in the physical world and
00:33:30 in simulation do you have hope for greater and greater power of simulation loop being boundless eventually to where
00:33:40 most of what we need to operate in the physical world would could be simulated to a degree that's directly transferable
00:33:48 to the physical world are we still very far away from that so I think we could even rephrase that question in some
00:34:03 sense please so the power of simulation right simulators get better and better of course become stronger and we can
00:34:11 learn more in simulation but there's also another version which is where you said the simulator doesn't even have to
00:34:17 be that precise as long as is somewhat representative and instead of trying to get one simulator that is sufficiently
00:34:24 precise to learn in and transfer really well to the real world I'm gonna build many simulators ensemble
00:34:30 of simulators ensemble of simulators not any single one of them is sufficiently representative of the real world such
00:34:37 that it would work if you train in there but if you train in all of them then there is something that's good in all of
00:34:45 them the real world will just be you know another one that's you know cannot identical to any one of them but just
00:34:51 another one of them another sample from the distribution of simulators exact we do live in a simulation so this is just
00:34:58 like oh one other one I'm not sure about that video it's definitely a very advanced simulator if it is yeah it's
00:35:06 pretty good one I've talked to this to Russell is something you think about a little bit too of course you're like
00:35:12 really trying to build these systems but do you think about the future of AI a lot of people have concerned about
00:35:18 safety how do you think about AI safety as you build robots that are operating in the physical world what what is 
00:35:25 yeah how do you approach this problem in an engineering kind of way in a systematic way so what a robot is doing
00:35:32 things you kind of have a few notions of safety to worry about one is that Throwbot is
00:35:40 physically strong and of course could do a lot of damage same for cars which we can think of as robots do in some way
00:35:47 and this could be completely unintentional so it could be not the kind of long-term AI safety concerns
00:35:53 that okay a is smarter than us and now what do we do but it could be just very practical okay this robot if it makes a mistake
00:36:00 whether the results going to be of course simulation comes in a lot there too to test in simulation it's a
00:36:08 difficult question and I'm always wondering like I was wondering at let's go back to drivings a lot of people know
00:36:16 driving well of course what do we do to test somebody for driving right to get a driver's license what do they really do
00:36:25 I mean you fill out some test and then you drive and I mean perfume in suburban California the driving test is just you
00:36:34 drive around the block pull over you do a stop sign successfully and then you know you pull over again and you pretty
00:36:41 much done and you're like okay if a self-driving car did dad would you trust it that it can drive and be like no
00:36:49 that's not enough for me to trust but somehow for humans we've figured out that somebody being able to do that it's
00:36:56 representative of them being able to do a lot of other things and so I think somehow for you must we figured out
00:37:03 representative tests of what it means if you can do this what you can really do of course testing you must you must all
00:37:08 want to be tested at all times self-driving cars the robots can be tested more often probably you can have
00:37:13 replicas that get testament are known to be identical because they use the same neural net and so forth but still I feel
00:37:20 like we don't have this kind of unit tests or proper tests for for robots and I think there's something very
00:37:26 interesting to be thought about there especially as you update things your software improves you have a better self
00:37:32 driving car suite you updated how do you know it's indeed more capable on everything than what you had before that
00:37:40 you didn't have any bad things creep into it so I think that's a very interesting direction of research that
00:37:45 there is no real solution yet except that's somehow for you must we do because we say okay you have a driving
00:37:51 test you passed you can go on the road now and you must have accents every like a million or ten million miles something
00:37:58 something pretty phenomenal compared to that short test yeah that is being done so let me ask you've mentioned
00:38:05 you mentioned that Andrew Aang by example showed you the value of kindness and to do you think the space of
00:38:16 policies good policies for humans and for AI is populated by policies that with kindness or ones that are the
00:38:28 opposite exploitation even evil so if you just look at the sea of policies we operate under as human beings or if AI
00:38:34 system had to operate in this real world do you think it's really easy to find policies that are full of kindness like
00:38:41 we naturally fall into them or is it mean there is kind of two optimizations happening for humans right so for you
00:38:53 most was kinda the very long-term optimization which evolution has done for us and we're kind of predisposed to
00:39:01 like certain things and that's in sometimes what makes our learning easier because I mean we know things like pain
00:39:08 and hunger and thirst and the fact that we know about those is not something that we were taught that's kind of
00:39:13 innate when we're hungry were unhappy when we're thirsty were unhappy when we have pain we're unhappy and ultimately
00:39:22 evolution built that into us to think about this thing so so I think there is a notion that it seems somehow humans
00:39:30 evolved in general to prefer to get along in some ways but at the same time also to be very territorial and kind of
00:39:42 centric to their own tribe is it like it seems like that's the kind of space we converge down to it I mean I'm not an
00:39:47 expert in anthropology but it seems like we're very kind of good within our own tribe but need to be taught but to be
00:39:55 nice to other tribes well if you look at Steven Pinker he highlights is pretty nicely in
00:40:02 better better angels of our nature where he talks about violence decreasing over time consistently so whatever attention
00:40:09 whatever teams we pick it seems that the long arc of history goes towards us getting along more and more so I hope so
00:40:20 so do you think that do you think it's possible to cheat teach RL bass robots the this kind of kindness this kind of
00:40:28 ability to interact with humans this kind of policy even - let me ask let me ask a fun one do you think it's possible
00:40:35 to teach RL based robot to love a human being and to inspire that human to love the robot back so - like RL based
00:40:46 algorithm that leads to a happy marriage that's interesting question maybe I'll oh I'll answer it with with another
00:40:56 question right I mean it's it but I'll come back to it so another question you can have is okay
00:41:01 I mean how close does some people's happiness get from interacting with just a really nice dog like I mean dogs you
00:41:10 come home that's what dogs did they greet you they're excited it makes you happy when you're coming home to your
00:41:16 dog just like okay this is exciting they're always happy when I'm here and if they don't greet you because maybe
00:41:22 whatever your partner took them on a trip or something you might not be nearly as happy when you get home right
00:41:29 and so the kind of it seems like the level of reasoning a dog houses is pretty sophisticated but then it's still
00:41:35 not yet at the level of human reasoning and so it seems like we don't even need to achieve human love reason to get like
00:41:42 very strong affection with humans and so my thinking is why not right why couldn't with an AI couldn't we achieve
00:41:49 the kind of level of affection that humans feel among each other or with friendly animals and so forth it's a
00:41:59 question is it a good thing for us or not that misses another going right because I mean
00:42:07 but I don't see why not why not yeah so he almost says love was the answer maybe he should say love is the objective
00:42:16 function and then RL is the answer maybe I'll Peter thank you so much I don't want to take up more of your time thank
