00:00:01 the following is a conversation with Greg Brockman he's the co-founder and CTO of open AI a world-class research
00:00:08 organization developing ideas and AI with the goal of eventually creating a safe and friendly artificial general
00:00:16 intelligence one that benefits and empowers humanity open AI is not only a source of publications algorithms tools
00:00:26 and datasets their mission is a catalyst for an important public discourse about our future with both narrow and general
00:00:34 intelligence systems this conversation is part of the artificial intelligence podcast at MIT and beyond if you enjoy
00:00:43 it subscribe on youtube itunes or simply connect with me on twitter at Lex Friedman spelled Fri D and now here's my
00:00:53 conversation with Greg Brockman so in high school and right after you wrote a draft of a chemistry textbook I saw that
00:00:58 that covers everything from basic structure of the atom to quantum mechanics so it's clear you have an
00:01:05 intuition and a passion for both the physical world with chemistry and now robotics to the digital world with AI
00:01:14 deep learning reinforcement learning so on do you see the physical world in the digital world is different and what do
00:01:20 you think is the gap a lot of it actually boils down to iteration speed right that I think that a lot of what
00:01:25 really motivates me is is building things right is the I you know think about mathematics for example where you
00:01:30 think you're really hard about a problem you understand it you're right down in this very obscure form that we call
00:01:36 proof but then this is in humanities library right it's there forever this is some truth that we've discovered you
00:01:41 know maybe only five people in your field will ever read it now but somehow you've kind of moved humanity forward
00:01:46 and so I actually used to really think that I was going to be a mathematician and then I actually started writing this
00:01:52 chemistry textbook one of my friends told me you'll never publish it because you don't have a PhD so instead I
00:01:58 decided to build a website and try to promote my ideas that way and then I discovered programming and I you know
00:02:04 that in programming you think hard about a problem you understand it you write down in a very obscure form that we call
00:02:10 a program but then once again it's in humanities library right and anyone could get the benefit from
00:02:15 and the scalability is massive and so I think that the thing that really appeals to me about the digital world is that
00:02:21 you can have this this this insane leverage right a single individual with an idea is able to affect the entire
00:02:27 planet and that's something I think is really hard to do if you're moving around physical atoms but you said
00:02:33 mathematics so if you look at the the what thing you know over here our mind do you ultimately see it as just math is
00:02:42 just information processing or is there some other magic as you've seen if you've seen through biology and
00:02:47 chemistry and so on I think it's really interesting to think about humans is just information processing systems and
00:02:52 that it seems like it's actually a pretty good way of describing a lot of kind of how the world works or a lot of
00:02:59 what we're capable of to think that that you know again if you just look at technological innovations over time that
00:03:05 in some ways the most transformative innovation that we've had it has been the computer right in some ways the
00:03:10 internet you know that what is the right the Internet is not about these physical cables it's about the fact that I am
00:03:15 suddenly able to instantly communicate with any other human on the planet I'm able to retrieve any piece of knowledge
00:03:21 that in some ways the human race has ever had and that those are these insane transformations do you see the our
00:03:29 society as a whole the collective as another extension of the intelligence of the human being so if you look at the
00:03:34 human being is an information processing system you mentioned the internet then networking do you see us all together as
00:03:41 a civilization as a kind of intelligence system yeah I think this is actually a really interesting perspective to take
00:03:46 and to think about to you sort of have this collective intelligence of all of society the economy itself is this
00:03:53 superhuman machine that is optimizing something right and it's all in some ways a company has a will of its own
00:03:58 right that you have all these individuals we're all pursuing their own individual goals and thinking really
00:04:02 hard and thinking about the right things to do but somehow the company does something that is this emergent thing
00:04:09 and that is it so there's a really useful abstraction and so I think that in some ways you know we think of
00:04:14 ourselves as the most intelligent things on the planet and the most powerful things on the planet but there are
00:04:19 things that are bigger than us that these systems that we all contribute to and so I think actually you know it's a
00:04:24 it's interesting to think about if you've read as a guys a models foundation right that that there's this concept of
00:04:30 psychohistory in there which is effectively this that if you have trillions or quadrillions of beings then
00:04:35 maybe you could actually predict what that being that that huge macro being will do I and almost independent of what
00:04:43 the individuals want I actually have a second angle on this I think is interesting which is thinking about a
00:04:48 technological determinism one thing that I actually think a lot about with with open a tie right is that we're kind of
00:04:54 coming on onto this insanely transformational technology of general intelligence right that will happen at
00:04:59 some point and there's a question of how can you take actions that will actually steer it to go better rather than worse
00:05:06 and that I think one question you need to ask is as a scientist as an inventor as a creator what impact can you have in
00:05:12 general right you look at things like the telephone invented by two people in the same day like what does that mean
00:05:17 what does that mean about the shape of innovation and I think that what's going on is everyone's building on the
00:05:22 shoulders of the same giants and so you can kind of you can't really hope to create something no one else ever would
00:05:27 you know if Einstein wasn't born someone else would have come up with relativity you know he changed the timeline a bit
00:05:31 right that maybe it would have taken another 20 years but it wouldn't be that fundamentally humanity would never
00:05:36 discover these these fundamental truths so there's some kind of invisible momentum that some people like Einstein
00:05:45 or open the eyes plugging into that's anybody else can also plug into and ultimately that wave takes us into a
00:05:52 certain direction that's me that's right that's right and you know this kind of seems to play out in a bunch of
00:05:56 different ways that there's some exponential that is being ridden and that the exponential itself which one it
00:06:01 is changes think about Moore's law an entire industry set its clock to it for 50 years like how can that be right how
00:06:08 is that possible and yet somehow it happened and so I think you can't hope to ever invent something that no one
00:06:14 else will maybe you can change the timeline a little bit but if you really want to make a difference I think that
00:06:19 the thing that you really have to do the only real degree of freedom you have is to set the initial conditions under
00:06:25 which a technology is born and so you think about the internet right that there are lots of other competitors
00:06:29 trying to build similar things and the internet one and that the initial conditions where that was created by
00:06:35 this group that really valued people being able to be you know anyone being able to plug in this very academic
00:06:40 mindset of being open and connected and I think that the Internet for the next 40 years
00:06:45 really played out that way you know maybe today things are starting to shift in a different direction but I
00:06:51 think if those initial conditions were really important to determine the next 40 years worth of progress that's really
00:06:57 beautifully put so another example of that I think about you know I recently looked at it I looked at Wikipedia the
00:07:04 formation of Wikipedia and I wonder what the internet would be like if Wikipedia had ads you know there's a interesting
00:07:12 argument that why they chose not to make it put advertisement wikipedia i think it's i think wikipedia is one of the
00:07:18 greatest resources we have on the internet it's extremely surprising how well it works and how well it was able
00:07:24 to aggregate all this kind of good information and they essentially the creator of wikipedia I don't know
00:07:29 there's probably some debates there but set the initial conditions and now it carried it itself forward that's really
00:07:35 interesting so you're the way you're thinking about AGI or artificial intelligences you're focused on setting
00:07:40 the initial conditions for the for the progress that's right that's powerful okay so look into the future if you
00:07:48 create an AGI system like one that can ace the Turing test natural language what do you think would be the
00:07:55 interactions you would have with it what do you think are the questions you would ask like what would be the first
00:08:01 question you would ask it her/him that's right I think it at that point if you've really built a powerful system that is
00:08:07 capable of shaping the future of humanity the first question that you really should ask is how do we make sure
00:08:13 that this plays out well and so that's actually the first question that I would ask a powerful AGI system is so you
00:08:19 wouldn't ask your colleague you wouldn't ask like Ilya you would ask the AGI system oh we've already had the
00:08:24 conversation with Ilya right and everyone here and so you want as many perspectives and a piece of wisdom as
00:08:30 you can for it for answering this question so I don't think you necessarily defer to whatever your
00:08:36 powerful system tells you but you use as one input I like to try to figure out what to do but and I guess fundamentally
00:08:42 what it really comes down to is if you built something really powerful and you think about think about for example the
00:08:47 creation of of shortly after the creation of nuclear weapons right the most important question the world was
00:08:52 what's the world order going to be like how do we set ourselves up in where we're going to be able to survive
00:08:59 this species with a GI I think the question is slightly different right that there is a question of how do we
00:09:03 make sure that we don't get the negative effects but there's also the positive side right you imagine that you know
00:09:09 like like what won't AGI be like like what will be capable of and I think that one of the core reasons that an AGI can
00:09:15 be powerful and transformative is actually due to technological development yeah right if you have
00:09:21 something that's capable as capable as a human and that it's much more scalable that you absolutely want that thing to
00:09:27 go read the whole scientific literature and think about how to create cures for all the diseases right you want it to
00:09:31 think about how to go and build technologies to help us create material abundance and to figure out societal
00:09:38 problems that we have trouble with like how we're supposed to clean up the environment and you know maybe you want
00:09:43 this to go and invent a bunch of little robots that will go out and be biodegradable and turn ocean debris into
00:09:52 harmless molecules and I think that that that positive side is something that I think people miss sometimes when
00:09:57 thinking about what an AGI will be like and so I think that if you have a system that's capable of all of that you
00:10:03 absolutely want its advice about how do I make sure that we're using your your capabilities in a positive way for
00:10:10 Humanity so what do you think about that psychology that looks at all the different possible trajectories of an
00:10:17 AGI system many of which perhaps the majority of which are positive and nevertheless focuses on the negative
00:10:24 trajectories I mean you get to interact with folks you get to think about this maybe within yourself as well you look
00:10:31 at sam harris and so on it seems to be sorry to put it this way but almost more fun to think about the negative
00:10:38 possibilities whatever that's deep in our psychology what do you think about that and how do we deal with it because
00:10:45 we want AI to help us so I think there's kind of two problems so I entailed in that question the first is more of the
00:10:53 question of how can you even picture what a world with a new technology will any like now imagine were in 1950 and
00:11:06 apps and the internet yeah I mean your yeah that's that's going to be extremely complicated but it's imaginable
00:11:12 it's imaginable right but and now imagine being a 1950 and predicting goober right and you need to describe
00:11:18 the internet you need to describe GPS you need to describe the fact that everyone's going to have this phone in
00:11:24 their pocket and so I think that that just the first truth is that it is hard to picture how a transformative
00:11:30 technology will play out in the world we've seen that before with technologies that are far less transformative than AG
00:11:37 I will be and so I think that that one piece is that it's just even hard to imagine and to really put yourself in a
00:11:42 world where you can predict what that that positive vision would be like and you know I think the second thing is
00:11:51 that it is I think it is always easier to support the negative side than the positive side it's always easier to
00:11:58 destroy than create and you know less than in a physical sense and more just in an intellectual sense right because
00:12:05 you know I think that with creating something you need to just get a bunch of things right and to destroy you just
00:12:10 need to get one thing wrong yeah and so I think that that what that means is that I think a lot of people's thinking
00:12:14 dead ends as soon as they see the negative story but that being said I actually actually have some hope right I
00:12:23 think that the the positive vision is something that I think can be something that we can we can talk about I think
00:12:29 that just simply saying this fact of yeah like there's positive there's negatives everyone likes to draw them
00:12:34 the negative people have to respond well to that message and say huh you're right there's a part of this that we're not
00:12:39 talking about not thinking about and that's actually something that's that's that's I think really been a key part of
00:12:46 how we think about AGI at open AI right you can kind of look at it as like okay like opening eye talks about the fact
00:12:51 that there are risks and yet they're trying to build this system like how do you square this those two facts so do
00:12:57 you share the intuition that some people have I mean from Sam Harris even Elon Musk himself that it's tricky as you
00:13:07 develop AGI to keep it from slipping into the existential threats into the negative what's your intuition about how
00:13:15 hard is it to keep a a development on the positive track and you what's your intuition there to
00:13:22 answer the question you can really look at how we structure open AI so we really have three main arms
00:13:27 we have capabilities which is actually doing the technical work and pushing forward what these systems can do
00:13:33 there's safety which is working on technical mechanisms to ensure that the systems we build are lined with human
00:13:39 values and then there's policy which is making sure that we have governance mechanisms answering that question of
00:13:46 well whose values and so I think that the technical safety one is the one that people kind of talk about the most right
00:13:52 you talk about like think about you know all of the dystopic AI movies a lot of that is about not having good technical
00:13:58 safety in place and what we've been finding is that you know I think that actually a lot of people look at the
00:14:02 technical safety problem and think it's just intractable right this question of what do humans want how am I supposed to
00:14:09 write that down can I even write down what I want no way and then they stop there but the thing is we've already
00:14:17 built systems that are able to learn things that humans can't specify you know even the rules for how to recognize
00:14:24 if there's a cat or a dog in an image turns out its intractable to write that down and yet we're able to learn it and
00:14:30 that what we're seeing with systems we build it open it yeah and there's still an early proof of concept stage is that
00:14:36 you are able to learn human preferences you're able to learn what humans want from data and so that's kind of the core
00:14:41 focus for our technical safety team and I think that they're actually we've had some pretty encouraging updates in terms
00:14:48 of what we've been able to make work so you have an intuition and a hope that from data you know looking at the value
00:14:54 alignment problem from data we can build systems that align with the collective better angels of our nature so aligned
00:15:04 with the ethics and the morals of human beings to even say this in a different way I mean think about how do we align
00:15:09 in humans right think about like a human baby can grow up to be an evil person or a great person and a lot of that is from
00:15:15 learning from data right that you have some feedback as a child is growing up they get to see positive examples and so
00:15:22 I think that that just like them that the the only example we have of a general intelligence that is able to
00:15:28 learn from data I too aligned with human values and to learn values I think we shouldn't be surprised
00:15:34 that we can do the same sorts of techniques or whether the same sort of techniques end up being how we we saw
00:15:41 value alignment for AG eyes so let's go even higher as I don't know if you've read the book sapiens mm-hmm but there's
00:15:48 an idea that you know that as a collective is us human beings who kind of develop together and ideas that we
00:15:56 hold there's no in that context objective truth we just kind of all agree to certain ideas and hold them as
00:16:02 a collective if you have a sense that there is in the world of good and evil do you have a sense that to the first
00:16:08 approximation there are some things that are good and that you could teach systems to behave to be good so I think
00:16:17 that this actually blends into our third team right which is the policy team and this is the one the the aspect I think
00:16:23 people really talk about way less than they should all right because imagine that we built super-powerful systems
00:16:29 that we've managed to figure out all the mechanisms for these things to do whatever the operator wants the most
00:16:34 important question becomes who's the operator what do they want and how is that going to affect everyone else right
00:16:41 and and I think that this question of what is good what are those values I mean I think you don't even have to go
00:16:47 to those those very grand existential places to start to realize how hard this problem is you just look at different
00:16:53 countries and cultures across the world and that there's there's a very different conception of how the world
00:17:00 works and you know what what what kinds of of ways that society wants to operate and so I think that the really core
00:17:09 question is is is actually very concrete and I think it's not a question that we have ready answers to right is how do
00:17:16 you have a world where all the different countries that we have United States China Russia and you know the hundreds
00:17:23 of other countries out there are able to continue to not just operate in the way that they see fit but in that the world
00:17:33 that emerges in these where you have these very powerful systems I operating alongside humans ends up being something
00:17:40 that empowers humans more that makes like exhuming existence be a more meaningful thing and the
00:17:47 people are happier in wealthier and able to live more fulfilling lives it's not nob vyas thing for how to design that
00:17:52 world once you have that very powerful system so if we take a little step back and we're having it like a fascinating
00:17:59 conversation and open eyes in many ways a tech leader in the world and yet we're thinking about these big existential
00:18:06 questions which is fascinating really important I think you're a leader in that space and it's a really important
00:18:12 space of just thinking how AI affect society in a big-picture view so Oscar Wilde said we're all in the gutter but
00:18:19 some of us are looking at the Stars and I think open air has a charter that looks to the Stars I would say to create
00:18:26 intelligence to create general intelligence make it beneficial safe and collaborative so can you tell me how
00:18:34 that came about how a mission like that and the path to creating a mission like that open yeah I was founded yeah so I
00:18:42 think that in some ways it really boils down to taking a look at the landscape alright so if you think about the
00:18:47 history of AI that basically for the past 60 or 70 years people have thought about this goal of what could happen if
00:18:55 you could automate human intellectual labor right imagine you can build a computer system that could do that
00:19:01 what becomes possible well out of sci-fi that tells stories of various dystopian and you know increasingly you have
00:19:05 movies like heard that tell you a little bit about maybe more of a little bit utopic vision I you think about the
00:19:13 impacts that we've seen from being able to have bicycles for our minds and computers and that I think that the the
00:19:21 impact of computers and the Internet has just far outstripped what anyone really could have predicted and so I think that
00:19:28 it's very clear that if you can build an AI it will be the most transformative technology that humans will ever create
00:19:35 and so what it boils down to then is a question of well is there a path is there hope is there a way to build such
00:19:42 a system and I think that for 60 or 70 years that people got excited and I they you know ended up not being able to
00:19:50 deliver on the hopes that the people I pinned on them and I think that then you know that after you know two to winters
00:19:56 of AI development that people I you know I think kind of almost stopped daring to dream right the
00:20:02 really talking about a GI or thinking about a GI became almost this taboo in the community but I actually think that
00:20:09 people took the wrong lesson from AI history and if you look back starting in nineteen fifty nine is when the
00:20:14 perceptron was released and this is basically you know one of the earliest neural networks it was released to what
00:20:20 was perceived as this massive overhype so in the New York Times in nineteen fifty-nine you have this article saying
00:20:26 that you know the the perceptron will one day recognize people call out their names instantly translate speech between
00:20:33 languages and people at the time looked at this and said this is Jack your system can't do any of that and
00:20:37 basically spent ten years trying to discredit the whole perceptron direction and succeeded and all the funding dried
00:20:44 up and you know people kind of went in other directions and you know the 80s there was a resurgence and I'd always
00:20:49 heard that the resurgence in the 80s was due to the invention of back propagation and these these algorithms that got
00:20:54 people excited but actually the causality was due to people building larger computers that you can find these
00:20:59 these articles from the 80s saying that the democratization of computing power suddenly meant that you could run these
00:21:04 larger neural networks and then people start to do all these amazing things the backpropagation algorithm was invented
00:21:09 and you know that the the neural nets people running were these tiny little like 20 neuron neural nets right what
00:21:14 are you supposed to learn with 20 neurons and so of course they weren't able to get great results and it really
00:21:21 wasn't until 2012 that this approach that's almost the most simple natural approach that people have come up with
00:21:28 in the 50s right in some ways even in the 40s before there were computers with a Pitts McCullen air and neuron suddenly
00:21:35 this became the best way of solving problems right and I think there are three core properties that deep learning
00:21:43 has that I think are very worth paying attention to the first is generality we have a very
00:21:49 small number of deep learning tools SGD deep neural net maybe some some you know RL and it solves this huge variety of
00:21:56 problems speech recognition machine translation game playing all these problems small set of tools so there's
00:22:03 the generality there's a second piece which is the competence you want to solve any of those problems throw it
00:22:08 forty years worth of computer vision research replacing the deep neural net it's kind of work better
00:22:14 and there's a third piece which is the scalability right the one thing that has been shown time and time again is that
00:22:20 you if you have a larger neural network for a more compute more data at it it will work better those three
00:22:28 properties together feel like essential parts of building a general intelligence now it doesn't just mean that if we
00:22:34 scale up what we have that we will have an AGI right there are clearly missing pieces they're missing ideas we need to
00:22:41 have answers for reasoning but I think that the core here is that for the first time it feels that we have a paradigm
00:22:48 that gives us hope the general intelligence can be achievable and so as soon as you believe that everything else
00:22:54 becomes comes into focus right if you imagine that you may be able to and you know that the timeline I think remains
00:23:00 uncertain on the but I think that that you know certainly within our lifetimes and possibly within a much shorter
00:23:05 period of time than then people would expect if you can really build the most transformative technology that will ever
00:23:11 exist you stop thinking about yourself so much right and you start thinking about just like how do you have a world
00:23:16 where this goes well and that you need to think about the practicalities of how do you build an organization and get
00:23:21 together a bunch of people and resources and to make sure that people feel motivated and ready to do it but I think
00:23:30 that then you start thinking about well what if we succeed and how do we make sure that when we succeed that the world
00:23:36 is actually the place that we want ourselves to exist then and almost in the Rawls the unveils sense of the word
00:23:43 and so that's kind of the broader landscape and opening I was really formed in 2015 with that high level
00:23:51 picture of AGI might be possible sooner than people think and that we need to try to do our best to make sure it's
00:23:58 going to go well and then we spent the next couple years really trying to figure out what does that mean how do we
00:24:01 do it and you know I think that typically with a company you start out very small so
00:24:08 you in a co-founder and you build a product you got some users you get a product market fit
00:24:12 you know then at some point you raise some money you hire people you scale and then you know down the road then the big
00:24:18 companies realize you exist and try to kill you and for opening I it was basically
00:24:22 everything in exactly the order let me just pause for a second he said a lot of things and let me just
00:24:31 admire the jarring aspect of what open AI stands for which is daring to dream I mean you said it's pretty powerful you
00:24:38 caught me off guard because I think that's very true the-the-the step of just daring to dream
00:24:45 about the possibilities of creating intelligence in a positive in a safe way but just even creating intelligence is a
00:24:56 much needed refreshing catalyst for the AI community so that's that's the starting point
00:25:02 okay so then formation of open AI was just I just say that you know when we were starting opening AI that kind of
00:25:07 the first question that we had is is it too late to start a lab with a bunch of the best people possible that was an
00:25:15 actual question so those were those that was the core question of you know hey there's dinner in July of 20 2015
00:25:20 and there's that was that was really what we spent the whole time talking about and you know cuz it's the you
00:25:26 think about kind of where AI was is that it transitioned from being an academic pursuit to an industrial pursuit and so
00:25:33 a lot of the best people were in these big research labs and that we wanted to start our own one that you know no
00:25:39 matter how much resources we could accumulate it would be you know pale in comparison to the big tech companies and
00:25:45 we knew that and there's a question of are we going to be actually able to get this thing off the ground you need
00:25:50 critical mass you can't just do you and a co-founder build a product right you really need to have a group of you know
00:25:57 five to ten people and we kind of concluded it wasn't obviously impossible so it seemed worth trying well you're
00:26:04 also dreamers so who knows right that's right okay so speaking of that competing with with the the big players let's talk
00:26:13 about some of the some of the tricky things as you think through this process of growing of seeing how you can develop
00:26:21 these systems a task at scale that competes so you recently recently formed open ILP a new cap profit company that
00:26:31 now carries the name open it so open has now this official company the original non profit company
00:26:38 still exists and carries the opening I nonprofit name so can you explain what this company is what the purpose of us
00:26:45 creation is and how did you arrive at the decision yep to create it openly I the whole entity and opening I LP as a
00:26:54 vehicle is trying to accomplish the mission of ensuring that artificial general intelligence benefits everyone
00:26:59 and the main way that we're trying to do that is by actually trying to build general intelligence ourselves and make
00:27:04 sure the benefits are distributed to the world that's the primary way we're also fine if someone else does this all right
00:27:10 it doesn't have to be us if someone else is going to build an AGI and make sure that the benefits don't get locked up in
00:27:16 one company or you know one one want with one set of people like we're actually fine with that and so those
00:27:25 ideas are baked into our Charter which is kind of the the foundational document that are describes kind of our values
00:27:32 and how we operate but it's also really baked into the structure of open at LP and so the way
00:27:39 that we've set up opening ILP is that in the case where we succeed right if we actually build what we're trying to
00:27:46 build then investors are able to get a return and but that return is something that is capped and so if you think of
00:27:53 AGI in terms of data the value that you could really create you're talking about the most transformative technology ever
00:27:58 created it's going to create orders of magnitude more value than any existing company and that all of that value will
00:28:06 be owned by the world like legally title to the nonprofit to fulfill that mission and so that's that's the structure so
00:28:15 the mission is a powerful one and it's a it's one that I think most people would agree with it's how we would hope a I
00:28:24 progresses and so how do you tie yourself to that mission how do you make sure you do not deviate from that
00:28:32 mission that you know other incentives that are profit driven wouldn't don't interfere with the mission so this was
00:28:39 actually a really core question for us for the past couple years because you know I'd say that like the way that our
00:28:44 history went was that for the first year we were getting off the ground right we had this high level picture but we
00:28:48 didn't know exactly how we wanted to accomplish it and really two years ago it's when we
00:28:55 first started realizing in order to build a GI we're just going to need to raise way more money than we can as a
00:29:01 nonprofit I mean you're talking many billions of dollars and so the first question is how are you supposed to do
00:29:08 that and stay true to this mission and we looked at every legal structure out there and concluded none of them were
00:29:13 quite right for what we wanted to do and I guess it shouldn't be too surprising if you're going to do something like
00:29:17 crazy unprecedented technology that you're gonna have to come up with some crazy unprecedent structure to do it in
00:29:22 and a lot of a lot of our conversation was with people at opening I write the people who really join because they
00:29:28 believe so much in this mission and thinking about how do we actually raise the resources to do it and also stay
00:29:36 true to to what we stand for and the place you got to start is to really align on what is it that we stand for
00:29:40 right what are those values what's really important to us and so I'd say that we spent about a year really
00:29:46 compiling the opening I'd charter and that determines and if you even look at the first the first line item in there
00:29:51 it says that look we expect we're gonna have to marshal huge amounts of resources but we're going to make sure
00:29:56 that we minimize conflicts of interest with the mission and that kind of aligning on all of those pieces was the
00:30:03 most important step towards figuring out how do we structure a company that can actually raise the resources to do what
00:30:12 we need to do I imagined open AI the decision to create open ILP was a really difficult one and there was a lot of
00:30:18 discussions as you mentioned for a year and there was different ideas perhaps detractors with an open AI sort of
00:30:27 different paths that you could have taken what were those concerns what were the different paths considered what was
00:30:33 that process of making that decision like yep but so if you look actually at the opening I charter that there's
00:30:40 almost two paths embedded within it there is we are primarily trying to build AGI ourselves but we're also ok if
00:30:48 someone else does it and this is a weird thing for a company it's really interesting actually yeah there there is
00:30:53 an element of competition that you do want to be the one that does it but at the same time you're ok somebody else's
00:30:59 and you know we'll talk about that a little bit that trade-off that's the day that's really interesting and I think
00:31:05 this was the core tension as we were designing open an ILP and really the opening eye strategy is how do you make
00:31:11 sure that both you have a shot at being a primary actor which really requires building an organization raising massive
00:31:19 resources and really having the will to go and execute on some really really hard vision all right you need to really
00:31:24 sign up for a long period to go and take on a lot of pain and a lot of risk and to do that normally you just import the
00:31:32 startup mindset right and that you think about okay like how do we how to execute everyone you give this very competitive
00:31:37 angle but you also have the second angle of saying that well the true mission isn't for opening high to build a GI the
00:31:43 true mission is for AGI to go well for Humanity and so how do you take all of those first actions and make sure you
00:31:51 don't close the door on outcomes that would actually be positive in fulfill the mission and so I think it's a very
00:31:56 delicate balance right I think that going 100% one direction or the other is clearly not the correct answer and so I
00:32:03 think that even in terms of just how we talk about opening I and think about it there's just like like one thing that's
00:32:08 always in the back of my mind is to make sure that we're not just saying opening eyes goal is to build AGI right that
00:32:15 it's actually much broader than that right that first of all I you know it's not just AGI it's safe AGI that's very
00:32:22 important but secondly our goal isn't to be the ones to build it our goal is to make sure it goes well for the world and
00:32:26 so I think that figuring out how do you balance all of those and to get people to really come to the table and compile
00:32:34 the the like a single document that that encompasses all of that wasn't trivial so part of the challenge here is your
00:32:43 mission is I would say beautiful empowering and a beacon of hope for people in the research community and
00:32:49 just people thinking about AI so your decisions are scrutinized more than I think a regular profit driven company do
00:32:57 you feel the burden of this in the creation of the Charter and just in the way you operate yes so why do you lean
00:33:08 into the burden by creating such a charter why not to keep it quiet I mean it just boils down to the to the mission right
00:33:15 I'm here and everyone else is here because we think this is the most important mission right dare to dream
00:33:20 all right so what do you think you can be good for the world or create an a GI system that's good when you're a
00:33:29 for-profit company from my perspective I don't understand why profit interferes with positive impact on society I don't
00:33:41 understand by Google that makes most of its money from ads you can't also do good for the world or other companies
00:33:48 Facebook anything I don't I don't understand why those have to interfere you know you can profit isn't the thing
00:33:56 in my view that affects the impact of a company what affects the impact of the company is the Charter is the culture is
00:34:04 the you know the people inside and profit is the thing that just fuels those people so what are your views
00:34:10 there yeah so I think that's a really good question and there's there's there's some some you know real like
00:34:14 long-standing debates in human society that are wrapped up in it the way that I think about it is just think about what
00:34:20 what are the most impactful nonprofits in the world what are the most impactful for profits in the world right is much
00:34:29 easier to lists the for profits that's right and I think that there's there's some real truth here that the system
00:34:35 that we set up the system for kind of how you know today's world is organized is one that that really allows for huge
00:34:43 impact and that that you know kind of part of that is that you need to be you know for profits are our self-sustaining
00:34:50 and able to to kind of you know build on their own momentum and I think that's a really powerful thing it's something
00:34:56 that when it turns out that we haven't set the guardrails correctly causes problems right think about logging
00:35:01 companies that go and DeForest you know you know the rain forest that's really bad we don't want that and it's actually
00:35:06 really interesting to me the kind of this this question of how do you get positive benefits out of a for-profit
00:35:12 company it's actually very similar to how do you get positive benefits out of an AGI right that you have this like
00:35:18 very powerful system it's more powerful than any human and it's kind of autonomous in some ways you know super
00:35:24 human and a lot of axes and somehow you have to set the guardrails to get good to happen but when you do the benefits
00:35:31 are massive and so I think that the when when I think about nonprofit vs. for-profit I think it's just not enough
00:35:37 happens in nonprofits they're very pure but it's just kind of you know it's just hard to do things they're in for profits
00:35:44 in some ways like too much happens but if if kind of shaped in the right way it can actually be very positive and so
00:35:51 with open NLP we're picking a road in between now the thing I think is really important to recognize is that the way
00:35:57 that we think about opening ILP is that in the world where AGI actually happens right in a world where we are successful
00:36:02 we build the most transformative technology ever the amount of value we're going to create will be
00:36:08 astronomical and so then in that case that the if it the the cap that we have will be a small fraction of the value we
00:36:17 create and the amount of value that goes back to investors and employees looks pretty similar to what would happen in a
00:36:24 pretty successful startup and that's really the case that we're optimizing for right that we're thinking about in
00:36:30 the success case making sure that the value we create doesn't get locked up and I expect that in another you know
00:36:35 for-profit companies that it's possible to do something like that I think it's not obvious how to do it right and I
00:36:41 think that as a for-profit company you have a lot of fiduciary duty to your shareholders and that there are certain
00:36:47 decisions you just cannot make in our structure we've set it up so that we have a fiduciary duty to the Charter
00:36:54 that we always get to make the decision that is right for the Charter rather than even if it comes at the expense of
00:37:01 our own stakeholders and and so I think that when I think about what's really important it's not really about
00:37:06 nonprofit vs. for-profit it's really a question of if you build a GI and you kind of you know humanities now in this
00:37:14 new age who benefits whose lives are better and I think that what's really important is to have an answer that is
00:37:21 everyone yeah which is one of the core aspects of the Charter so one concern people have not just with open the eye
00:37:29 but with Google Facebook Amazon anybody really that's that's creating impact that scale is how do we avoid as your
00:37:39 Charter says avoid enabling the use of or AGI to unduly concentrate power why would not a company like open a I keep
00:37:48 all the power of an AGI system to itself the Charter the Charter so you know how does the Charter actualize itself in day
00:37:59 to day so I think that first to zoom out right there the way that we structure the company is so that the the power
00:38:05 first sort of you know dictating the actions that opening eye takes ultimately rests with the board right
00:38:11 the board of the nonprofit I'm and the board is set up in certain ways certain certain restrictions that you can read
00:38:16 about in the opening hi LP blog post but effectively the board is the is the governing body for opening ILP and the
00:38:23 board has a duty to fulfill the mission of the nonprofit and so that's kind of how we tie how we thread all these
00:38:31 things together now there's a question of so day to day how do people the individuals who in some ways are the
00:38:37 most empowered ones ain't no the board sort of gets to call the shots at the high level but the people who are
00:38:42 actually executing are the employees the way that people here on a day-to-day basis who have the you know the the keys
00:38:49 to the technical Kingdom and their I think that the answer looks a lot like well how does any company's values get
00:38:55 actualized right I think that a lot of that comes down to that you need people who are here because they really believe
00:39:02 in that mission and they believe in the Charter and that they are willing to take actions that maybe are worse for
00:39:08 them but are better for the Charter and that's something that's really baked into the culture and honestly I think
00:39:14 it's I you know I think that that's one of the things that we really have to work to preserve as time goes on and
00:39:20 that's a really important part of how we think about hiring people and bringing people into opening I so there's people
00:39:25 here there's people here who could speak up and say like hold on a second this is totally against what we stand for
00:39:35 cultural eyes yeah yeah for sure I mean I think that that we actually have I think that's like a pretty important
00:39:41 part of how we operate and how we have even again with designing the Charter and designing open alp in the first
00:39:47 place that there has been a lot of conversation with employees here and a lot of times where employees said wait a
00:39:52 second this seems like it's coming in the wrong direction and let's talk about it and so
00:39:56 I think one thing that's that's I think I really and you know here's here's actually one thing I think is very
00:40:01 unique about us as a small company is that if you're at a massive tech giant that's a little bit hard for someone
00:40:07 who's aligned employee to go and talk to the CEO and say I think that we're doing this wrong and you know you look at
00:40:14 companies like Google that have had some collective action from employees to you know make ethical change around things
00:40:20 like maven and so maybe there are mechanisms that other companies that work but here super easy for anyone to
00:40:25 pull me aside to pull Sam aside to Balilla aside and people do it all the time one of the interesting things in
00:40:31 the Charter is this idea that it'd be great if you could try to describe or untangle switching from competition to
00:40:37 collaboration and late-stage AGI development it was really interesting this dance between competition and
00:40:42 collaboration how do you think about that yeah assuming you can actually do the technical side of AGI development I
00:40:48 think there's going to be two key problems with figuring out how do you actually deploy it make it go well the
00:40:53 first one of these is the run-up to building the first AGI you look at how self-driving cars are being developed
00:41:00 and it's a competitive race I'm the thing that always happens in a competitive race is that you have huge
00:41:05 amounts of pressure to get rid of safety and so that's one thing we're very concerned about right is that people
00:41:12 multiple teams figuring out we can actually get there but you know if we took the slower path that is more
00:41:20 guaranteed to be safe we will lose and so we're going to take the fast path and so the more that we can both ourselves
00:41:27 be in a position where we don't generate that competitive race where we say if the race is being run and that you know
00:41:32 someone else's is further ahead than we are we're not gonna try to to leapfrog we're gonna actually work with them
00:41:39 right we will help them succeed as long as what they're trying to do is to fulfill our mission then we're good we
00:41:45 don't have to build AGI ourselves and I think that's a really important commitment from us but it can't just be
00:41:49 unilateral right I think that's really important that other players who are serious about building AGI make similar
00:41:56 commitments right I think that that you know again to the extent that everyone believes that AGI should be something to
00:42:00 benefit everyone then it actually really shouldn't matter which company builds it and we should all be concerned about the
00:42:05 case where we just race so hard to get there that something goes wrong so what role do you think government our favorite
00:42:13 entity has in setting policy and rules about this domain from research to the development to early stage to late stage
00:42:23 a a inhi development so I think that first of all is really important the government's in their right in some way
00:42:30 shape or form you know at the end of the day we're talking about building technology that will shape how the world
00:42:36 operates and that there needs to be government as part of that answer and so that's why we've we've we've done a
00:42:42 number of different congressional testimonies we interact with a number of different lawmakers and the you know
00:42:49 right now a lot of our message to them is that it's not the time for regulation it is the time for measurement right
00:42:58 that our main policy recommendation is that people and you know the government does this all the time with bodies like
00:43:04 NIST spend time trying to figure out just where the technology is how fast it's moving and can really become
00:43:11 literate and up to speed with respect to what to expect so I think that today the answer really
00:43:17 is about about about measurement and I think if there will be a time in place where that will change and I think it's
00:43:24 a little bit hard to predict exactly I what what exactly that trajectory should look like so there will be a point
00:43:32 oh it's regulation federal in the United States the government steps in and and helps be the I don't want to say the
00:43:40 adult in the room to make sure that there is strict rules may be conservative rules that nobody can cross
00:43:46 well I think there's this kind of maybe to two angles to it so today with narrow AI applications that I think there are
00:43:52 already existing bodies that are responsible and should be responsible for regulation you think about for
00:43:57 example with self-driving cars that you want the you know the National Highway it's exactly to be very good mat that
00:44:04 makes sense right that basically what we're saying is that we're going to have these technological systems that are
00:44:10 going to be do performing applications that humans already do great we already have ways of thinking about standards
00:44:16 and safety for those so I think actually empowering those regulators today is also pretty important
00:44:23 and then I think for for a GI you know that there's going to be a point where we'll have better answers and I think
00:44:28 that maybe a similar approach of first measurement and you know start thinking about what the rules should be I think
00:44:33 it's really important that we don't prematurely squash you know progress I think it's very easy to kind of smother
00:44:40 the budding field and I think that's something to really avoid but I don't think it's the right way of doing it is
00:44:46 to say let's just try to blaze ahead and not involve all these other stakeholders so you've recently released a paper on
00:44:59 GPT two language modeling but did not release the full model because you have concerns about the possible negative
00:45:06 effects of the availability of such model it's  outside of just that decision is super interesting because of
00:45:15 the discussion as at a societal level the discourse it creates so it's fascinating in that aspect but if you
00:45:22 think that's the specifics here at first what are some negative effects that you envisioned and of course what are some
00:45:29 of the positive effects yeah so again I think to zoom out like the way that we thought about GPT 2 is that with
00:45:37 language modeling we are clearly on a trajectory right now where we scale up our models and we get qualitatively
00:45:45 better performance right GPT 2 itself was actually just a scale-up of a model that we've released in the previous June
00:45:51 right and we just ran it at you know much larger scale and we got these results we're suddenly starting to write
00:45:57 coherent prose which was not something we'd seen previously and what are we doing now well we're gonna scale up GPT
00:46:05 2 by 10x by hundred X by thousand X and we don't know what we're going to get and so it's very clear that the model
00:46:11 that we were that we released last June you know I think it's kind of like it's it's it's it's a good academic toy it's
00:46:18 not something that we think is something that can really have negative applications or you know to the sense
00:46:23 that it can the positive of people being able to play with it is you know far far outweighs the possible harms you fast
00:46:32 forward to not GPT to buy GPU 20 and you think about what that's gonna be like and I think that the capabilities
00:46:38 are going to be substantive and so if there needs to be a point in between the two where you say this is something
00:46:45 where we are drawing the line and that we need to start thinking about the safety aspects and I think for GPT too
00:46:50 we could have gone either way and in fact when we had conversations internally that we had a bunch of pros
00:46:55 and cons and it wasn't clear which one which one outweighed the other and I think that when we announced that hey we
00:47:02 decide not to release this model then there was a bunch of conversation where various people said it's so obvious that
00:47:06 you should have just released it there other people said it's so obvious you should not have released it and I think
00:47:10 that that almost definitionally means that holding it back was the correct decision right if it's contra if there's
00:47:16 if it's not obvious whether something is beneficial or not you should probably default to caution and so I think that
00:47:22 the overall landscape for how we think about it is that this decision could have gone either way there are great
00:47:28 arguments in both directions but for future models down the road and possibly sooner than you'd expect because you
00:47:33 know scaling these things up doesn't have to take that long those ones but you're definitely not going to want to
00:47:40 release into the wild and so I think that we almost view this as a test case and to see can we even design you know
00:47:47 how do you have a society or how do you have a system that goes from having no concept of responsible disclosure where
00:47:53 the mere idea of not releasing something for safety reasons is unfamiliar to a world where you say okay we have a
00:47:59 powerful model let's at least think about it let's go through some process and you think about the security
00:48:03 community it took them a long time to design responsible disclosure right you know you think about this question of
00:48:09 well I have a security exploit I send it to the company the companies like tries to prosecute me or just sit just ignores
00:48:16 it what do I do right and so you know the alternatives of oh I just just always publish your exploits that
00:48:20 doesn't seem good either right and so it really took a long time and took this this it was bigger than any individual
00:48:26 right is really about building the whole community that believed that okay we'll have this process where you send it to
00:48:30 the company you know if they don't act in a certain time then you can go public and you're not a bad person you've done
00:48:37 the right thing and I think that in AI part of the response of gbt to just proves that we don't have any concept of this
00:48:47 so that's the high level picture and so I think that I think this was this was a really important move to make and
00:48:53 we could have maybe delayed it for D BT 3 but I'm really glad we did it for GPT too and so now you look at GPT 2 itself
00:48:58 and you think about the substance of okay what are potential negative applications so you have this model
00:49:04 that's been trained on the Internet which you know it's also going to be a bunch of very biased data a bunch of you
00:49:08 know very offensive content and there and you can ask it to generate content for you on basically any topic right you
00:49:15 just give it a prompt and we'll just start start writing and all writes content like you see on the internet you
00:49:20 know even down to like saying advertisement in the middle of some of its generations and you think about the
00:49:27 possibilities for generating fake news or abusive content and you know it's interesting seeing what people have done
00:49:32 with you know we released a smaller version of GPT too and the people have done things like try to generate now I
00:49:39 you know take my own Facebook message history and generate more Facebook messages like me and people generating
00:49:48 fake politician content or you know there's a bunch of things there where you at least have to think is this going
00:49:55 to be good for the world there's the flip side which is I think that there's a lot of awesome applications that we
00:50:00 really want to see like creative applications in terms of if you have sci-fi authors that can work with this
00:50:06 tool and come up with cool ideas like that seems that seems awesome if we can write better sci-fi through the use of
00:50:11 these tools and we've actually had a bunch of people write in to us asking hey can we use it for you know for a
00:50:17 variety of different creative applications so the positive I actually pretty easy to imagine there if you know
00:50:25 the usual NLP applications are really interesting but let's go there it's kind of
00:50:33 interesting to think about a world where look at Twitter where that just fake news but smarter and smarter BOTS being
00:50:44 able to spread in an interesting complex in that working way in information that just floods out us regular human beings
00:50:54 with our original thoughts so what are your views of this world with deep t20 right what are you how do we think
00:51:02 about again it's like one of those things about in the 50s trying to describe the the internet or the
00:51:09 smartphone what do you think about that world the nature of information do we and one possibility is that we'll always
00:51:17 try to design systems that identify it robot versus human and we'll do so successfully and so we will authenticate
00:51:24 that we're still human and the other world is that we just accept the part the fact that we're swimming in a sea of
00:51:31 fake news and just learn to swim there well have you ever seen the there so you know popular meme of of robot eye with a
00:51:41 physical physical arm and pen clicking the I'm not a robot button yeah I think I think the truth is that that really
00:51:50 trying to distinguish between robot and human is a losing battle ultimately you think it's a losing battle I think it's
00:51:55 a losing battle ultimately right I think that that is that in terms of the content in terms of the actions that you
00:52:00 can take I mean think about how captures have gone alright the captures used to be a very
00:52:05 nice simple you have this image all of our OCR is terrible you put a couple of of artifacts in it you know humans are
00:52:11 gonna be able to tell what what it is an AI system wouldn't be able to today like I can barely do CAPTCHAs yeah and I
00:52:17 think that that this is just kind of where we're going I think CAPTCHAs where we're a moment in time thing and as AI
00:52:22 you systems become more powerful that they're being human capabilities that can be measured in a very easy automated
00:52:29 way that the a eyes will not be capable of I think that's just like it's just an increasingly hard technical battle but
00:52:36 it's not that all hope is lost right and you think about how do we already authenticate ourselves right the you
00:52:42 know we have systems we have social security numbers if you're in the u.s. or you know you have you have  you
00:52:48 know ways of identifying individual people and having real world identity tied to to digital identity seems like a
00:52:55 step towards you know authenticating the source of content rather than the content itself now there are problems
00:53:01 with that how can you have privacy and unanimity in a world where the only content you can really trust is or the
00:53:06 only way you can trust content is by looking at where it comes from and so I think that building out good reputation
00:53:13 networks maybe maybe one possible solution but yeah I think that this this question is it's not an obvious one and
00:53:19 I think that we you know maybe sooner than we think we'll be in a world where you know today I often will read a tweet
00:53:25 and be like I feel like a real human wrote this or you know don't feel like this is like genuine I feel like I kind
00:53:30 of judge the content a little bit and I think in the future it just won't be the case you will get for example the FCC
00:53:37 comments on net neutrality it came out later that millions of those were auto-generated and that the researchers
00:53:42 were able to do various statistical tik techniques to do that what do you do in a world where those statistical
00:53:47 techniques don't exist it's just impossible to tell the difference between humans at any highs and in fact
00:53:53 the the the the most persuasive arguments are written by by AI all that stuff it's not sci-fi anymore you okay
00:54:00 GPT to making a great argument for why recycling is bad for the world you got to read that be like huh you're right
00:54:08 yeah that's that's quite interesting I mean ultimately it boils down to the physical world being the last frontier
00:54:14 of proving so you said like basically networks of people humans vouching for humans in the physical world and somehow
00:54:23 the authentication and ends there I mean if I had to ask you I mean you're way too eloquent for a human so if I had to
00:54:31 ask you to authenticate like prove how do I know you're not a robot and how do you know I'm not a robot you know I
00:54:39 think that's so far were this in the space this conversation we just had the physical movements we did is the biggest
00:54:47 gap between us and AI systems is the physical relation so maybe that's the last frontier well here's another
00:54:55 question is is you know why why is why is solving this problem important right like what aspects are really important
00:55:00 to us I think that probably where we'll end up is will hone in on what do we really want out of knowing if we're
00:55:08 talking to a human and and I think that again this comes down to identity and so I think that the Internet of the future
00:55:13 I expect to be one that will have lots of agents out there that will interact with with you but I think that the
00:55:19 question of is this you know a real flesh-and-blood human or is this an automated system
00:55:26 be less important let's actually go there it's GPT two is impressive and let's look at GPT 20 why is it so bad
00:55:39 that all my friends are GPT 20 well why is it so why is it so important on the internet do you think to interact with
00:55:48 only human beings why can't we live in a world where ideas can come from models trained on human data yeah I think this
00:55:54 is I think is actually a really interesting question this comes back to the how do you even picture a world with
00:55:59 some new technology right and I think that that one thing I think is important is is you know Gosei honesty and I
00:56:06 think that if you have you know almost in the Turing test style sense sense of technology you have a eyes that are
00:56:13 pretending to be humans and deceiving you I think that is you know that that feels like a bad thing right I think
00:56:19 that it's really important that we feel like we're in control of our environment right that we understand who we're
00:56:24 interacting with and if it's an AI or a human that that's not something we're being deceived about but I think that
00:56:30 the flipside of can I have as a meaningful of an interaction with an AI as I can with a human well I actually
00:56:37 think here you can turn to sci-fi and her I think is a great example of asking this very question right and one thing I
00:56:42 really love about her is it really starts out almost by asking how meaningful are human virtual
00:56:48 relationships right and and then you have a human who has a relationship with an AI and that you really start to be
00:56:54 drawn into that right and that all of your emotional buttons get triggered in the same way as if there was a real
00:56:59 human that was on the other side of that phone and so I think that that this is one way of thinking about it is that I
00:57:06 think that we can have meaningful interactions and that if there's a funny joke some sense it doesn't really matter
00:57:12 if it was written by a human or an AI but what you don't want anyway I think we should really draw hard lines is deception
00:57:19 and I think that as long as we're in a world where you know why do why do we build AI systems at all alright the
00:57:23 reason we want to build them is to enhance human lives to make humans be able to do more things to have human
00:57:29 humans feel more fulfilled and if we can build AI systems that do that I you know sign me up so the process of language modeling
00:57:38 how far do you think it take us let's look at movie her do you think a dialog natural language conversation is
00:57:46 formulated by the Turing test for example do you think that process could be achieved through this kind of
00:57:53 unsupervised language modeling so I think the Turing test in it seems real form isn't just about language right
00:58:00 it's really about reasoning to write that to really pass the Turing test I should be able to teach calculus to
00:58:06 whoever's on the other side and have it really understand calculus and be able to you know go and solve new calculus
00:58:12 problems and so I think that to really solve the Turing test we need more than what we're seeing with language models
00:58:17 we need some way of plugging and reasoning now how different will that be from what we already do that's an open
00:58:24 question right might be that we need some sequence of totally radical new ideas or it might be that we just need
00:58:30 to kind of shape our existing systems in a slightly different way but I think that in terms of how far language
00:58:36 modeling will go it's already gone way further than many people would have expected right I think that things like
00:58:41 and I think there's a lot of really interesting angles to poke in terms of how much does GBT to understand physical
00:58:49 world like you know you you read a little bit about fire under water in ng bt - so it's like okay maybe it doesn't
00:58:54 quite understand what these things are but at the same time I think that you also see various things like smoke
00:59:01 coming from flame and you know a bunch of these things that gbg - it has no body it is no physical experience it's
00:59:08 just statically read data and I think that I think that if the answer is like we don't know yet
00:59:14 then these questions though we're starting to be able to actually ask them to physical systems the real systems
00:59:19 that exist and that's very exciting do you think what's your intuition do you think if you just scale language
00:59:27 modeling maintain like significantly scale that reasoning can emerge from the same exact mechanisms I think it's
00:59:33 unlikely that if we just scale gbt - that will have reasoning in the full-fledged way and I think that there
00:59:40 is like you know the type signature is a little bit wrong right that like there's something we do with that we call
00:59:46 thinking right where we spend a lot of compute like a variable amount of compute get to better answers right I think a
00:59:52 little bit harder I get a better answer and that that kind of type signature isn't quite encoded in a gbt all right G
01:00:01 BT well kind of like it's been a long time and it's like evolutionary history baking and all this information getting
01:00:06 very very good at this predictive process and then at runtime I just kind of do one forward pass and and am able
01:00:14 to generate stuff and so you know there might be small tweaks to what we do in order to get the type signature right
01:00:19 for example well you know it's not really one forward pass right you know you generate symbol by symbol and so
01:00:24 maybe you generate like a whole sequence of thoughts and you only keep like the last bit or something right but I
01:00:29 think that at the very least I would expect you have to make changes like that yeah yeah just exactly how we you
01:00:36 said think is the process of generating thought by thought in the same kind of way you like you said keep the last bit
01:00:43 the thing that we converge towards you know and I think there's there's another piece which is which is interesting
01:00:48 which is this out of distribution generalization right that like thinking somehow lets us do that right that we
01:00:54 have an experience a thing and yet somehow we just kind of keep refine our mental model of it this is again
01:01:01 something that feels tied to whatever reasoning is and maybe it's a small tweak to what we do maybe it's many
01:01:07 ideas and we'll take as many decades yeah so the the assumption they're generalization out of distribution is
01:01:16 that it's possible to create new new ideas the pot you know it's possible that nobody's ever creating new ideas
01:01:25 and then was scaling GPT 2 to GPT 20 you would you would essentially generalize to all possible thoughts the Aussie was
01:01:34 gonna have I think just to play devil's ne how many new new story ideas have we come up with since Shakespeare right
01:01:42 yeah exactly it's just all different forms of love and drama and so on okay not sure if you read bitter lesson a
01:01:50 recent blog post by Ray Sutton no I have he basically says something that echoes some of the ideas that you've been
01:01:57 talking about which is he says the biggest lesson that can be read from so many years of AI research is that
01:02:02 general methods the leverage computation are ultimately going to ultimately win out do you agree with
01:02:12 this so basically and openly I in general about the ideas you are exploring about coming up with methods
01:02:17 whether it's GPT to modeling or whether its opening i-5 playing dota or a general method is better than a more
01:02:29 fine-tuned expert to tuned a method yeah so I think that well one thing that I think was really interesting about the
01:02:35 reaction to that blog post was that a lot of people have read this as saying that compute is all that matters and
01:02:41 it's a very threatening idea right and I don't think it's a true idea either right it's very clear that we have
01:02:46 algorithmic ideas that have been very important for making progress and to really build a GI you want to push as
01:02:51 far as you can on the computational scale and you want to push as far as you can on human human ingenuity and so I
01:02:57 think you need both but I think the way that you phrase the question is actually very good right that it's really about
01:03:03 what kind of ideas should we be striving for and absolutely if you can find a scalable idea you'd pour more compute
01:03:10 into you pour more data into it it gets better like that's that's the real Holy Grail and so I think that the answer to
01:03:18 the question I think is yes that that's really how we think about it that part of why we're excited about the power of
01:03:24 deep learning the potential for building an AGI is because we look at the system that exists in the most successful AI
01:03:32 systems and we realize that you scale those up they're gonna work better and I think that that scalability is something
01:03:37 that really gives us hope for being able to build transformative systems so I'll tell you this is a partially an
01:03:44 emotional you know a thing that responds that people often have is computers so important for state-of-the-art
01:03:50 performance you know individual developers maybe a 13 year old sitting somewhere in Kansas or something like
01:03:55 that you know they're sitting they that might not even have a GPU and or may have a single GPU a 1080 or something
01:03:59 like that and there's this feeling like well how can I possibly compete or contribute to
01:04:08 this world of AI if scale is so important so for if you can comment on that and in general do you think we need
01:04:16 to also in the future focus on democratizing compute resources more more or as much as we democratize the
01:04:23 algorithms well so the way that I think about it is that there's this space of a possible progress right there's a space
01:04:31 of ideas and sort of systems that will work that will move us forward and there's a portion of that space and to
01:04:36 some extent increasingly significant portion in that space that does just require massive compute resources and
01:04:44 for that fit I think that the answer is kind of clear and that part of why we have this structure that we do is
01:04:49 because we think it's really important to be pushing the scale and to be you know building these large clusters and
01:04:55 systems but there's another part portion of the space that isn't about the large scale compute that are these ideas that
01:05:01 and again I think that for the a is to really be impactful and really shine that they should be ideas that if you
01:05:06 scaled them up would work way better than they do at small scale but you can discover them without
01:05:12 massive computational resources and if you look at the history of recent developments you think about things like
01:05:18 began or the VA II that these are ones that I think you could come up with them without having and you know in practice
01:05:23 people did come up with with them without having massive massive computational resources alright I just
01:05:28 talked to being good fellow but the thing is the initial gaen produce pretty terrible results right so only because
01:05:36 it was in a very specific it was because only because they're smart enough to know that this is quite surprising can
01:05:43 generate anything that they know and do you see a world there's that too optimistic and dreamer like to imagine
01:05:50 that the compute resources are something that's owned by governments and provided as utility actually some extent this
01:05:58 this question reminds me of of blog post from one of my former professors at Harvard this guy map Matt Welsh
01:06:04 who was a systems professor I remember sitting in his tenure talk right and you know that he had literally just gotten
01:06:10 tenure he went to Google for the summer and I then decided he wasn't going back it's academia right and that kind of in
01:06:18 his bog post makes this point that look as a systems researcher that I come with these cool system ideas right and I kind
01:06:25 of a little proof of concept and the best thing I can hope for is that the people at Google or Yahoo
01:06:32 which was around at the time I will implement it and like actually make it work at scale
01:06:36 right that's like the dream for me right I built the little thing and they the big thing that's actually working and
01:06:43 for him he said I'm done with that I want to be the person who's who's actually doing this building and and
01:06:48 deploying and I think that there's a similar dichotomy here right I think that there are people who really
01:06:53 actually find value and I think it is a valuable thing to do to be the person who produces those ideas right who
01:06:59 builds the proof of a concept and yeah you don't get to generate the coolest possible Ganim ajiz but you invent it
01:07:05 again right and so that there's that there's there's a real trade-off there and I think that's a very personal
01:07:09 choice but I think there's value in both sides do you think creating AGI something or some new models would we
01:07:20 would see echoes of the brilliance even at the prototype level so you would be able to develop those ideas without
01:07:29 scale the initial so seeds you know I always like to look at at examples that exist right look at real precedent and
01:07:36 so take a look at the June 2018 model that we released that we scaled up to turn into GPT - and you can see that at
01:07:42 small scale it set some records right this was you know the devotional GPT we actually had some some cool generations
01:07:48 that weren't nearly as amazing and really stunning as the GPT - ones every but it was promising it was interesting
01:07:55 and so I think it is the case that with a lot of these ideas do you see prominence at small-scale but
01:07:59 there is an asterisk here a very big asterisk which is sometimes we see behaviors that emerge that are
01:08:07 qualitatively different from anything we saw it's small scale and that the original inventor of whatever algorithm
01:08:14 looks at and says I didn't think it could do that this is what we saw in DotA all right so PPO was was created by John
01:08:21 Schulman who's a researcher here and and with with dota we basically just ran PPO at massive massive scale and I there's
01:08:29 some tweaks and in order to make it work but fundamentally it's PPO with the core and we were able to get this long-term
01:08:36 planning these behaviors to really play out on a time scale that we just thought was not possible and John looked at that
01:08:43 and it was like I didn't think it could do that that's what happens when you're at three
01:08:47 orders of magnitude more scale contest to that yeah but it still has the same flavors of you know at least echoes of
01:08:56 the expected billions although I suspect with GPT is scaled more and more you might get surprising things so yeah yeah
01:09:04 you're right it's it's interesting that it's it's difficult to see how far an idea will go when it's scaled it's an
01:09:10 open question we've also at that point with with dota and PPO like I mean here's a very
01:09:16 concrete one right it's like it's actually one thing that's very surprising about dota that I think
01:09:19 people don't really pay that much attention to is the decree of generalization out of distribution that
01:09:25 happens right that you have this AI that's trained against other bots for its entirety the entirety of its
01:09:31 existence sorry to take a step back and you can't talk through in his you know a story of dota a story of leading up to
01:09:41 opening high five and that passed and what was the process of self play it's a lot of training yeah yeah yeah yeah so
01:09:47 with donors dota yeah it's a complex video game and we started training we started trying to
01:09:53 solve dota because we felt like this was a step towards the real world relative to other games like chess or go right
01:09:59 those various free board games where you just kind of have this board very discrete moves dota starts to be much
01:10:04 more continuous time so you have this huge variety of different actions that you have a 45 minute game with all these
01:10:10 different units and it's got a lot of messiness to it that really hasn't been captured by previous games and famously
01:10:17 all of the hard-coded bots for dota were terrible right just impossible to write anything good for it because it's so
01:10:22 complex and so this seems like a really good place to push what's the state of the art in reinforcement learning and so
01:10:28 we started by focusing on the one versus one version of the game and and and were able to solve that we were able to beat
01:10:34 the world champions and that the learning you know the skill curve was this crazy exponential right it was like
01:10:41 constantly we were just scaling up that we were fixing bugs and you know that you look at the at the skill curve and
01:10:46 it was really very very smooth one it's actually really interesting to see how that like human iteration loop yielded
01:10:53 very steady exponential progress and to want one side note first of all it's an exceptionally popular video game this
01:10:59 effect is that there's a lot of incredible human experts at that video again so the benchmark the trying to
01:11:06 reach is very high and the other can you talk about the approach that was used initially and throughout training these
01:11:12 agents to play this game yep and so they person that we used is self play and so you have cue agents they don't know
01:11:18 anything they battle each other they discover something a little bit good and now they both know it and they just get
01:11:23 better and better and better without bound and that's a really powerful idea right that we then went from the one
01:11:30 versus one version of the game and scaled up to four five versus five right so you think about kind of like with
01:11:35 basketball where you have this like team sport you know I need to do all this coordination and we were able to push
01:11:44 the same idea the same self play to to really get to the professional level at the full thigh versus by version of the
01:11:51 game and and and the things I think are really interesting here is that these agents in some ways they're almost like
01:11:57 an insect like intelligence right where the you know there's they've a lot in common with how an insect is trained
01:12:01 right insect kind of lives in this environment for a very long time or you know the the ancestors of this insect
01:12:06 I've been around for a long time and had a lot of experience it gets baked into into into this agent and you know it's
01:12:12 not really smart in the sense of a human right it's not able to go and learn calculus but it's able to navigate its
01:12:17 environment extremely well and simple they handle unexpected things in the environment that's never seen before
01:12:23 pretty well and we see the same sort of thing with our dota BOTS right they're able to in within this
01:12:28 game they're able to play against humans which are something that never existed in its evolutionary environment totally
01:12:33 different playstyles from humans versus the bots and yet it's able to handle it extremely well and that's something I
01:12:40 think was very surprising to us was something that doesn't really emerge from what we've seen with PPO at smaller
01:12:47 scale writing the kind of scale we're running the stuff out was you know I could take a hundred thousand CPU cores
01:12:54 running with like hundreds of GPUs it's probably about I you know like you know it's something like hundreds of years of
01:13:01 experience going into this bot every single real day and so that scale is massive and we start to see very
01:13:09 different kinds of behaviors out of the algorithms that we all know and love Dora he mentioned beat the world expert 1v1
01:13:19 and then you didn't weren't able to win 505 this year yeah at the best in the world so what's what's the comeback
01:13:27 story what's first of all talk through that does exceptionally exciting event and what's what's the following months
01:13:33 and this year look like yeah yeah so well one thing that's interesting is that you know we lose all the time
01:13:41 because we we so the dota team at opening I we played the bot against better players than our system all the
01:13:47 time or at least we used to it right like you know the the first time we lost publicly was we went up on stage at the
01:13:53 International and we played against some of the best teams in the world and we ended up losing both games but we gave
01:13:58 them a run for their money right the both games were kind of 30 minutes 25 minutes and that they went
01:14:03 back and forth back and forth back and forth and so I think that really shows that we're at the professional level and
01:14:09 that kind of looking at those games we think that the coin could have gone a different direction and it could have
01:14:14 could have had some wins and so that was actually very encouraging for us and you know it's interesting because the
01:14:19 international was at a fixed time right so we we knew exactly what day we were going to be playing and we pushed as far
01:14:24 as we could as fast as we could two weeks later we had a bot that had an 80% win rate versus the one that played
01:14:31 at ti so the march of progress you know you should think of as a snapshot rather than as an end state and so in fact well
01:14:38 we'll be announcing our our finals pretty soon I actually think that we'll announce our final match I prior to this
01:14:45 podcast being released Cassell's there should be will be playing will be playing against the the world champions
01:14:51 and you know for us it's really less about like that the way that we think about what's upcoming is the final
01:14:59 milestone the file competitive milestone for the project right that our goal in all of this isn't really about beating
01:15:06 humans at dota our goal is to push the state of the art and reinforcement learning and we've done that right and
01:15:10 we've actually learned a lot from our system and that we have I you know I think a lot of exciting next steps that
01:15:15 we want to take and so you know kind of a final showcase of what we built we're going to do this match but for us it's
01:15:21 not really the success or failure to see you know do do we have the coin flip go in our
01:15:26 direction or against where do you see the field of deep learning heading in the next few years what do you see the
01:15:35 work and reinforcement learning perhaps heading and more specifically with open AI all the exciting projects that you're
01:15:43 working on what is 2019 hold for you massive scale scale I will put a naturist on that and
01:15:50 just say you know I think that it's about ideas plus scale you need both so that's a really good point so the
01:15:59 question in terms of ideas you have a lot of projects that are exploring different areas of intelligence and the
01:16:07 question is when you when you think of scale do you think about growing scale those individual projects so do you
01:16:13 think about adding new projects and society today in if you are thinking about adding new projects or if you look
01:16:20 at the past what's the process of coming up with new projects and new ideas so we really have a life cycle of project here
01:16:26 so we start with a few people just working on a small scale idea and language is actually a very good example
01:16:31 of this that it was really you know one person here who was pushing on language for a long time I mean then you get
01:16:37 signs of life right and so this is like let's say you know with with the original gbt we had something that was
01:16:43 interesting and we said okay it's time to scale this right it's time to put more people on it put more computational
01:16:50 resources behind it and and then we just kind of keep pushing and keep pushing and the end state is something that
01:16:54 looks like dota or robotics where you have a large team of you know 10 or 15 people that are running things at very
01:17:00 large scale and that you're able to really have material engineering and and and and you know sort of machine
01:17:07 learning science coming together to make systems that work and get material results that just would've been
01:17:12 impossible otherwise so we do that whole lifecycle we've done it a number of times you know typically end to end it's
01:17:20 probably to two years or so to do it I you know the organization's been around for three years so maybe we'll find it
01:17:24 we also have longer life cycle projects but you know we we will work up to those we have so so one one team that we were
01:17:32 actually just starting Illya and I are kicking off a new team called the reasoning team
01:17:36 and that this is to really try to tackle how do you get neural networks to reason and we think that this will be a
01:17:43 long-term project and we're very excited about in terms of reasoning super exciting topic woody what kind of
01:17:52 benchmarks what kind of tests of reasoning oh do you envision what what would if you set back with whatever
01:18:00 drink and you would be impressed that this system is able to do something what would that look like not fear improving
01:18:08 they are improving so some kind of logic and especially mathematical logic I think so right I think that there's
01:18:13 there's there's kind of other problems that are dual to if you're improving in particular you know you think about
01:18:18 programming I think about even like security analysis of code that these all kind of capture the same sorts of core
01:18:26 reasoning and being able to do some amount of distribution generalization it would be quite exciting if open ai
01:18:33 reasoning team was able to prove that P equals NP that would be very nice I be very very very exciting especially if it
01:18:40 turns out the P equals NP that'll be interesting too it just it would be ironic and humorous
01:18:49 you know so what problem stands out to you is  the most exciting and challenging
01:18:55 impactful to the work for us as a community in general and for open AI this year he mentioned reasoning I think
01:19:01 that's that's a heck of a problem yeah so I think reasoning is an important one I think it's gonna be hard to get good
01:19:06 results in 2019 you know again just like we think about the life cycle takes time I think for 2019 language modeling seems
01:19:12 to be kind of on that ramp right it's at the point that we have a technique that works we want to scale 100 X thousand X
01:19:17 see what happens awesome do you think we're living in a simulation I think it's I think it's
01:19:24 hard to have a real opinion about it I you know it's actually interesting I separate out things that I think can
01:19:30 have like you know yield materially different predictions about the world from ones that are just kind of you know
01:19:36 fun fun to speculate about and I kind of view simulation it's more like is there a flying teapot between Mars and Jupiter
01:19:43 like maybe but it's a little bit hard to know what that would mean for my life so there is something actionable I'd so
01:19:50 some of the best work opening has done is in the field of reinforcement learning and some of the success of
01:19:57 reinforcement learning come from being able to simulate the problem you trying to solve so it do you have a hope for
01:20:04 reinforcement for the future of reinforcement learning and for the future of simulation like what we're
01:20:09 talking about autonomous vehicles or any kind of system do you see that scaling so we'll be able to simulate systems and
01:20:17 enhance be able to create a simulator that echoes our real world and proving once and for all even though you're
01:20:23 denying it that we're living in a simulation question right so you know kind for the core thereof like can we
01:20:30 use simulation for self-driving cars take a look at our robotic system dactyl right that was trained in simulation
01:20:38 using the DOTA system in fact and it transfers to a physical robot and I think everyone looks at our dota system
01:20:43 the wreck okay it's just a game how are you ever going to escape to the real world and the answer is well we did it with
01:20:47 the physical robot the noble could program and so I think the answer is simulation goes a lot further than you
01:20:53 think if you apply the right techniques to it now there's a question of you know are the beings in that simulation gonna
01:20:59 wake up and have consciousness I think that one seems a lot a lot harder to again reason about I think that you know
01:21:05 you really should think about like where where exactly just human consciousness come from and our own self-awareness and
01:21:10 you know is it just that like once you have like a complicated enough neural net do you have to worry about the
01:21:17 agents feeling pain and I think there's like interesting speculation to do there but but you know again I think it's a
01:21:23 little bit hard to know for sure well let me just keep with a speculation do you think to create intelligence general
01:21:30 intelligence you need one consciousness and to a body do you think any of those elements are needed or as intelligence
01:21:38 something that's that's orthogonal to those I'll stick to the kind of like the the non grand answer first
01:21:43 right so the non grand answer is just to look at you know what are we already making work yoga GPG to a lot of people
01:21:48 would have said that even get these kinds of results you need real-world experience you need a body you need
01:21:53 grounding how are you supposed to reason about any of these things how are you supposed to like even kind of know about
01:21:57 smoke and fire and those things if you've never experienced them and GPT two shows
01:22:02 it you can actually go way further than that kind of reasoning would predict so I think that the the in terms of doing
01:22:11 any consciousness do we need a body it seems the answer is probably not right that we can probably just continue to
01:22:16 push kind of the systems we have they already feel general they're not as competent or as general or able to learn
01:22:23 as quickly as an aged guy would but you know they're at least like kind of proto AGI in some way and they don't need any
01:22:31 of those things now now let's move to the grand answer which is you know if our neural next Nets conscious already
01:22:38 would we ever know how can we tell right yeah here's where the speculation starts become become you know at least
01:22:45 interesting or fun and maybe a little bit disturbing it depending on where you take it but it certainly seems that when
01:22:51 we think about animals that there's some continuum of consciousness you know my cat I think is is conscious in some way
01:22:57 right I you know not as conscious as a human and you could imagine that you could build a little consciousness meter
01:23:02 right you pointed a cat gives you a little reading we ran a human gives you much bigger reading what would happen if
01:23:08 you pointed one of those at a dota neural net and if your training of this massive simulation do the neural nets
01:23:15 feel pain you know it becomes pretty hard to know that the answer is no and it becomes pretty hard to to really
01:23:22 think about what that would mean if the answer were yes and it's very possible you know for example you could imagine
01:23:30 that maybe the reason these humans are have consciousness is because it's a it's a convenient computational shortcut
01:23:35 all right if you think about it if you have a being that wants to avoid pain which seems pretty important to survive
01:23:41 in this environment I'm and once you like you know eat food then that may be the best way of doing it is to have a
01:23:47 being that's conscious right that you know in order to succeed in the environment you need to have those
01:23:51 properties and how are you supposed to implement them and maybe this this consciousness is way of doing that if
01:23:57 that's true then actually maybe we should expect that really competent reinforcement learning agents will also
01:24:02 have consciousness but you know it's a big if and I think there a lot of other arguments they can make in other
01:24:07 directions I think that's a really interesting idea that even GPT to has some degree of consciousness that's
01:24:14 something is actually not as crazy to think about it's useful to think about as we think
01:24:18 about what it means to create intelligence of a dog intelligence of a cat and the intelligence of human so
01:24:29 last question do you think we will ever fall in love like in the movie her with an artificial intelligence system or an
01:24:36 artificial intelligence system falling in love with a human I hope so if there's any better way to end it on
