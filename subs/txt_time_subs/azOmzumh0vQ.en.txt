00:00:02 what difference between biological neural networks and artificial neural networks is most mysterious captivating
00:00:11 and profound for you first of all there's so much we don't know about biological neural networks and that's
00:00:18 very mysterious and captivating because maybe it holds the key to improving our differential neural networks one of the
00:00:29 things I studied recently something that we don't know how biological neural networks do but would be really useful
00:00:39 for artificial ones is the ability to do credit assignment through very long time spans there are things that we can in
00:00:49 principle do with artificial neural nets but it's not very convenient and it's not biologically plausible and this
00:00:56 mismatch I think this kind of mismatch may be an interesting thing to study to a understand better how brains might do
00:01:05 these things because we don't have good corresponding theories with artificial neural Nets and B maybe provide new
00:01:17 ideas that we could explore about things that brain do differently and that we could incorporate in artificial neural
00:01:24 Nets so let's break created assignment up a little bit yeah what it's a beautifully technical term but it could
00:01:31 incorporate so many things so is it more on the RNN memory side that thinking like that or is it something about
00:01:39 knowledge building up common sense knowledge over time or is it more in the reinforcement learning sense that you're
00:01:47 picking up rewards over time for a particular to achieve certain kind of goals so I was thinking more about the
00:01:57 first two meanings whereby we store all kinds of memories episodic memories in our brain which we can access later in
00:02:11 order to help us both infer causes of things that we are observing now and assign credit to decisions or
00:02:21 interpretations we came up with a while ago when you know those memories were stored and then we can change the way we
00:02:30 would have reacted or interpreted things in the past and now that's credit assignment used for learning so in which
00:02:40 way do you think artificial neural networks the current LS TM the current architectures are not able to capture
00:02:52 the presumably you're thinking of very long term yes so current recurrent Nets are doing a fairly good jobs for
00:03:01 sequences with dozens or say hundreds of time stamps and then it gets harder and harder and depending on what you have to
00:03:08 remember and so on as you consider longer durations whereas humans seem to be able to do credit assignment through
00:03:18 essentially arbitrary times like I could remember something I did last year and then now because I see some new evidence
00:03:25 I'm gonna change my mind about the way I was thinking last year and hopefully not do the same mistake again I think a big
00:03:35 part of that is probably forgetting you're only remembering the really important things it's very efficient
00:03:43 forgetting yes so there's a selection of what we remember and I think there are really cool connection to higher-level
00:03:52 cognition here regarding consciousness deciding and and emotions like sort of deciding what comes to consciousness and
00:04:00 what gets stored in memory which which are not trivial either so you've been at the forefront there all along showing
00:04:09 some of the amazing things that neural networks deep neural networks can do in the field of artificial intelligence is
00:04:15 just broadly in all kinds of applications but we can talk about that forever but what in your view because
00:04:24 we're thinking towards the future is the weakest aspect of the way deep neural networks represent the world what is
00:04:32 that what is in your view is missing so currently current state-of-the-art neural nets trained on large quantities
00:04:44 of images or texts have some level of understanding of you know what explains those datasets but it's very basic it's
00:04:56 it's very low-level and it's not nearly as robust and abstract in general as our understanding okay so that doesn't tell
00:05:07 us how to fix things but I think it encourages us to think about how we can maybe train our neural nets differently
00:05:21 so that they would focus for example on causal explanations something that we don't do currently with neural net
00:05:30 training also one thing I'll talk about in my talk this afternoon is instead of learning separately from images and
00:05:40 videos on one hand and from text on the other hand we need to do a better job of jointly learning about language and
00:05:51 about the world to which it refers so that you know both sides can help each other we need to have good world models
00:06:01 in our neural nets for them to really understand sentences which talk about what's going on in the world and I think
00:06:11 we need language input to help provide clues about what high-level concepts like semantic concepts should be
00:06:20 represented at the top levels of these neural nets in fact there is evidence that the purely unsupervised learning of
00:06:30 representations doesn't give rise to high level representations that are as powerful as the ones we are getting from
00:06:37 supervised learning and so the the clues we're getting just with the labels not even sentences is
00:06:45 already very powerful do you think that's an architecture challenge or is it a data set challenge neither I'm
00:06:58 tempted to just end it there in your library of course data sets and architectures are something you want to
00:07:07 always play with but but I think the crucial thing is more the training objectives the training frameworks for
00:07:15 example going from passive observation of data to more active agents which learn by intervening in the world the
00:07:27 relationships between causes and effects the sort of objective functions which could be important to allow the the
00:07:38 highest level explanations to to to rise from from the learning which I don't think we have now the kinds of objective
00:07:47 functions which could be used to reward exploration the right kind of exploration so these kinds of questions
00:07:55 are neither in the dataset nor in the architecture but more in how we learn under what objectives and so on yeah
00:08:03 that's a afraid you mentioned in several contexts the idea is sort of the way children learn they interact with
00:08:09 objects of the world and it seems fascinating because it's some sense except with some cases in reinforcement
00:08:19 learning that idea is not part of the learning process in artificial neural network so it's almost like do you
00:08:26 envision something like an objective function saying you know what if you poke this object in this kind of way
00:08:36 would be really helpful for me to further yes further learn right right sort of almost guiding some aspect of learning
00:08:44 right right so I was talking to Rebecca Saxe just an hour ago and she was talking about lots and lots of evidence for
00:08:57 infants seem to clearly take what interest them in a directed way and so they're not passive learners they they
00:09:09 focus their attention on aspects of the world which are most interesting surprising in in a non-trivial way that
00:09:17 makes them change their theories of the world so that's a fascinating view of the future progress but Anna the more
00:09:30 maybe boring a question do you think going deeper and large so do you think just increasing the size of the things
00:09:39 that have been increasing a lot in the past few years will will also make significant progress so some of the
00:09:47 representational issues that you mentioned that is they're kind of shallow in some sense Oh higher in a
00:09:55 sense of abstraction up straight in a sense of abstraction they're not getting some I don't think that having more more
00:10:02 depth in the network in the sense of instead of a hundred layers we have ten thousand is going to solve our problem
00:10:10 you don't think so is that obvious to you yes what is clear to me is that engineers and companies and labs grad
00:10:19 students will continue to tune architectures and explore all kinds of tweaks to make the current state of the
00:10:27 Arts that he ever slightly better but I don't think that's gonna be nearly enough I think we need some fairly
00:10:32 drastic changes in the way that we're considering learning to achieve the goal that these learners actually understand
00:10:42 in a deep way the environment in which they are you know observing and acting but I guess I was trying to ask a
00:10:51 question is more interesting than just more layers is basically once you figure out a way to learn through interacting
00:11:01 how many parameters does it take to store that information so I think our brain is quite bigger than
00:11:08 most neural networks right right oh I see what you mean oh I I'm with you there so I agree that in order to build
00:11:16 neural nets with the kind of broad knowledge of the world that typical adult humans have probably the kind of
00:11:24 computing power we have now is going to be insufficient so well the good news is there are hardware companies building
00:11:30 neural net chips and so it's gonna get better however the good news in a way which is also a bad news is that even
00:11:42 our state-of-the-art deep learning methods fail to learn models that understand even very simple environments
00:11:51 like some Grid worlds that we have built even these fairly simple environments I mean of course if you train them with
00:11:57 enough examples eventually they get it but it's just like instead of what instead of what humans might need just
00:12:05 dozens of examples these things will need millions right for very very very simple tasks and so I think there's an
00:12:15 opportunity for academics who don't have the kind of computing power that say Google has to do really important and
00:12:23 exciting research to advance the state-of-the-art in training frameworks learning models agent learning in even
00:12:32 simple environments that are synthetic that seem trivial but yet current machine learning fails on we've talked
00:12:42 about priors and common-sense knowledge it seems like we humans take a lot of knowledge for granted so what what's
00:12:54 your view of these priors of forming this broad view of the world this accumulation of information and how we
00:13:01 can teach a neural networks or learning systems to pick that knowledge up so knowledge you know for a while the
00:13:10 artificial intelligence what's maybe in the 80 there's a time or knowledge representation knowledge
00:13:17 acquisition expert systems I mean though the symbolic AI was was a view was an interesting problem set to solve and it
00:13:26 was kind of put on hold a little bit it seems like because it doesn't work it doesn't work that's right but that's
00:13:35 right but the goals of that remain important yes remain important kind of how do you think those goals can be
00:13:41 addressed right so first of all I believe that one reason why the classical expert systems approach failed
00:13:51 is because a lot of the knowledge we have so you talked about common sense intuition there's a lot of knowledge
00:14:01 like this which is not consciously accessible the lots of decisions we're taking that we can't really explain even
00:14:11 if sometimes we make up a story and that knowledge is also necessary for machines to take good decisions and that
00:14:20 knowledge is hard to codify in expert systems rule-based systems and you know Costco EAJA formalism and there are
00:14:27 other issues of course with the old AI like not really good ways of handling uncertainty I would say something more
00:14:36 subtle which we understand better now but I think still isn't enough in the minds of people there is something
00:14:45 really powerful that comes from distributed representations the thing that really makes neural Nets work so
00:14:54 well and it's hard to replicate that kind of power in a symbolic world the knowledge in in expert systems and so on
00:15:05 is nicely decomposed into like a bunch of rules whereas if you think about a neural net it's the opposite you have
00:15:12 this big blob of parameters which work intensely together to represent everything the network knows and it's
00:15:20 not sufficiently factorized and so I think this is one of the weaknesses of current neural nets that we have to take
00:15:28 lessons from classically I in order to bring in another kind of compositionality which is common in
00:15:35 language for example and in these rules but that isn't so native to New Ulm Ed's and on that line of thinking
00:15:47 disentangled representations yes so so let me connect with disentangled representations if you might if don't
00:15:54 mind yes exactly so for many years I've thought and I still believe that it's really important that we come up with
00:16:01 learning algorithms either unsupervised or supervised but or enforcement whatever that build representations in
00:16:09 which the important factors hopefully causal factors are nicely separated and easy to pick up from the representation
00:16:16 so that's the idea of disentangle representations it says transform the data into a space where everything
00:16:23 becomes easy we can maybe just learn with linear models about the things we care about and and I still think this is
00:16:30 important but I think this is missing out on a very important ingredient which classically AI systems can remind us of
00:16:39 so let's say we have these design technologies invation you still need to learn about the the
00:16:45 relationships between the variables those high-level semantic variables they're not going to be independent I
00:16:49 mean this is like too much of an assumption they're gonna have some interesting relationships that allow to
00:16:55 predict things in the future to explain what happened in the past the kind of knowledge about those relationships in a
00:17:02 classically AI system is encoded in the rules like a rule is just like a little piece of knowledge that says oh I have
00:17:09 these two three four variables that are linked in this interesting way then I can say something about one or two of
00:17:15 them given a couple of others right in addition to disentangling the the elements of the representation which are
00:17:23 like the variables in rule-based system you also need to disentangle the the mechanisms that relate those variables
00:17:34 to each other so like the rules so the rules are neatly separated like each rule is you know living on its own and
00:17:41 when I change a rule because I'm learning it doesn't need to break other rules whereas current your Mets for example
00:17:48 are very sensitive to what's called catastrophic forgetting where after I've learned some things and then I learn new
00:17:55 things they can destroy the old things that I had learned right if the knowledge was better factorized and and
00:18:04 separated disentangled then you would avoid a lot of that now you can't do this in the sensory domain but a decent
00:18:15 okay like an pixel space but but my idea is that when you project the data in the right semantic space it becomes possible
00:18:22 to now represent this extra knowledge beyond the transformation from input to representations which is how
00:18:28 representations act on each other and predict the future and so on in a way that can be neatly disentangled so now
00:18:37 it's the rules or disentangle from each other and not just the variables that are disentangled from each other and you
00:18:43 draw a distinction between semantic space and pixel like yes there need to be an architectural difference or well
00:18:49 yeah so there's the sensory space like pixels which where everything is untangled the the information like the
00:18:56 variables are completely interdependent in very complicated ways and also computation like the it's not just
00:19:03 variables it's also how they are related to each other is is all intertwined but but I I'm hypothesizing that in the
00:19:12 right high-level representation space both the variables and how they relate to each other can be disentangled and
00:19:19 that will provide a lot of generalization power generalization power yes distribution of the test set
00:19:28 he assumed to be the same as a distribution of the training set right this is where current machine learning
00:19:36 is too weak it doesn't tell us anything is not able to tell us anything about how are you let's say our gonna
00:19:43 generalize to a new distribution and and you know people may think well but there's nothing we can say if we don't
00:19:48 know what the new distribution will be the truth is humans are able to generalize to new distributions how are
00:19:56 we able to do that so yeah because something these new distributions even though they could look very different
00:20:01 from the training solutions they have things in common so let me give you a concrete example you read a science
00:20:07 fiction novel the science fiction novel maybe you know brings you in some other planet where things look very different
00:20:17 on the surface but it's still the same laws of physics all right and so you can read the book and you understand what's
00:20:22 going on so the distribution is very different but because you can transport a lot of
00:20:30 the knowledge you had from Earth about the underlying cause and effect relationships and physical mechanisms
00:20:37 and all that and maybe even social interactions you can now make sense of what is going on on this planet where
00:20:43 like visually for example things are totally different taking that analogy further and
00:20:50 distorting it let's enter a sign science fiction world to say Space Odyssey 2001 with hell yeah or or maybe which is
00:21:00 probably one of my favourite AI movies and then then - and then there's another one that a lot of people love that it
00:21:07 may be a little bit outside of the AI community is ex machina right I don't know if you've seen it yes yes but what
00:21:14 are your views on that movie alright does it does are you able to wear things I like and things I hate so maybe you
00:21:24 could talk about that in the context of a question I want to ask which is  there's quite a large community of
00:21:30 people from different backgrounds often outside of AI who are concerned about existential threat of artificial
00:21:37 intelligence right you've seen now this community develop over time you've seen you have a perspective so what do you
00:21:43 think is the best way to talk about a a safety to think about it to have this course about it within AI community and
00:21:51 outside and grounded in the fact that ex machina is one of the main sources of information for the general public about
00:21:58 AI so I think I think you're putting it right there's a big difference between the sort of discussion we oughta have
00:22:06 within the AG community and the sort of discussion that really matter in the general public
00:22:12 so I think the the picture of terminator and you know AI lose and killing people and super intelligence that's gonna
00:22:22 destroy us whatever we try isn't really so useful for the public discussion because for the public discussion that
00:22:32 things I believe really matter are the short-term and mini term very likely negative impacts of AI on society
00:22:41 whether it's from security like you know Big Brother scenarios with face recognition or killer robots or the
00:22:47 impact on the job market or concentration of power and discrimination all kinds of social
00:22:55 issues which could actually some of them could really threaten democracy for example just to clarify when you said
00:23:02 killer robots you mean autonomous weapons yes weapon systems yes I do not terminator that's right
00:23:09 so I think these these short and medium-term concerns should be important parts of the public debate now
00:23:17 existential risk for me is a very unlikely consideration but still worth academic investigation in the same way
00:23:28 that you could say should we study what could happen if meteorite you know came to earth and destroyed it so I think
00:23:34 it's very unlikely that this is gonna happen and or happen it in a reasonable future it's it's very the the sort of
00:23:43 scenario of an AI getting loose goes against my understanding of at least current machine learning and current
00:23:49 neural nets and so on it's not plausible to me but of course I don't have a crystal ball and who knows what a I will
00:23:55 be in fifty years from now so I think it is worth at scientists study those problems it's just not a pressing
00:24:01 question as far as I'm concerned so before continuing down the line a few questions there but what what do you
00:24:09 like and not like about ex machina as a movie because I I actually watch it for the second time and enjoyed it I hated
00:24:17 it the first time and I enjoyed it quite a bit more the second time when I sort of learned to accept
00:24:25 certain pieces of it CC is the concept movie hi what was your experience wouldn't Laura your thoughts so the
00:24:34 negative is the picture it paints of science is totally wrong science in general and AI in particular science is
00:24:45 not happening in some hidden place by some you know really smart guy one person one person
00:24:52 this is totally unrealistic this is not how it happens even a team of people in some isolated
00:24:59 place will not make it science moved by small steps thanks to the collaboration and community of a
00:25:10 large number of people interacting and all the scientists who are expert in their field Canon Oh what is going on
00:25:19 even in the industrial labs its information flows and leaks and so on and and and the spirit of it is very
00:25:27 different from the way science is painted in this movie yeah let me let me ask on that on that point it's been the
00:25:34 case to this point yeah that kind of even if the research happens inside Google or Facebook inside companies it
00:25:40 still kind of comes out like yes come on absolutely think that will always be the case so there I is is it possible to
00:25:48 bottle ideas to the point where there's a set of breakthrough the go completely undiscovered by the general research
00:25:53 community do you think that's even possible it's possible but it's unlikely unlikely it's not how it is done now
00:26:03 it's not how I can foresee it in in the foreseeable future but of course I don't have a crystal ball and so who knows
00:26:15 this is science fiction after all but but usually the ominous that the lights went off during during that discussion
00:26:23 so the problem again there's a you know one thing is the movie and you could imagine all kinds of science fiction the
00:26:28 problem wouldn't for me may be similar to the question about existential risk is that this kind of movie pain
00:26:37 such a wrong picture of what is actual you know the actual science and how it's going on that that it can have
00:26:44 unfortunate effects on people's understanding of current science and and so that's kind of sad is it an important
00:26:55 principle in research which is diversity so in other words research is exploration resources explosion in the
00:27:03 space of ideas and different people will focus on different directions and this is not just good it's essential so I'm
00:27:12 totally fine with people exploring directions that are contrary to mine or look orthogonal to mine it's I I am more
00:27:22 than fine I think it's important I and my friends don't claim we have universal truth about what well especially about
00:27:29 what will happen in the future now that being said we have our intuitions and then we act accordingly according to
00:27:37 where we think we can be most useful and where society has the most gain or to lose we should have those debates and
00:27:47 and and and not end up in a society where there's only one voice and one way of thinking in research money is spread
00:27:57 out so disagreement is a sign of good research good science so yes the idea of bias in in the human sense of bias yeah
00:28:08 how do you think about instilling in machine learning something that's aligned with human values in terms of
00:28:15 bias we and intuitively human beings have a concept of what bias means of what fundamental respect for other human
00:28:23 beings means but how do we instill that into machine learning systems do you think so I think there are short-term
00:28:31 things that are already happening and then there are long-term things that we need to do and the short term there are
00:28:39 techniques that have been proposed and I think will continue to be improved and maybe alternatives will come up to take
00:28:46 datasets in which we know there is bias we can measure it pretty much any data set where humans are you know being
00:28:53 observed taking decisions will have some sort of bias discrimination against particular groups and so on and we can
00:29:01 use machine learning techniques to try to build predictors classifiers that are going to be less biased we can do it for
00:29:11 example using adversarial methods to make our systems less sensitive to these variables we should not be sensitive to
00:29:20 so these are clear well-defined ways of trying to address the problem maybe they have weaknesses and you know more
00:29:26 research is needed and so on but I think in fact they are sufficiently mature that governments should start regulating
00:29:33 companies where it matters say like insurance companies so that they use those techniques because those
00:29:42 techniques will produce the bias but at a costs for example maybe their predictions will be less accurate and so
00:29:47 companies will not do it until you force them all right so this is short term long term I'm really interested in
00:29:57 thinking of how we can instill moral values into computers obviously this is not something we'll achieve in the next
00:30:05 five or ten years how can we you know there's already work in detecting emotions for example in images and
00:30:16 sounds and texts and also studying how different agents interacting in different ways may correspond to
00:30:26 patterns of say injustice which could trigger anger so these are things we can do in in the medium term and eventually
00:30:38 train computers to model for example how humans react emotionally I would say the simplest thing is unfair situations
00:30:49 which trigger anger this is one of the most basic emotions that we share with other animals I think it's quite
00:30:55 feasible within the next few years so we can build systems that can take these kind of things to the extent
00:31:01 unfortunately that they understand enough about the world around us which is a long time away but maybe we can
00:31:09 initially do this in virtual environments so you can imagine like a video game we're agents interact in in
00:31:16 some ways and then some situations trigger an emotion I think we could train machines to detect those
00:31:23 situations and predict that the particular emotion you know will likely be felt if a human was playing one of
00:31:31 the characters you have shown excitement and done a lot of excellent work with supervised learning but on a superbug
00:31:38 you know there's been a lot of success on the supervised learning yes yes and one of the things I'm really passionate
00:31:45 about is how humans and robots work together and in the context of supervised learning that means the
00:31:53 process of annotation do you think about the problem of annotation of put in a more interesting way is humans teaching
00:32:03 machines yes is there yes I think it's an important subject reducing it to annotation may be useful for somebody
00:32:12 building a system tomorrow but longer-term the process of teaching I think is something that deserves a lot
00:32:18 more attention from the machine learning community so there are people have coined the term machine teaching so what
00:32:24 are good strategies for teaching a learning agent and can we design train a system that gonna be is gonna be a good
00:32:34 teacher so so in my group we have a project called the baby I or baby I game where there is a game or scenario where
00:32:45 there's a learning agent and a teaching agent presumably the teaching agent would eventually be a human but we're
00:32:56 not there yet and the the role of the teacher is to use its knowledge of the environment which it can acquire using
00:33:06 whatever way brute force to help the learner learn as quickly as possible so the learner is going to try to learn by it
00:33:13 of maybe be using some exploration and whatever but the teacher can choose can can can have an influence on the
00:33:22 interaction with the learner so as to guide the learner maybe teach it the things that the learner has most trouble
00:33:30 with or just at the boundary between what it knows and doesn't know and so on so this is there's a tradition of these
00:33:39 kind of ideas from other fields and like tutorial systems for example and they I and and of course people in the
00:33:46 humanities have been thinking about these questions but I think it's time that machine learning people look at
00:33:51 this because in the future we'll have more and more human machine interaction with a human in the loop and I think
00:33:59 understanding how to make this work better all the problems around that are very interesting and not sufficiently
00:34:06 addressed you've done a lot of work with language to what aspect of the traditionally formulated Turing test a
00:34:15 test of natural language understanding a generation in your eyes is the most difficult of conversation but in your
00:34:21 eyes is the hardest part of conversation to solve for machines so I would say it's everything having to do with the
00:34:30 non linguistic knowledge which implicitly you need in order to make sense of sentences things like the
00:34:37 Winograd schemas so these sentences that are semantically ambiguous in other words you need to understand enough
00:34:44 about the world in order to really interpret probably those sentences I think these are interesting challenges
00:34:51 for our machine learning because they point in the direction of building systems that both understand how the
00:34:59 world works and it's causal relationships in the world and associate that knowledge with how to express it in
00:35:11 language either for reading or writing you speak French yes it's my mother tongue it's one of the Romance languages
00:35:19 do you think passing the Turing test and all the underlying challenges we just mentioned depend on language do you
00:35:24 think it might be easier in front that is in English now is independent of language mmm
00:35:31 I think it's independent of language I I would like to build systems that can use the same principles the same learning
00:35:44 mechanisms to learn from human agents whatever their language well certainly us humans can talk more beautifully and
00:35:52 smoothly in poetry some Russian originally I know poetry and Russian is maybe easier to convey complex ideas
00:36:03 than it is in English but maybe I'm showing my bias and some people could say that about front half French but of
00:36:11 course the goal ultimately is our human brain is able to utilize any kind of those languages to use them as tools to
00:36:19 convey meaning you know of course there are differences between languages and maybe some are slightly better at some
00:36:24 things but in the grand scheme of things where we're trying to understand how the brain works and language and so on
00:36:33 I think these differences are a minut so you've lived perhaps through an AI winter of sorts yes how did you stay
00:36:43 warm and continue and you're resurfacing stay warm with friends and with friends okay so it's important to have friends
00:36:51 and what have you learned from the experience listen to your inner voice don't you know be trying to just please
00:37:04 the crowds and the fashion and if you have a strong intuition about something that is not contradicted by actual
00:37:14 evidence go for it I mean it could be contradicted by people but not your own instinct of based on everything you know
00:37:21 of course of course you have to adapt your beliefs when your experiments contradict those beliefs but but you
00:37:30 have to stick to your beliefs otherwise it's it's it's what allowed me to go through those years it's what allowed me to
00:37:39 persist in directions that you know took time whatever all the people think took time to mature and you bring fruits so
00:37:52 history of AI is marked with these of course it's mark with technical breakthroughs but it's also marked with
00:37:58 these seminal events that capture the imagination of the community most recent I would say alphago beating the world
00:38:06 champion human go player was one of those moments what do you think the next such moment might be okay surface first
00:38:15 of all I think that these so-called seminal events are overrated as I said science really moves by small steps now
00:38:30 what happens is you make one more small step and it's like the the drop that you know allows to that fills the bucket and
00:38:39 and and then you have drastic consequences because now you're able to do something you were not able to do
00:38:46 before or now say the cost of building some device or solving a problem becomes cheaper than what existed and you have a
00:38:52 new market that opens up right so so especially in the world of Commerce and applications the impact of a small
00:39:04 scientific progress could be huge but in the science itself I think it's very very gradual and where these steps being
00:39:13 taken now so there's supervised right so if I look at one trend that I like in in in my community so for example in at me
00:39:23 lie in my Institute what are the two hottest topics Gans and rain for spurning even though in the montreal in
00:39:33 particular like reinforcement learning was something pretty much absent just two or three years ago so it is really a
00:39:42 big interest from students and there's a so I would say this is something where are we gonna see more progress even
00:39:54 though it hasn't yet provided much in terms of actual industrial fallout like even though there's alphago there's no
00:40:01 like Google is not making money on this right now but I think over the long term this is really really important for many
00:40:10 reasons so in other words agent I would say reinforcement learning baby more generally agent learning because it
00:40:15 doesn't have to be with rewards it could be in all kinds of ways that an agent is learning about its environment now
00:40:22 reinforced learning you're excited about do you think do you think Gans could provide something yes some moment in in
00:40:34 a well Gans or other generative models I believe will be crucial ingredients in building agents that can understand the
00:40:44 world a lot of the successes in reinforcement learning in the past has been with policy gradient where you
00:40:51 you'll just learn a policy you don't actually learn a model of the world but there are lots of issues with that and
00:40:57 we don't know how to do model-based our rel right now but I think this is where we have to go in order to build models
00:41:05 that can generalize faster and better like to new distributions that capture to some extent at least the underlying
00:41:16 causal mechanisms in in the world last question what made you fall in love with artificial intelligence if you look back
00:41:25 what was the first moment in your life when he's when you were fascinated by either the human mind or the artificial
00:41:32 mind you know when I wasn't at the lesson I was reading a lot and then I I started reading science fiction there
00:41:40 you go but I got that's that's it that that's that's where I got hooked and then and then you know I had one of the
00:41:48 first personal computers and I got hooked in programming and so it just you know start with fiction and then make it
