00:00:01 the following is a conversation with francois chalet his second time in the podcast he's both
00:00:07 a world-class engineer and a philosopher in the realm of deep learning and artificial intelligence
00:00:15 this time we talk a lot about his paper titled on the measure of intelligence that discusses how we might define and measure
00:00:23 general intelligence in our computing machinery quick summary of the sponsors babel masterclass and cash app click the sponsor links in
00:00:32 the description to get a discount and to support this podcast as a side note let me say that the serious
00:00:39 rigorous scientific study of artificial general intelligence is a rare thing the mainstream machine
00:00:44 learning community works on very narrow ai with very narrow benchmarks this is very good for incremental and sometimes
00:00:53 big incremental progress on the other hand the outside the mainstream renegade you could say
00:01:01 agi community works on approaches that verge on the philosophical and even the literary without big public benchmarks walking
00:01:09 the line between the two worlds is a rare breed but it doesn't have to be i ran the agi
00:01:15 series at mit as an attempt to inspire more people to walk this line deep mind and open ai for time and still
00:01:23 on occasion walk this line francois chole does as well i hope to also it's a beautiful dream to work towards
00:01:30 and to make real one day if you enjoy this thing subscribe on youtube
00:01:35 review it with five stars on apple podcast follow on spotify support on patreon or connect with me on
00:01:41 twitter at lex friedman as usual i'll do a few minutes of ads now and no ads in the middle
00:01:47 i try to make these interesting but i give you time stamps so you can skip but still please do check
00:01:53 out the sponsors by clicking the links in the description it's the best way to support this podcast
00:02:00 this show is sponsored by babel an app and website that gets you speaking in a new language within weeks
00:02:06 go to babble.com and use colex to get three months free they offer 14 languages including
00:02:12 spanish french italian german and yes russian daily lessons are 10 to 15 minutes
00:02:19 super easy effective designed by over 100 language experts let me read a few lines from the russian poem
00:02:29 by alexander bloch that you'll start to understand if you sign up to babble now i say that you'll start to
00:02:48 understand this poem because russian starts with a language and ends with the vodka now the latter part
00:02:56 is definitely not endorsed or provided by babel it will probably lose me this sponsorship
00:03:02 although it hasn't yet but once you graduate with babel you can roll my advanced course of late
00:03:09 night russian conversation over vodka no app for that yet so get started by visiting babel.com
00:03:17 and use codelex to get three months free this show is also sponsored by masterclass sign up
00:03:24 at masterclass.com lex to get a discount and to support this podcast when i first heard about masterclass i
00:03:29 thought it was too good to be true i still think it's too good to be true for 180
00:03:35 a year you get an all-access pass to watch courses from to list some of my favorites chris
00:03:41 hadfield on space exploration hope to have him in this podcast one day neil degrasse tyson on scientific
00:03:46 thinking communication neil two will wright creator of simcity and sims on game design carlos santana
00:03:52 on guitar carrie casparov von chasse daniel negrano and poker and many more
00:03:58 chris hadfield explaining how rockets work and the experience of being launched at the space alone is worth the money
00:04:05 by the way you can watch it on basically any device once again sign up at masterclass.com
00:04:10 lex to get a discount and to support this podcast this show finally is presented by
00:04:17 cash app the number one finance app in the app store when you get it use code lex podcast
00:04:23 cash app lets you send money to friends buy bitcoin and invest in the stock market with as little as one dollar
00:04:28 since cash app allows you to send and receive money digitally let me mention a surprising fact related
00:04:33 to physical money of all the currency in the world roughly eight percent of it
00:04:40 is actually physical money the other 92 percent of the money only exists digitally and that's only going to increase so again
00:04:47 if you get cash out from the app store google play and use code lex podcast you get ten bucks and cash
00:04:53 app will also donate ten dollars to first an organization that is helping to advance robotics and stem education
00:05:00 for young people around the world and now here's my conversation with francois chalet what philosophers
00:05:08 thinkers or ideas had a big impact on you growing up and today so one
00:05:14 author that had a big impact on me when i read these books as a teenager with jean
00:05:19 pierre who is a swiss psychologist is considered to be the father of developmental
00:05:26 psychology and he has a large body of work about basically how intelligence develops
00:05:35  in children and so it's really old work like most of it is from the 1930s 1940s so it's not quite up to date it's
00:05:42 actually superseded by many neural developments in developmental psychology but to me it was
00:05:48 it was very  very interesting very striking and actually shaped the early ways in which i started
00:05:53 thinking about the mind and development of intelligence as a teenager his actual ideas or the way he
00:05:59 thought about it or just the fact that you could think about the developing mind at all
00:06:03 i guess both jean-pierre is the author that's reintroduced me to the notion that intelligence and the mind is
00:06:09 something that you construct through throughout your life and that you the children
00:06:16  construct it in stages and i thought that was a very interesting idea which is you know of course very relevant
00:06:23  to ai to building artificial minds another book that i read around the same time that had a big impact on me
00:06:31  and and there was actually a little bit of overlap with john pierre as well and i read it around the same
00:06:37 time is jeff hawkins on intelligence which is a classic and he has this vision
00:06:45 of the mind as a multi-scale hierarchy of temporal prediction modules and these ideas really resonated with me
00:06:53 like the the notion of a modular hierarchy of you know potentially of compression functions or prediction functions
00:07:02 i thought it was really really interesting and it reshaped  the way it started thinking about how
00:07:08 to build minds the hierarchical nature the which aspect also he's a
00:07:15 neuroscientist so he was thinking yes actual he's basically talking about how our mind works yeah the notion that
00:07:23 cognition is prediction was an idea that was kind of new to me at the time and that i really loved at
00:07:27 the time and yeah and the notion that yeah there are multiple scales of processing
00:07:36  in the brain the hierarchy yes this is before deep learning these ideas of hierarchies
00:07:43 in here i've been around for a long time even before on intelligence i mean they've been around since the
00:07:50 1980s and yeah that was before deep learning but of course i think these ideas really found their
00:07:56 practical implementation in deep learning what about the memory side of things i think he's talking about knowledge representation
00:08:04 do you think about memory a lot one way you can think of neural networks as a kind of memory you're memorizing things
00:08:13 but it doesn't seem to be the kind of memory that's in our brains or it doesn't have the same rich
00:08:19 complexity long-term nature that's in our brains yes the brain is more for sparse access
00:08:25 memory so that you can actually retrieve very precisely like bits of your experience the retrieval aspect you can like introspect
00:08:36 you can ask yourself questions again yes you can program your own memory and language is actually
00:08:41 the tool you used to do that i think language is a kind of operating system for the mind and use language
00:08:51 well one of the uses of language is as a query that you run over your own memory use
00:08:57 words as keys to retrieve specific experiences of basic concepts specific starts like language is the way you store
00:09:03 thoughts not just in writing in the in the physical world but also in your own mind and it's also how you reach with them
00:09:09 like imagine if you didn't have language then you would have to you would not have really have a
00:09:16 self internally triggered  way of retrieving past thoughts you would have to rely on external experiences
00:09:23 for instance you you see a specific site you smell specific smell and it brings up memories
00:09:28 but you would naturally have a way to deliberately deliberately access these memories without language well the interesting
00:09:34 thing you mentioned is you can also program the memory you can change it probably with language yeah using language yes well
00:09:44 let me ask you a chomsky question which is like first of all do you think language is
00:09:49 like fundamental like  there's turtles what's at the bottom of the turtles they
00:09:56 don't go it can't be turtles all the way down is language at the bottom of cognition of everything is like
00:10:04 language the fundamental aspect of like what it means to be a thinking thing no i don't think so
00:10:13 i think language you disagree with noam chomsky yes language is a layer on top of cognition so
00:10:20 it is fundamental to cognition in the sense that to to use a computing metaphor i see
00:10:24 language as the operating system  of the brain of the human mind
00:10:30 yeah and the operating system you know is a layer on top of the computer the computer exists before the operating
00:10:37 system but the operating system is how you make it truly useful and the operating system is most likely
00:10:42 windows not not linux because it's  language is messy yeah it's messy and it's  it's um
00:10:50 pretty difficult to   inspect it introspect it how do you think about language
00:10:58 like we use actually sort of human interpretable language but is there something like
00:11:05 a deeper that's closer to like like logical type of statements like yeah what is the nature of
00:11:16 language do you think because there's something deeper than like the syntactic rules we construct is there something that
00:11:24 doesn't require utterances or writing or so on are you asking about the possibility that there could exist
00:11:31  languages for thinking that are not made of words yeah yeah i think so i think so
00:11:39  the mind is layers right and language is almost like the the outermost the uppermost layer
00:11:47 but before we think in words i think we think in in terms of emotion in space
00:11:52 and we think in terms of physical actions and i think a baby babies in particular probably express his thoughts in terms of
00:12:02 the actions  that they've seen of that or that they can perform and in terms of the in in terms of
00:12:07 motions of objects in their environment before they start thinking in terms of words it's amazing to
00:12:15 think about that as the building blocks of language so like the kind of actions and
00:12:22 ways the babies see the world as like more fundamental than the beautiful shakespearean language you
00:12:28 construct on top of it and we we probably don't have any idea what that looks like right
00:12:34 like what because it's important for them trying to engineer it into ai systems
00:12:42 i think visual analogies and motion is a fundamental building block of the mind and you
00:12:47 you actually see it reflected in language like language is full of special metaphors and when you think
00:12:53 about things i consider myself very much as a visual thinker you you often express your thoughts
00:13:04 by using things like  visualizing concepts in in 2d space or like you solve
00:13:10 problems by image imagining yourself navigating a concept space i don't know if you have this sort
00:13:17 of experience you said visualizing concept space so i certainly met i certainly visualize
00:13:27 mathematical concepts but you mean like in concept space visually you're embedding ideas into some
00:13:37 into a three-dimensional space you can explore with your mind essentially yeah 2d you're a flatlander
00:13:46 you're okay no i i i do not i always have to  before i jump from concept to concept i have to
00:13:56 put it back down on pape and it has to be on paper i can only travel on 2d paper not inside my mind
00:14:05 you're able to move inside your mind but even if you're writing like a paper for instance don't you have
00:14:10 like a special representation of your paper like you you visualize where ideas lie topologically in relationship to other ideas
00:14:21 kind of like a subway map of the ideas in your paper yeah that's true i mean there there is 
00:14:28 in papers i don't know about you but there feels like there's a destination there's a there's a key
00:14:36 idea that you want to arrive at and a lot of it is in in the fog and you're trying to kind of
00:14:45 it's almost like what's that called when you do a path planning search from both directions
00:14:52 from the start and from the end but and then you find you do like shortest path but like
00:14:57  you know in game playing you do this with like a star from both sides when you see where they join
00:15:05 yeah so you kind of do at least for me i think like first of all just exploring from the
00:15:09 start from like  first principles what do i know  what can i
00:15:16 start proving from that right and then from the destination if i you start backtracking like
00:15:24 if if i want to show some kind of sets of ideas what would it take to show them and you
00:15:28 kind of backtrack but yeah i don't think i'm doing all that in my mind though like i'm putting it down
00:15:33 on paper do you use mind maps to organize your ideas yeah i like mind maps
00:15:40 let's get into this i've been so jealous of people i haven't really tried it i've been jealous of
00:15:45 people that seem to like they get like this fire of passion in their eyes because everything starts making sense
00:15:52 it's like  tom cruise in the movie was like moving stuff around some of the most brilliant people i know
00:15:56 use mind maps i haven't tried really can you explain what the hell a mind map is
00:16:03 i guess mind map is a way to make connected mess inside your mind to just put it on paper so that you gain
00:16:09 more control over it it's a way to organize things on paper and as as kind of like a consequence for
00:16:18 organizing things on paper it start being more organized inside inside your own mind what what does that look like
00:16:24 you put like do you have an example like what what what do you what's the first thing you
00:16:28 write on paper what's the second thing you write i mean typically  you you draw a mind
00:16:31 map to organize the way you think about a topic so you would start by
00:16:38 writing down like the the key concept about that topic like you would write intelligence or something and then you
00:16:43 would start adding  associative connections like what do you think about when you think about
00:16:49 intelligence what do you think are the key elements of intelligence so maybe you would have language for instance
00:16:53 instead of motion and so you would start drawing notes with these things and then you would see
00:16:57 what do you think about when you think about motion and so on and you would go like that
00:17:01 like a tree it's a tree or a tree mostly there's a graph to like a tree oh it's it's more of a graph than
00:17:09 a tree and and it's not limited to just you know writing down
00:17:15 words you can also  draw things and it's not it's not supposed to be purely hierarchical right
00:17:22 like you can the point is that you can start once once you start writing it down you can start reorganizing it so
00:17:28 that it makes more sense so that it's connected in a more effective way see but i'm so
00:17:34 ocd that you just mentioned intelligence and language emotion i would start becoming paranoid that the
00:17:41 categorization isn't perfect like that i'll become paralyzed with the mind map that like this may not be
00:17:52 so like the even though you're just doing associative kind of connections there's an implied hierarchy
00:17:58 that's emerging and i would start becoming paranoid that's not the proper hierarchy
00:18:04 so you're not just one way to see mind maps is you're putting thoughts on paper it's like a
00:18:11 stream of consciousness but then you can also start getting paranoid well if is this the right hierarchy sure like
00:18:17 which it's a mind map it's your mind map you're free to draw anything you want you're free to draw any connection you
00:18:21 want and you can just make a different mind my opinion is if you think the central node is not the
00:18:26 right node yeah so i suppose there's a fear of being wrong
00:18:31 if you want to if you want to organize your ideas by writing down what you think which i
00:18:38 think is is very effective like how do you know what you think about something if you don't write it down
00:18:44 right  if you do that the thing is that it imposes a much more  syntactic structure
00:18:50 over your ideas which is not required with mind map so mind map is kind of like a lower level
00:18:56 more freehand way of organizing your thoughts and once you've drawn it then you can start  actually voicing your thoughts in
00:19:04 terms of you know paragraphs it's a two-dimensional aspect of layout too right
00:19:11 yeah and it's it's a kind of flower i guess you start there's usually you want to start with a central concept
00:19:18 yes typically it ends up more like a subway map so it ends up more like a graph a topological graph without a root note
00:19:23 yeah so like in a subway map there are some nodes that are more connected than
00:19:28 others and there are some nodes that are more important than others right so there are destinations but
00:19:35 it's it's not going to be purely like a tree for instance yeah it's fascinating to think that if
00:19:40 there's something to that about our about the way our mind thinks by the way i just kind of remembered
00:19:47 obvious thing that i have probably thousands of documents in google doc at this point
00:19:54 that bullet point lists  which is you can probably map a mine it's the same it's a no it's not it's a tree
00:20:07 it's a tree yeah so i create trees but also they don't have the visual element like i guess i'm comfortable with the
00:20:14 structure it feels like it the narrowness the constraints feel more comforting if you have thousands of
00:20:21 documents with your own thoughts in google docs why don't you write  some kind of search engine like maybe
00:20:28 a mind map a piece of software mind mapping software where you write down a concept
00:20:35 and then it gives you sentences or paragraphs from your thousand google docs document that match
00:20:41 this concept the problem is it's so deeply unlike mind maps
00:20:47 it's so deeply rooted in natural language so it's not it's not semantically searchable i would say
00:20:57 because the categories are very you kind of mention intelligence language and motion they're very strong
00:21:03 semantic like it feels like the mind map forces you to be semantically clear and specific the
00:21:11 bullet points list i have are are sparse desperate thoughts that 
00:21:21 poetically represent a category like motion as opposed to saying motion so unfortunately it's that's the same
00:21:29 problem with the internet that's why the idea of semantic web is difficult to get it's  most language on the internet is
00:21:37 a giant mess of natural language that's hard to interpret which so do you think  do you think
00:21:44 there's something to mind maps as you actually originally brought up as we were talking about
00:21:53 kind of cognition and language do you think there's something to mind maps about how our brain actually
00:22:01 deals like think reasons about things it's possible i think it's reasonable to assume that there is
00:22:10 some level of topological processing in the brain that the brain is very associative in nature
00:22:18 and i also believe that to encode thoughts than a geometric space then so i think what's the difference in
00:22:30 topological and geometric space well if you're talking about topologies 
00:22:36 then points are either connected or not so the topology is more like a subway map and geometry is when you're interested
00:22:44 in the distance between things and in subway maps you don't really have the concept of distance you only have the
00:22:48 concept of whether there is a train going from station a to station b and
00:22:54 what we do in deep learning is that we're we're actually dealing with  geometric spaces we're dealing with
00:22:59 concept vectors word vectors  that have a distance between the gist expressed in terms of dot
00:23:06 product we are not we are not really building topological models usually
00:23:11 i think you're absolutely right like distance is a fundamental importance in deep learning
00:23:17 i mean it's the continuous aspect of it yes because everything is a vector and everything
00:23:22 has to be a vector because everything has to be differentiable if your space is discrete it's no longer
00:23:27 differentiable you cannot do deep learning in it anymore well you could but you could only do it
00:23:32 by embedding it in a bigger continuous space so if you do topology in the in the context of deep
00:23:40 learning you have to do it by embedding your topology in a geometry right yeah well let me  let me zoom out for a second
00:23:47  let's get into your paper on the measure of intelligence that  did you put on 2019 yes
00:24:00 yeah remember 2018 that was a different time it feels like a different and different different world
00:24:11 you could travel you can you know actually go outside and see friends yeah
00:24:19 let me ask the most absurd question i think  there's some non-zero probability there'll be a textbook one day
00:24:26 like 200 years from now on artificial intelligence or it'll be called like just intelligence because humans will already
00:24:32 be gone it'll be your picture with a quote you know one of the early biological
00:24:39 systems would consider the nature of intelligence and they'll be like a definition of how they thought
00:24:44 about intelligence which is one of the things you do in your paper on measure intelligence is
00:24:51 to ask like well what is intelligence and and  how to test for intelligence and so on
00:24:58 so is there a spiffy quote about what is intelligence what is the definition of intelligence
00:25:06 according to francois charley yes so do you think the the superintendent ais of the future will
00:25:14 want to remember us do we remember humans from the past and do you think they would be
00:25:19 you know they won't be ashamed of having a biology called origin  no i i think it would be a niche
00:25:25 topic it won't be that interesting but it'll be it'll be like the people that study in
00:25:30 certain contexts like historical civilization that no longer exist
00:25:36 the aztecs and so on that that's how it'll be seen and it'll be studying also the context
00:25:44 on social media there will be hashtags about the atrocity committed to human beings when when the when the robots finally
00:25:54 got rid of them like it was a mistake it'll be seen as a as a giant mistake but
00:26:00 ultimately in the name of progress and it created a better world because humans were  over consuming the resources and all
00:26:07 they were not very rational and were destructive in the end in terms of productivity and putting more love in the world and so
00:26:15 within that context there'll be a chapter about these biological systems seems to have a very detailed vision
00:26:21 of that feature you should write a sci-fi novel about it i said i'm working i'm working on a sci-fi novel currently yes
00:26:29 yes self-published yeah the definition of intelligence so intelligence is the efficiency
00:26:38 with which you acquire new skills at tasks that you did not previously know about that you did not prepare for
00:26:46 all right so it is not intelligence is not skill itself it's not what you know it's not what you
00:26:51 can do it's how well and how efficiently you can learn new things
00:26:58 new things yes the idea of newness there seems to be fundamentally important yes so you would see intelligence on
00:27:05 display for instance whenever you see a human being or you know an ai
00:27:12 creature adapt to a new environment that it has not seen before that its creators did not anticipate
00:27:18 when you see adaptation when you see improvisation when you see generalization that's intelligence
00:27:24  in reverse if you have a system that's when you put it in a slightly new environment it cannot adapt
00:27:31 it cannot improvise it cannot deviate from what it's hardcoded to do oh what what it has been trying to do
00:27:39 that is a system that is not intelligent there's actually a quote from einstein that captures this idea which is
00:27:48 the measure of intelligence is the ability to change i i like that quote i think it captures
00:27:54 at least part of this idea you know there might be something interesting about the difference between
00:27:59 your definition and einsteins i mean he's just being einstein and clever but acquisition of
00:28:12 new ability to deal with new things versus ability to just change what's the difference between those two
00:28:20 things so just changing itself do you think there's something to that
00:28:27 just being able to change yes being able to adapt so not not change but certainly 
00:28:34 changes direction being able to adapt yourself to your environment whatever the environment that's
00:28:40 that's a big part of intelligence yes and intelligence is more precisely you know how efficiently you're able to adapt how
00:28:47 efficiently you're able to basically master your environment how efficiently you can acquire new skills and i think
00:28:54 there's a there's a big distinction to be drawn between intelligence which is a process
00:29:02 and the output of that process which is skill so for instance if you have a very smart human programmer
00:29:10 that considers the game of chess and that writes down a static program that can play chess then
00:29:19 the intelligence is the process of developing that program but the program itself is just encoding
00:29:28 the output artifact of that process the program itself is not intelligent and the way you tell it's not intelligent
00:29:33 is that if you put it in a different context you ask it to play go or something it's not going to be able to perform
00:29:39 well with human involvement because the source of intelligence the entity that is capable of that
00:29:44 process is the human programmer so we should be able to tell the difference between
00:29:50 the process and its output we should not confuse the output and the process it's the same as you know do not confuse
00:29:59 a road building company and one specific road because one specific road takes you from point a to point b
00:30:05 but a road building company can take you from you can make a path from anywhere to anywhere
00:30:10 else yeah that's beautifully put but it's also to play devil's advocate a little bit
00:30:18 you know it's possible that there's something more fundamental than us humans so you kind of said the programmer creates
00:30:28  the difference between the the choir of the skill and the skill itself there could be something like you could
00:30:35 argue the universe is more intelligent like the the deep the base intelligence of that we
00:30:42 should be trying to measure is something that created humans
00:30:49 we should be measuring god or what the source the universe as opposed to like there's there could be a deeper
00:30:56 intelligence sure there's always deeper intelligence you can argue that but that does not
00:31:00 take anything away from the fact that humans are intelligence and you can't tell that
00:31:05 because they are capable of adaptation and and generality and you see that in particular and
00:31:10 the fact that  humans are capable of handling  situations and tasks that
00:31:19 are quite different from anything that any of our evolutionary ancestors has ever encountered
00:31:26 so we are capable of generalizing very much out of distribution if you consider our evolutionary history as being in a
00:31:33 way else training data course evolutionary biologists would argue that we're not going too far out
00:31:37 of the distribution we're like mapping the skills we've learned previously
00:31:43 desperately trying to like jam them into like these new situations i mean there's definitely a little bit
00:31:50 a little bit of that but it's pretty clear to me that we're able to  you know most of the things we do
00:31:58 any given day in our modern civilization are things that are very very different from what you know
00:32:03 our ancestors a million years ago would have been doing in in a given day and your environment is very different so i agree that
00:32:12 everything we do we do it with cognitive building blocks that we acquired over the course of revolution
00:32:20 right and that anchors our cognition to a certain context which is the human condition very much but still
00:32:28 our mind is capable of a pretty remarkable degree of generality far beyond anything we can create in
00:32:34 artificial systems today like the degree in which the mind can generalize from its evolutionary history
00:32:42 can generalize away from its evolutionary history is much greater than the
00:32:48 degree to which a depending system today can generalize away from its training data and like the key point you're making
00:32:54 which i think is quite beautiful is like we shouldn't measure if we talk about measurement we shouldn't measure the skill we should
00:33:03 measure like the creation of the new skill the ability to create that new skill yes but there it's tempting
00:33:11 like it's weird because the skill is a little bit of a small window into the into the system so whenever you have a
00:33:19 lot of skills it's tempting to measure the skills yes i mean the skill is the
00:33:25  only thing you can objectively measure but yeah so the the thing to keep in mind is that
00:33:34 when you see skill in the human it gives you a strong signal that that human is intelligent because you knew
00:33:41 they weren't born with that skill typically like you say this you see a very strong chess player maybe you're a
00:33:47 very stronger player yourself i think you're and you're you're saying that because i'm russian and now now you're
00:33:54 you're prejudiced you assume oh yeah it's just biased i'm biased yeah well you're dead by us
00:34:01 so if you see a very strong chess player you know they weren't born knowing how to play chess so they had to
00:34:07 acquire that skill with their limited resources with their limited lifetime
00:34:13 and you know they did that because they are generally intelligent and so they may as well have acquired
00:34:19 any other skill you know they have this potential and on the other hand if you see a computer playing a chess you cannot make the same
00:34:28 assumptions because you cannot you know just assume the computer is generally intelligent the computer may be born knowing
00:34:36 how to play chess in the sense that it may have been programmed by a human that has understood chess for
00:34:41 the computer and and that has just encoded the output of that understanding in aesthetic program and that program
00:34:50 is not intelligent so let's zoom out just for a second and say like what is the goal of the on the measure
00:34:57 of intelligence paper like what do you hope to achieve with it so the goal of the paper
00:35:02 is to clear up some long-standing misunderstandings about the way we've been conceptualizing intelligence in the ai community and
00:35:13 in the way we've been evaluating progress in ai there's been a lot of progress recently in machine learning and people
00:35:21 are you know extrapolating from that progress that we're about to solve general intelligence
00:35:29 and if you want to be able to evaluate these statements you need to precisely define what you're
00:35:34 talking about when you're talking about general intelligence and you need a formal way a reliable way to measure
00:35:43 how much intelligence how much general intelligence a system processes and ideally this measure of intelligence should be
00:35:52 actionable so it should not just describe what intelligence is it should not just be a binary indicator that tells you
00:36:01 the system is intelligent or it isn't it should be actionable it should have explanatory
00:36:07 power right so you could use it as a feedback signal it would show you  the way towards
00:36:13 building more intelligent systems so at the first level you draw a distinction between two divergent views
00:36:19 of intelligence of as we just talked about intelligence is a collection of tax
00:36:29 task specific skills and a general learning ability so what's the difference between
00:36:35 kind of this memorization of skills and a general learning ability we've talked about a little bit but can you
00:36:41 try to linger on this topic for a bit yeah so the first part of the paper
00:36:48  is  an assessment of the different ways  we've been thinking about intelligence and the different ways
00:36:53 we've been evaluating progress in ai and the history of cognitive sciences has been shaped by
00:36:59 two views of the human mind and one view is the evolutionary psychology view in which
00:37:06 the mind is a collection of fairly static special purpose ad-hoc mechanisms
00:37:17 that have been hard coded by evolution over our our history as a species over a very long time
00:37:26 and early ai researchers people like marvin minsky for instance
00:37:33 they clearly subscribed to this view and they saw they saw the mind as a kind of you know collection of static programs 
00:37:41 similar to the programs they would they would run on like mainframe computers and in fact they i think they very much
00:37:47 understood the mind  through the metaphor of the mainframe computer because that was the tool they
00:37:53 they were working with right and so you had the static programs this collection of very different static
00:37:57 programs operating over a database like memory and in this picture learning was not very important
00:38:04 learning was considered to be just memorization and in fact learning is basically not featured in ai
00:38:12 textbooks until the 1980s with the rise of machine learning it's kind of fun to think about that
00:38:21 learning was the outcast like the the weird people were learning like the mainstream
00:38:29 ai world was i mean i don't know what the best term is but it's non-learning
00:38:36 it was seen as like reasoning yes would not be learning based yes it was seen it was considered that
00:38:42 the mind was a collection of programs that were primarily logical in nature and that's all you
00:38:49 needed to do to create a mind was to write down these programs and they would operate over your knowledge
00:38:54 which would be stored in some kind of database and as long as your database would encompass you know
00:38:59 everything about the world and your logical rules were  comprehensive then you would have in
00:39:06 mind so the other view of the mind is the brain as a sort of blank slate right this is a very old idea you find
00:39:14 it in john locke's writings this is the tabulata and this is this idea that the mind is
00:39:24 some kind of like information sponge that starts empty it starts blank and that absorbs 
00:39:34 knowledge and skills from experience right so it's  it's a sponge that reflects the complexity of the world the
00:39:41 complexity of your life experience essentially that everything you know and everything you can do is
00:39:48 a reflection of something you found in the outside world essentially so this is an idea that's very old 
00:39:55 that was not very popular for instance in the in the 1970s but that had gained a lot of vitality
00:40:01 recently with the rise of connectionism in particular deep learning and so today deep learning is
00:40:06 the dominant paradigm in ai and i feel like lots of ai researchers are conceptualizing the mind
00:40:16 via a deep learning metaphor like they see the mind as a kind of randomly initialized neural network that
00:40:23 starts blank when you're born and then that gets trained yeah
00:40:28 exposure to training data that acquires knowledge and skills exposure to training data
00:40:34 by the way it's a small tangent i feel like people who are thinking about intelligence
00:40:41 are not conceptualizing it that way i actually haven't met too many people who believe that a neural network
00:40:49 will be able to reason who seriously think that rigorously because i think it's actually interesting world view
00:40:56 and and we'll talk about it more but it it's been impressive what the  what neural networks have
00:41:01 been able to accomplish and it's i to me i don't know you might disagree but it's an open question whether
00:41:09 like like scaling size eventually might lead to incredible results to us mere humans will appear as
00:41:16 if it's general i mean if you if you ask people who are seriously thinking about intelligence
00:41:21 they will definitely not say that all you need to do is is
00:41:27 like the mind is just in your network  however it's actually you that's that's very popular i think in the deep learning
00:41:32 community that many people are kind of  conceptually you know intellectually lazy about it
00:41:39 right but what i guess what i'm saying exactly right it's  i i me i haven't met many people and
00:41:46 i think it would be interesting  to meet a person who is not intellectualized about this particular
00:41:51 topic and still believes that neural networks will go all the way i think january
00:41:57 is probably closest to that there are definitely people who argue that 
00:42:03 current deep learning techniques are already the way to general artificial intelligence and
00:42:08 that all you need to do is to scale it up to all the available training data
00:42:16 and that's if you look at the the waves that open ai's gpt stream model has made you see echoes of this idea so on that topic
00:42:27 gpt-3 similar to gpt-2 actually have captivated some part of the imagination of the public
00:42:34 there's just a bunch of hype of different kind that's i would say it's emergent it's not
00:42:41 artificially manufactured it's just like people just get excited for some strange reason in in the case of gpt3 which is funny
00:42:48 that there's i believe a couple months delay from release to historically correct on that but it
00:43:00 feels like there was a little bit of a lack of hype and then there's a phase shift into into
00:43:06 hype but nevertheless there's a bunch of cool applications that seem to captivate the imagination
00:43:11 of the public about what this language model that's trained in unsupervised way
00:43:18 without any fine tuning is able to achieve so what do you make of that what are your thoughts about gbt3
00:43:24 yeah so i think what's interesting about gpg3 is the idea that it may be able to learn new tasks in
00:43:33 after just being shown a few examples so i think if it's actually capable of doing that
00:43:37 that's novel and that's very interesting and that's something we should investigate that said i must say i'm not entirely convinced
00:43:45 that we have shown it's it's capable of doing that it's very likely given the amount
00:43:52 of data that the model is trained on that what it's actually doing is pattern matching  a new task you
00:43:57 give it with the task that it's been exposed to in its training data it's just
00:44:01 recognizing the task instead of just developing a model of the task
00:44:07 right but there's a side to interrupt there's there's a parallels to what you said before
00:44:13 which is it's possible to see gpt3 as like the prompts that's given as a kind of
00:44:18 sql query into this thing that it's learned similar to what you said before which is language is used
00:44:24 to query the memory yes so is it possible that neural network is a giant memorization thing
00:44:32 but then if it gets sufficiently giant it'll memorize sufficiently large amounts of thing in the world
00:44:38 where it becomes more intelligence becomes a querying machine i think it's possible that  a
00:44:45 significant chunk of intelligence is this giant associative memory  i definitely don't believe that
00:44:52 intelligence is just a giant issue of memory but it may well be a big component
00:45:00 so do you think gpt 3 4 5 gpt 10 will eventually like what do you think where's the
00:45:09 ceiling do you think you'll be able to reason no that's a bad question  like what is the ceiling is the better
00:45:18 question how well is it going to scale how good is gptn going to be yeah so i believe gptn is going to
00:45:28 chiptn is going to improve on the strength of gpt2 and 3 which is it will be able to generate you know
00:45:37 ever more plausible text in context just monitoring the process performance yes if you train if you're training bigger
00:45:45 more on more data then your text will be increasingly more context aware
00:45:51 and increasingly more plausible in the same way that gpd3 it is much better at generating
00:45:58 clausable text compared to gpd2 but that said i don't think just getting up  the model to more transformer layers
00:46:06 and more train data is going to address the flaws lgbt3 which is that it can generate
00:46:12 plausible text but that text is not constrained by anything else other than plausibility
00:46:19 so in particular it's not constrained by factualness  or even consistency which is why it's very easy to get gpt3 to generate
00:46:26 statements that are factually untrue  or to general statements that are even self-contradictory
00:46:34 right  because it's  it's it's only goal is plausibility and it has no other constraints
00:46:40 it's not constrained to be self-consistent for instance right and so for this reason one thing that i
00:46:46 thought was very interesting with gpd3 is that you can present mind the answer it will
00:46:51 give you by asking the question in specific way because it's very responsive to the way
00:46:58 you ask the question since it has no understanding of the content of the question right
00:47:05 and if you if you ask the same question in two different ways that are basically adversarially engineered to
00:47:12 produce certain answers you will get two different answers to contractor answers it's very susceptible to adversarial
00:47:19 attacks essentially potentially yes so in in general the problem with these models is
00:47:24 generative models is that they are very good at generating plausible text but that's just
00:47:31 that's just not enough right you need  i think one one avenue that would be very
00:47:38 interesting to make progress is to make it possible to write programs over the latent space
00:47:45 that these models operate on that you would rely on these self-supervised models to
00:47:51 generate a sort of flag pool of knowledge and concepts and common sense and then you will be able
00:47:57 to write explicit  reasoning programs over it  because the current problem with gpt
00:48:04 stream is that you it's it can be quite difficult to get it to do what you want to do if you want to
00:48:13 turn gpd3 into products you need to put constraints on it you need to force it to
00:48:20 obey certain rules so you need a way to program it explicitly yeah so if you look at its ability to do
00:48:26 program synthesis it generates like you said something that's plausible yeah so
00:48:31 if you if you try to make it generate programs it will perform well for any program that it has seen it in
00:48:38 its training data but because  program space is not interpretive right it's not going to be able to
00:48:47 generalize to problems it hasn't seen before now that's currently do you think sort of an absurd but i think useful
00:49:00 i guess intuition builder is  you know the gpt-3 has 175 billion parameters a human brain has a hundred has about a
00:49:12 thousand times that or or more in terms of number of synapses do you think obviously
00:49:22 very different kinds of things but there is some degree of similarity do you think what do you think gpt will look
00:49:32 like when it has a hundred trillion parameters you think our conversation might be so in nature different
00:49:41 like because you've criticized gbt3 very effectively now do you think no i don't think so
00:49:49 so the the to begin with the bottleneck with scaling upgrades gbt models  alternative pre-trained
00:49:54 transformer models is not going to be the size of the model or how long
00:50:00 it takes to train it the bottleneck is going to be the trained data because openui is already training gpt3
00:50:08 on a crore of basically the entire web right and that's a lot of data so you could imagine training on more data than
00:50:13 that like google could try on more data than that but it would still be only incrementally
00:50:17 more data and i i don't recall exactly how much more data gpd3 was trained on compared
00:50:24 to gpt2 but it's probably at least like 100 or maybe even a thousand x don't have the exact number
00:50:29  you're not going to be able to train the model on 100 more data than with what you already with what you're
00:50:34 already doing so that's that's brilliant so it's not you know it's easier to think of compute
00:50:38 as a bottleneck and then arguing that we can remove that bottleneck but we can remove the compute
00:50:43 bottleneck i don't think it's a big problem if you look at the at the base at which we've 
00:50:50 improved the efficiency of deep learning models in the past a few years i'm not worried about  trying time bottlenecks or model size bottlenecks
00:51:01 the the bottleneck in the case of these generative transformer models is absolutely the trained data
00:51:08 what about the quality of the data so so yeah so the quality of the data is an interesting point the thing is
00:51:13 if you're going to want to use these models in real products then you
00:51:20 you want to feed them data that's as high quality as factual i would say as unbiased as possible
00:51:26 but you know there's there's not really such a thing as unbiased data in the first place but you probably
00:51:32 don't want to to train it  on reddit for instance it sounds sounds like a bad plan so from my
00:51:39 personal experience working with a large scale deep learning models so at some point i was working on a
00:51:46 model at google that's trained on extra 150 million labeled images it's image classification
00:51:55 model that's a lot of images that's like probably most publicly available images on the web at the time
00:52:04 and it was a very noisy data set because the labels were not originally annotated by hand by
00:52:10 humans they were automatically derived from like tags on social media
00:52:16 or just keywords in in the same page as the image was fun and so on so it was very noisy and
00:52:22 it turned out that you could  easily get a better model  not just by training like if you train on more
00:52:30 of the noisy data you get an incrementally better model but you you you very quickly hit diminishing
00:52:36 returns on the other hand if you try on smaller data set with higher quality annotations quality that are
00:52:44 annotations that are actually made by humans you get a better model and it also takes you know less
00:52:49 time to train it  yeah that's fascinating it's the self-supervised learnings there's a way to get better
00:52:57 doing the automated labeling yeah so you can enrich or refine your labels
00:53:07 in an automated way that's correct do you have a hope for i don't know if you're familiar with the
00:53:11 idea of a semantic web is this a semantic web just for people who are not familiar
00:53:18 and is  is the idea of being able to convert the internet or be able to attach like semantic
00:53:25 meaning to the words on the internet this the sentences the paragraphs
00:53:33 to be able to contr convert information on the internet or some fraction of the internet into something that's
00:53:38 interpretable by machines that was kind of a dream for i think the the semantic white
00:53:47 papers in the 90s it's kind of the dream that you know the internet is full of rich exciting
00:53:52 information even just looking at wikipedia we should be able to use that
00:53:58 as data for machines and so information is not it's not really in a format that's available to machines so no i
00:54:03 don't think the semantic web will ever work simply because it would be a lot of work
00:54:09 right to make to provide that information in structured form and there is not really any incentive
00:54:15 for anyone to provide that work  so i think the the way forward to make the knowledge on the web available to
00:54:23 machines is actually something closer to unsupervised deep learning yeah the gpg 3 is actually a bigger step
00:54:33 in the direction of making the knowledge of the web available to machines than the semantic web was yeah perhaps
00:54:40 in a human-centric sense it it feels like gpt-3 hasn't learned anything that could be used
00:54:51 to reason but that might be just the early days yeah i think that's correct i think the
00:54:56 forms of reasoning that you that you see it perform are basically just reproducing
00:55:02 patterns that it has seen in string data so of course if you're trained on  the entire web then you
00:55:09 can produce an illusion of reasoning in many different situations but it will break down
00:55:14 if it's presented with a novel  situation that's the opening question between the illusion of reasoning and actual reasoning
00:55:21 yes the power to adapt to something that is genuinely new because the thing is even imagine you had
00:55:29  you could train on every bit of data ever generated in history of humanity  it remains
00:55:39 so that model would be capable of of anticipating  many different possible situations but it remains that
00:55:46 the future is going to be something different like for instance if you train a gpt stream
00:55:52 model on on data from the year 2002 for instance and then use it today it's going to be
00:55:58 missing many things it's going to be missing many common sense facts about the world
00:56:04 it's even going to be missing vocabulary and so on yeah it's interesting that  gbt3 even
00:56:09 doesn't have i think any information about the coronavirus yes which is why you know 
00:56:20 a system that's  you you tell that the system is intelligent when it's capable to adapt
00:56:25 so intelligence is gonna require  some amount of continuous learning but it's also gonna require some amount
00:56:31 of improvisation like it's not enough to assume that what you're going to be
00:56:37 asked to do is something that you've seen before or something that is a simple
00:56:41 interpolation of things you've seen before yeah in fact that model breaks down for  even even very
00:56:51 tasks that look relatively simple from a distance like l5 self-driving for instance google had a paper couple of years back
00:57:03 showing that something like 30 million different road situations were actually completely insufficient to train
00:57:11 a driving model it wasn't even l2 right and that's a lot of data that's a lot more data than the
00:57:17 the 20 or 30 hours of driving that a human needs to learn to drive given the knowledge
00:57:21 they've already accumulated well let me ask you on that topic elon musk tesla autopilot
00:57:31 one of the only companies i believe is really pushing for a learning based approach are you you're skeptical that that kind
00:57:39 of network can achieve level four l4 is probably achievable l5 is probably not what's the
00:57:45 distinction there this l5 is completely you can just fall asleep yeah alpha is basically human level well
00:57:52 it will drive you have to be careful saying human level because like that's yeah most of the drivers yeah
00:57:59 that's the clearest example of like you know cars will most likely be much safer than humans in situ
00:58:06 in many situations where humans fail it's the vice versa so i'll tell you you know
00:58:13 the thing is the the amounts of training data you would need to anticipate for pretty much every
00:58:17 possible situation you'll encounter in the real world  is such that
00:58:24 it's not entirely unrealistic to think that at some point in the future we'll develop a system that's running on
00:58:29 enough data especially  provided that we can  simulate a lot of that data
00:58:33 we don't necessarily need actual  actual cars on the road for everything but it's a massive
00:58:41 effort and it turns out you can create a system that's much more adaptative that can generalize
00:58:45 much better explicit models of the surroundings of the car
00:58:55 and if you use deep learning for what it's good at which is to provide perceptive information so in general
00:59:01 deep learning is is a way to encode perception and a way to encode intuition
00:59:08 but it is not a good medium for any sort of explicit reasoning and  in ai systems today  strong
00:59:16 generalization tends to come from explicit models tend to come from abstractions in
00:59:25 the human mind that are encoded in program form by a human engineer right yeah these are the abstractions
00:59:31 you can actually generalize not the sort of weak abstraction that is learned by a neural network yeah and the question is
00:59:37 how much how much reasoning how much strong abstractions are required to solve
00:59:44 particular tasks like driving that's that's the question or human life existence how much how much strong obs
00:59:52 abstractions does existence require but more specifically on driving that's that seems to be that seems to be
00:59:59 a coupled question about intelligence is like  how much
01:00:05 intelligence like how do you build an intelligent system and  the coupled problem how hard is
01:00:12 this problem how much intelligence does this problem actually require so we're we get to cheat right
01:00:20 because we get to look at the problem like it's not like you get to close our eyes and completely new to driving
01:00:26 we get to do what we do as human beings which is  for the majority of our life
01:00:31 before we ever learn quote unquote to drive we get to watch other cars and other people drive we get to be
01:00:37 in cars we get to watch we get to get to see movies about cars we get to
01:00:42 you know get to observe all this stuff and that's similar to what neural networks are doing
01:00:48 it's getting a lot of data and the the the question is yeah how much is  how many leaps of reasoning genius is required
01:00:59 to be able to actually effectively drive i think it's an example of driving i mean sure you've seen a lot of cars
01:01:07 in your life before you learn to drive but let's say you've learned to drive in silicon valley and now you rent a car in tokyo
01:01:15 well now everyone is driving on the other side of the road and the signs are different and the roads are more narrow and so on
01:01:21 so it's a very very different environment  a smart human even an average human should be able to just zero shot it to just
01:01:32 be operational in this in this very different environment yeah right away despite having add
01:01:41 new contacts with the novel complexity that is contained in this environment right and that is another complexity is
01:01:47 not just interpolation over the situations that you've encountered previously
01:01:55 like learning to drive in the u.s right i would say the reason i ask this one of the most
01:02:00 interesting tests of intelligence we have today actively which is driving in terms of having an impact on the
01:02:07 world like when do you think we'll pass that test of intelligence so i i don't think driving is that much
01:02:14 of a test institutions because again there is no task for which skid at that task demonstrates intelligence unless
01:02:23 it's a kind of meta task that involves acquiring new skills so i don't think i think you can actually solve driving
01:02:35 any any real amount of intelligence for instance if you really did have infinite trained data
01:02:41 you could just literally train an end-to-end deep learning model that's driving provided infinite training data the only problem
01:02:50 with the whole idea is collecting a data sets that's sufficiently comprehensive that covers
01:02:55 the very long tail of possible situations you might encounter and it's really just a scale problem so
01:03:00 i think the there's nothing fundamentally wrong   with this plan with this idea it's
01:03:07 just that it strikes me as a fairly inefficient thing to do because you run
01:03:16 into this  this  scanning issue with diminishing returns whereas if instead you took a more manual
01:03:22 engineering approach where you use deep learning modules in combination
01:03:32 with engineering an explicit model of the surrounding of the cars and you and you bridge the two in a clever way
01:03:37 your model will actually start generalizing much earlier and more effectively than the
01:03:42 end-to-end depleting model so why would you not go with the more manually engineering
01:03:47 oriented approach like even if you created that system either the end-to-end deep learning
01:03:55 model system that's infinite data or the slightly more human system i i don't think achieving alpha would demonstrate
01:04:04  general intelligence or intelligence of any generality at all again the only possible test
01:04:11 of generality in ai would be a test that looks at skill acquisition over unknown tasks but for
01:04:16 instance you could take your l5 driver and ask it to to learn to to pilot a
01:04:22 a commercial airplane for instance and then you would look at how much human involvement is required and how much training data
01:04:28 is required  for the system to learn to pirate an airplane and that that gives you a measure of how
01:04:35 intelligent that system is yeah well i mean that's a big leap i get you but
01:04:42 i'm more interested as a problem i would see to me driving is a black box that can generate novel situations at
01:04:50 some rate that what people call edge cases like so it does have
01:04:56 newness that keeps being like we're confronted let's say once a month it is a very long time yes
01:05:02 long term that doesn't mean you cannot solve it  just by by training a statistical
01:05:08 model a lot of data huge amount of data it's it's really a matter of scale
01:05:14 but i guess what i'm saying is if you have a vehicle that achieves level five it is
01:05:19 going to be able to deal with new situations or i mean the data is so large
01:05:31 that the rate of new situations is very low yes that's not intelligent so if we go back to your kind of definition
01:05:39 of intelligence it's the efficiency with which you can adapt to new situations to truly new situations not
01:05:44 situations you've seen before right not situations that could be anticipated by your creators by the
01:05:50 creators of the system but three new situations the efficiency with which you acquire new skills
01:05:57 if you require if in order to pick up a new skill you require a very extensive training data sets
01:06:06 of most possible situations that can that can occur in the practice of that skill then the
01:06:11 system is not intelligent it is mostly just a lookup table yeah well likewise if  in order to acquire
01:06:20 a skill you need a human engineer to write down a bunch of rules that cover
01:06:26 most or every possible situation likewise the system is is not intelligent the system is merely
01:06:33 the output artifact of a process that that depends that happens in the minds
01:06:39 of the engineers that are creating it right it is including  an abstraction that's
01:06:46 produced by the human mind the process of producing of autonomously producing this abstraction
01:06:59 yeah not like if you take an abstraction you encode it on a piece of paper or in a computer
01:07:04 program the abstraction itself is not intelligent what's intelligent is the the agent that's capable of
01:07:11 producing these abstractions right yeah it feels like there's a little bit of a gray area
01:07:17 like because you're basically saying that deep learning forms abstractions too but those abstractions
01:07:26 do not seem to be effective for generalizing far outside of the things that's already seen but
01:07:31 generalize a little bit yeah absolutely no depending does generalize a little bit like
01:07:36 generalization is not it's not a binary it's mark a spectrum yeah and there's a certain point it's a
01:07:42 gray area but there's a certain point where there's an impressive degree of generalization that happens no like
01:07:51 i guess exactly what you were saying is  intelligence is how efficiently you're able to generalize
01:08:01 far outside of the distribution of things you've seen already yes so it's both like the the distance
01:08:06 of how far you can like how new how radically new something is and how efficiently yes absolutely so
01:08:13 you you can think of  intelligence as a measure of an information conversion ratio
01:08:21 like imagine  a space of possible situations and you've covered some of them so you have some amount of information
01:08:31 about your space of possible situations that's provided by the situations you already know
01:08:35 and that's on the other hand also provided by the prior knowledge that the system
01:08:41 brings to the table the prior knowledge that's embedded in the system so the system starts with
01:08:45 some information right about the problem but the task and it's about going from that information to
01:08:54 a program what you would call a skill program a behavioral program that can cover a large area of possible
01:09:01 situation space and essentially the ratio between that area and the amount of information you
01:09:06 start with so a very smart agent  can make efficient uses
01:09:17 of very little information about a new problem and very little prior knowledge as well
01:09:22 to cover a very large area of potential situations in that problem without knowing what these future new
01:09:31 situations are going to be so one of the other big things you talk about in in the paper
01:09:36 we've talked about a little bit already but let's talk about it some more is  actual tests of intelligence so
01:09:45 if we look at like human and machine intelligence do you think tests of intelligence should be different
01:09:50 for humans and machines or how we think about testing of intelligence intelligences that we're after and
01:10:02 therefore the test should be similar so if your goal is to create ais that are more human-like
01:10:12 then it will be super variable obviously to have a test that's that's universal at a price to
01:10:18 both  ais  and humans so that you can you could establish a comparison
01:10:24  between the two that you could tell exactly how  intelligent in terms of human intelligence a given system is so that said the constraints
01:10:36 that apply to artificial intelligence and to human intelligence are very different and your tests should
01:10:44 account for this difference because if you look at artificial systems it's always possible
01:10:52 for an experimenter to buy arbitrary levels of skill at arbitrary tasks either by
01:11:01 injecting a hard-coded prior knowledge into the system via rules and so on that come from the human mind
01:11:08 from the minds of the programmers and also buying  higher levels of skill just by training on more data
01:11:17 for instance you could generate an infinity of different goal games and you could train a good playing system
01:11:25 that way but you could not directly compare it to human goal playing skills because a human that plays go had to develop
01:11:32 that skill in a very constrained environment they had a limited amount of time
01:11:39 their limited amount of energy and of course this started from a different set of priors to solids from
01:11:47  you know innate  human priors so i think if you want to compare the intelligence of two systems like the
01:11:53 intentions of an ai and the intelligence of a human you have to control for priors you have to
01:12:03 start from the the same set of knowledge priors about the task and you have to control for for
01:12:09 experience and that is to say for training data so prior what's priors
01:12:18 so prior is whatever information you have about a given task before you start learning about this task
01:12:25 and how's the difference from experience well experience is acquired right so for instance if you're if
01:12:31 you're trying to play goal your experience with goal is all the goal games you've played
01:12:37 or you've seen or you've simulated in your mind let's say and  your priors are things like
01:12:45 well go go is a game on on a 2d grid and we have lots of hard-coded priors about the organization of 2d space and the
01:12:56 rules of how the the dynamics of the physics of this game in this 2d space
01:13:02 yes and the idea that you have what winning is yes exactly so like and all other board games
01:13:09 can also share some similarities with school and if you've played these board games then
01:13:13  with respect to the game of go that would be part of your priors about the game well it's interesting to think about the
01:13:19 game of goes how many priors are actually brought to the table when you look at  self-play
01:13:28 reinforcement learning based mechanisms that do learning it seems like the number of prizes pretty low yes but
01:13:32 you're saying you should be exp there's a 2d special priority in the covenant right but you should be clear at making
01:13:40 those priors explicit yes  so in particular i think if your if your goal
01:13:46 is to measure a human-like form of intelligence then you should clearly establish that you want
01:13:54 the ai your testing to start from the same set of priors that humans start with right
01:14:01 so i mean to me personally but i think to a lot of people the human side of things is very
01:14:06 interesting so testing intelligence for humans what what do you think is a good test of human intelligence
01:14:16 well that's the question that psychometrics is is interested in what is there's an entire subfield of psychology
01:14:24 that deals with this question so what's psychometrics the psychometrics is the sub-field of psychology that that
01:14:30 tries to measure quantify aspects of the human mind so in particular community
01:14:36 abilities intelligence and personality threats as well so  like what are might be a weird
01:14:44 question but what are like the first principles of the of psychometrics that operates on the you know what what are
01:14:55 the priors it brings to the table so it's a filled with a with a fairly long history
01:15:03 it's so you know psychology sometimes gets a bad reputation for not having very reproducible 
01:15:09 results and some psychometrics as actually some fairly solidly or producible results
01:15:17 so the ideal goals of the field is you know tests should be be reliable which is a an ocean type reproducibility
01:15:25 it should be valid  meaning that it should actually measure what you say but you say it measures so for
01:15:32 instance if you're if you're saying that you're measuring intelligence then your test results should be created with
01:15:39 things that you expect to be correlated with intelligence like success in school or success in the
01:15:43 workplace and so on should be standardized meaning that you can administer your tests to many different
01:15:50 people in the same conditions and it should be free from bias meaning that for instance  if you're if if
01:15:56 your test involves  the english language then you have to be aware that
01:16:02 this creates a bias against people who have english as their second language or people who can't speak english at all
01:16:09 so of course these these principles for creating psychometric tests are very much nighty old i don't think every
01:16:15 psychometric test is is really either reliable valid or offer from bias but at least
01:16:24 the field is aware of these weaknesses and is trying to address them so
01:16:30 it's kind of interesting ultimately you're only able to measure like you said previously
01:16:34 the skill but you're trying to do a bunch of measures of different skills that
01:16:40 correlate as you mentioned strongly with some general concept of cognitive ability yes yes so what's the
01:16:47 factor so right there are many different kinds of tests tests of intelligence and 
01:16:54 each of them is interested in in  different aspects of intelligence you know some of them will deal with
01:16:58 language some of them we deal with a special vision maybe mental rotations numbers and so on
01:17:07 when you run these very different tests at scale what you start seeing is that there are clusters of correlations
01:17:14 among test results so for instance if you look at  homework at school you will see that people
01:17:23 who do well at math are also likely statistically to do well in physics and what's more  there there also
01:17:29 people do well at math and physics are also statistically likely to do well in things that
01:17:36 sound completely unrelated like writing in english essay for instance and so when you see clusters of correlations
01:17:45  in in statical statistical terms you would explain them with a latent variable and the latent variable that would for
01:17:51 instance explain  the relationship between being good at math and being good at physics would be
01:17:59 cognitive ability right and the g factor is the the latent variable that explains  the fact that
01:18:05 every test of intelligence that you can come up with results on that on on this test end up
01:18:12 being correlated so there is some a single  a unique variable  that that explains this correlations
01:18:18 that's the g factor so it's a statistical construct it's not really something you can directly
01:18:25 measure for instance in a person but it's there but it's there it's there it's the art
01:18:30 scale and that's also one thing i want to mention about psychometrics like you know when you talk about
01:18:37 measuring intelligence in in humans for instance some people get a little bit worried they will say you
01:18:42 know that sounds dangerous maybe that's not potentially discriminatory and so on and they're not wrong and the thing is so
01:18:48 personally i'm not interested in psychometrics as a way to characterize one individual person like if if i get
01:18:59 your psychometric personality assessment or your iq i don't think that actually tells me much
01:19:05 about you as a person i think psychometrics is most useful as a statistical tool so it's most
01:19:12 useful at scale it's most useful when you start getting test results for
01:19:18 a large number of people and you start cross-correlating these test results because that gives you information about the structure
01:19:27 of the human mind particularly about the structure of human cognitive abilities so at scale psychometrics paints
01:19:35 a certain picture of the human mind and that's interesting and that's what's relevant to ai the
01:19:41 structure of human currency abilities yeah it gives you an insight into it i mean to me i remember when i learned
01:19:44 about g it it seemed like it would be impossible for it even
01:19:54 it to be real even as a statistical variable like it felt  kind of like astrology like
01:20:00 it's like wishful thinking among psychologists but  the more i learned i realized that there's some
01:20:06 i mean i'm not sure what to make about human beings the fact that the jig factor is a thing
01:20:11 that there's a commonality across all of human species is there destiny to be a strong
01:20:17 correlation between cognitive abilities that's kind of fascinating yeah actually so human connectivities have
01:20:23  a structure like the the most mainstream theory of the structure of cancer abilities
01:20:29 it's called a chc theory it's a cattle horn carol it's name of the industry psychologist
01:20:35 who contributed key pieces of it and it describes  cognitive abilities as a hierarchy with three levels and at
01:20:43 the top you have the g-factor then you have broad cognitive abilities for instance fluid intelligence right
01:20:52 that that encompass a broad set of possible kinds of tasks that are all
01:20:58 related and then you have narrow cognitivity is at the last level which is  closer to task specific skill
01:21:07 and there are actually different theories of the structure of clinical abilities that just emerge from different
01:21:12 statistical analysis of iq test results but they all describe a hierarchy with a kind of g factor
01:21:22 at the top and you're right that the g factor is it's not quite real in the sense that
01:21:27 it's not something you can observe and measure like your height for instance but it's
01:21:32 really in the sense that you you see it in in a statistical analysis of the data
01:21:38 right one thing i want to mention is that the fact that there is a g-factor does not really mean that
01:21:44 human intelligence is a general in a strong sense does not mean human intentions can can
01:21:49 be applied to any problem at all and that someone who has a high iq is going to be able to solve any
01:21:53 problem at all that's not quite what it means i think um
01:22:00 one one popular analogy to understand it is the sports analogy if you consider the concept of
01:22:07 physical fitness it's a concept that's very similar to intelligence because it's a useful concept it's something you
01:22:13 can intuitively understand some people are fit  maybe like you
01:22:20 some people are not as fit maybe like me but none of us can fly absolutely it's so constrained even if
01:22:26 you're very fit that doesn't mean you can do  anything at all in any environment you you obviously
01:22:33 cannot fly you cannot  survive at the bottom of the ocean and so on and if you were a scientist say you want
01:22:40 you wanted to precisely define and measure physical fitness in humans then you would come up with a battery
01:22:47  of tests  like you would you know have running android meter  playing soccer playing
01:22:54 table tennis swimming and so on and  if you run these tests over many different people you will start seeing
01:23:01 correlations and test results for instance people who are good at soccer are so good
01:23:06 at sprinting right and you will explain these correlations with physical abilities that are
01:23:12 strictly analogous to cognitive abilities right and then you would start also observing
01:23:20 correlations between biological  characteristics like maybe lung volume is correlated with being
01:23:27 a a fast runner for instance  in the same way that there are neurophysical  correlates
01:23:35 of cognitive abilities right and at the top of the hierarchy of physical abilities that you would be able to observe you
01:23:40 would have a g-factor a physical g-factor which would map to physical fitness right
01:23:48 and as you just said that doesn't mean that people with a with high physical fitness can fly doesn't mean
01:23:54  human morphology and human physiology is universal it's actually super specialized we can
01:23:59 only do the things and that we were evolved to do right like we are not appropriate to to
01:24:09 to you you could not exist on venus or mars or in the void of space but on the ocean so that said one thing
01:24:15 that's really striking and remarkable is that our morphology generalizes
01:24:25 far beyond the environments that we evolved for like in a way you could say we evolved
01:24:30 to run after prey in the seminar right that's very much
01:24:36 where our human morphology comes from and that said we can we can do a lot of things that
01:24:41 are that are completely unrelated to that we can climb mountains we can
01:24:47 we can swim across lakes  we can play a table tennis i mean table tennis is very different from what we were evolved
01:24:53 to do right so our morphology our bodies or our sense of motor affordances
01:24:59 are of a degree of generality that is absolutely remarkable right and i think cognition is very
01:25:05 similar to that our cognitive abilities have a degree of generality that goes far beyond
01:25:11 what the mind was initially supposed to do which is why we can you know play music and write novels and
01:25:17 and go to mass and do all kinds of crazy things but it's not universal in the same way that human morphology and our body
01:25:26 is not appropriate for actually most of the universe by volume in the same way you could say that the
01:25:30 human mind is naturally appropriate for most of problem space potential problem space 
01:25:37 by volume so we have very strong cognitive biases actually that mean that there are certain types of problems that
01:25:44 we handle very well and certain certain types of problem that we are completely adapted for
01:25:51 so that's really how we interpret the g-factor it's not a sign of strong generality it's it's really just a broader the
01:26:01 broadest cognitive ability but our abilities whether we are talking about sensory motor abilities
01:26:07 or cognitive abilities they still they remain very specialized in the human condition right
01:26:14 within the constraints of the human cognition they're general yes absolutely so but the constraints as you're saying are
01:26:21 very limited what i think what's yeah limiting so we we evolved
01:26:27 our cognition and our body evolved in in very specific environments because our environment was so viable
01:26:34 fast changing and so unpredictable part of the constraints that that drove our evolution
01:26:40 is generality itself so we were in a way evolved to to be able to improvise in all kinds of physical
01:26:47 or cognitive environments right yeah and for this reason it turns out that  the the minds and bodies that we
01:26:54 ended up with  can be applied to much much broader scope than what they were evolved for right
01:27:01 and that's truly remarkable and that goes that's the degree of generalization that is far beyond
01:27:07 anything you can see in artificial systems today right that's it it does not mean that that
01:27:15  human intelligence is anywhere universal yes yeah it's not general you know it's a kind of exciting topic
01:27:21 for people even you know outside of artificial there i think it's mensa whatever
01:27:30 there's different degrees of difficulty for questions we talked about this offline a little
01:27:35 bit too about sort of difficult questions you know what makes a question on an iq test more difficult or less difficult do you
01:27:44 think so the the thing to keep in mind is that there's no such thing as
01:27:50 a question that's intrinsically difficult it has to be difficult to respect to the things you already know
01:27:58 and the things you can already do right so in in terms of an iq test question typically you would have it will be
01:28:05 structured for instance as a set of demonstration input and output pairs right and then
01:28:13 you would be given a test input a prompt and you you you would need to recognize or produce
01:28:20 the corresponding output and in that narrow context you could say a difficult
01:28:28 question is a question where the input prompt is very surprising and unexpected given the
01:28:37 the training examples just even the nature of the patterns that you're observing in the input problem
01:28:41 for instance let's say you have a rotation problem you must rotate the shape by 90 degrees
01:28:48 if i give you two examples and then i'll give you one one prompt which is actually one of the two
01:28:52 training examples then there is zero generalization difficulty for the task it's actually
01:28:57 triggered task you just recognize that it's one one of the training examples and you produce
01:29:02 the same answer now if it's  if it's a more complex shape there is you know a little bit more
01:29:08 generalization but it remains that you are still doing the same thing at this time
01:29:14 as you were being demonstrated at training time a difficult task starts to require some amount of  test time adaptation
01:29:23 some amount of improvisation right so  consider i don't know you're teaching a
01:29:31 class on like quantum physics or something if  if you wanted to kind of test
01:29:40 the understanding that students have of the material you would come up with an exam that's very different
01:29:49 from anything they've seen like on the internet when they were cramming  on the other hand if you wanted to make it easy
01:29:56 you would just give them something that's very similar to the the mock exams that that that
01:30:02 they've taken something that's just a simple interpolation of questions that they've
01:30:07 they've already seen and so that would be an easy exam it's very similar to what you've been trained on
01:30:14 and a difficult exam is one that really probes your understanding because it forces you
01:30:21 to improvise it forces you to do things  that are different from what you were exposed to before
01:30:28 so that said it doesn't mean that the exam that requires improvisation is intrinsically hard right because maybe
01:30:34 you're you're a quantum physics expert so when you take the exam this is actually stuff that despite being you
01:30:41 new to the students it's not new to you right so it can only be difficult with respect to what the test taker
01:30:50 already knows and with respect to the information that the test taker has about the task so that's what i mean by controlling for
01:30:58 priors what you the information you bring to the table and the exp and experience which is
01:31:03 the training data so in in the case of the the quantum physics exam that would be  all the the the course material
01:31:10 itself and all the mock exams that students might have taken online yeah it's interesting because i've
01:31:16 also i i sent you an email and i asked you like i've been
01:31:24 this just this curious question of you know what's a really hard iq test question and i've been talking to also people who
01:31:32 have designed iq tests there's a few folks on the internet it's like a thing people are really
01:31:37 curious about it first of all most of the iq tests they designed they like religiously
01:31:45 protect against the correct answers like you can't find the correct answers anywhere in fact the question is ruined once you
01:31:51 know even like the approach you're supposed to take so they're very
01:31:57 the approach is implicit in in the training examples so here it is the training examples it's over
01:32:04 well which is why in arc for instance there is a test set that is private and no one has seen it
01:32:12 no for really tough iq questions it's not obvious it's not because the ambiguity like it's  and you have to
01:32:21 look to them but like some number sequences and so on it's not completely clear so like you can get a
01:32:27 sense but there's like some you know when you look at a number sequence i don't know
01:32:37  like your fibonacci number sequence if you look at the first few numbers that sequence could be completed in a
01:32:42 lot of different ways and you know some are if you think deeply or more correct than others
01:32:50 like there's a kind of intuitive simplicity and elegance to the correct solution yes
01:32:56 i am personally not a fan of ambiguity in in test questions actually but i think you can have difficulty
01:33:03  without requiring ambiguity simply by making the test  require a lot of extrapolation over
01:33:09 the training examples but the beautiful question is difficult but gives away everything
01:33:17 when you give the training example basically yes meaning that so the the tests i'm interested in in creating
01:33:26 are not necessarily difficult  for humans because  human intelligence is the benchmark  they're supposed to be difficult 
01:33:34 for machines in ways that are easy for humans like i think an ideal
01:33:39  test of human and machine intelligence is a test that is  actionable  that
01:33:48 highlights  the need for progress and that highlights the direction in which you should be making progress i i think
01:33:54 we'll talk about the arc challenge and the test you've constructed you have these elegant examples
01:33:59 i think that highlight like this is really easy for us humans but it's really hard for machines but
01:34:09 on the you know the designing an iq test for iqs of like a higher than 160 and so on you have to say you have to take that
01:34:16 and put on steroids right you have to think like what is hard for humans and that's a fascinating exercise
01:34:27 and it was an interesting question of what it takes to create a really hard question for humans because um
01:34:36 you again have to do the same process as you mentioned which is  you know something
01:34:45 basically where the experience that you have likely to have encountered throughout your whole life even if you've prepared
01:34:50 for iq tests which is a big challenge that this will still be novel for you
01:34:58 yeah i mean novelty is a requirement you should not be able to practice for the questions that you're gonna be
01:35:04 tested on that's important because otherwise what you're doing is not exhibiting intelligence what
01:35:10 you're doing is just retrieving  what you've been exposed before it's it's the same thing as deep learning
01:35:15 model if you train a deep learning model on  all the possible answers then it will ace your test in the same way that
01:35:24  you know  as a stupid student  can still ace the test if they cram for it they memorize
01:35:34 you know 100 different possible mock exams and then they hope that the actual exam will be a very simple
01:35:41 interpolation of the mock exams and that student could just be a deep learning model at that point
01:35:46 but you can actually do that without any understanding of the material and in fact many students
01:35:51 pass the exams in exactly this way and if you want to avoid that you need an exam that's
01:35:57 unlike anything they've seen that really probes their understanding so how do we design an iq test for machines
01:36:07 and intelligent tests for machines all right so in the paper i outline a number of requirements
01:36:15 that you expect of such a test and in particular we should start by acknowledging the priors
01:36:23 that we expect to be required in order to perform the test so we should be explicit about the
01:36:28 priors right  and if the goal is to compare machine intelligence and human intelligence then
01:36:34 we should assume  human cognitive bias right and secondly we should make sure that we are testing
01:36:45 for skilled acquisition ability  skill acquisition efficiency in particular and not for skill itself
01:36:51 meaning that every task featured in your test should be novel and should not be something that
01:36:55 you can anticipate so for instance it should not be possible to
01:37:00 brute force the space of possible questions right to pre-generate every possible question
01:37:06 and the answer so it should be tasks that cannot be anticipated not just by the system itself but by the creators
01:37:16 of the system right yeah you know what's fascinating i mean one of my favorite aspects of
01:37:21 the paper and the work you do with the arc challenge is the the process of making priors explicit
01:37:32 just even that act alone is a really powerful one of like what are it's a
01:37:39 it's a really powerful question ask of us humans what are the priors that we bring to the table
01:37:46 so the the next step is like once you have those priors how do you use them to solve a novel task but like just even
01:37:52 making the prize explicit is a really difficult and really powerful step
01:37:59 and that's like visually beautiful and conceptually philosophically beautiful part of the work you did
01:38:05 with  and i guess continue to do  probably with the with the paper and the arc challenge
01:38:10 can you talk about some of the priors that we're talking about here yes so a researcher has done a lot of
01:38:15 work on what exactly  are the knowledge priors that
01:38:23 that are innate to humans is elizabeth spelkie from harvard so she developed the core knowledge  theory
01:38:33 which  outlines four different  core knowledge systems  so systems of knowledge that we are
01:38:41 basically either born with or that we are hardwired to acquire very early on in our development and there's no 
01:38:50 there's no strong distinction between the two like if you are
01:38:58 primed to acquire as a certain type of knowledge  in just a few weeks you might as well
01:39:04 just be born with it it's just it's just part of who you are and so there are there are four
01:39:09 different core knowledge systems like the first one is the notion of objectness and
01:39:18 a basic physics like you recognize that something that moves  currently for instance is an object
01:39:27 so we intuitively naturally innately divide the world into objects based on this notion of
01:39:33 coherence physical currents and in terms of elementary physics there's the the fact that  you know objects
01:39:41 can bump against each other and the fact that they can occlude each other so these are
01:39:48 things that we are essentially born with or at least that we are going to be acquiring extremely early
01:39:53 because really hard wire to acquire them so a bunch of points pixels that move together on objects
01:40:05 are partly the same object yes i mean i mean that like i don't i don't smoke weed but
01:40:12 if i did that's something i could sit like all night and just like think about i remember right
01:40:18 in your paper just object-ness i wasn't self-aware i guess of how that particular prior
01:40:26 that that's such a fascinating prior that like and that's that's the most basic one but
01:40:34 yes just identity just yeah object yes it's it's very basic i suppose but it's so fundamental is this phenomenal
01:40:41 team and cognition yeah and  the second prior that's also fundamental is
01:40:48 agent-ness which is not a real world a real world but so agentness the fact that some of
01:40:52 these objects  that you that you segment your environment into
01:40:58 some of these objects are agents so what's an agent it's  basically it's an object that has
01:41:07 goals so that has what that has goals this this capable of person goals so for instance if you see
01:41:13 two dots  moving in in a roughly synchronized fashion you will intuitively infer that one of the dots is pursuing
01:41:21 the other so that one of the dots is  and and one of the dots is an agent
01:41:29 and its goal is to avoid the other dot and one of the dots the other dot is also an asian and its goal is to catch
01:41:37 the first start pelkey has shown that babies you know as young as three months identify  agentness and goal directedness
01:41:48 in their environment another prior is basic you know geometry and topology like the notion of distance the ability
01:41:56 to navigate in your environment and so on this is something that is fundamentally
01:42:02 hardwired into our brain it's in fact backed by very specific neural mechanisms
01:42:10 like for instance grid cells and plate cells so it's it's something that's literally hard coded at the at the new level
01:42:20  you know you know hypocampus and the last prior would be the notion of numbers like
01:42:25 numbers are not actually cultural constructs we are intuitively innately able to do some basic counting and to compare quantities
01:42:35  so it doesn't mean we can do arbitrary arithmetic   counting the actual accounting
01:42:42 scanning like counting one two three ish then maybe more than three  you can also compare
01:42:46 quantities if i give you   three dots and five dots you can tell the the
01:42:51 the side with five dots there's more dots  so this is actually an innate  prior so
01:43:00 that said the list may not be exhaustive  the potential existence of new knowledge systems for instance
01:43:13  knowledge systems that would deal with social  relationships yeah yeah i mean
01:43:20 which is which is much much less relevant   to something like arc or iq testing
01:43:25 right so there could be stuff that's  like like you said rotation symmetry is really interesting it's very
01:43:32 likely that there is  speaking about rotation that there is  in the brain
01:43:39 a hard-coded system that is capable of performing rotations  one one famous experiment  that
01:43:45 people did in the  i don't remember who it was exactly but in the in
01:43:52 the 70s was that people found that if you asked people if you give them
01:43:58  two different shapes and one of the shapes is a rotated version of the first shape and you ask them
01:44:05 is is that shape a related version of first step or not what you see is that the time it takes
01:44:11 people to answer is linearly proportional right to the angle of rotation so it's almost
01:44:19 like you have in somewhere in your brain like a turntable with a fixed speed and if you want to know if two two
01:44:27 objects   are rotated version of each other you put the object on the turntable
01:44:34 you let it move around a little bit and then you and then you stop when you have a match and and that that's really interesting
01:44:43 so what's the arc challenge so in in the paper outline you know all these principles
01:44:49 that a good test of machine intelligence and humanitarian should follow and the arc challenge is one attempt
01:44:57 to embody as many of these principles as possible so i don't think it's it's anywhere near a perfect attempt
01:45:05 right it does not actually follow every principle but it is what i was able to do given the
01:45:10 given the constraints so the format of arc is very similar to classic iq tests in particular
01:45:18 ravens progressive mattresses ravens yeah ravens privacy mattresses i mean if you've done like you test in the past
01:45:24 you know where that is probably at least you've seen it even if you don't know what it's called and
01:45:31 so you have a set of  tasks that's what they're called and for each task you have
01:45:38 training data which is a set of input and output pairs so i  an input or output pair is a grid of colors
01:45:46 basically the grid the size of the grades these variables is the size of the grid is variable
01:45:54 and you're given an input and you must transform it into the proper outputs
01:46:00 right and so you're shown a few demonstrations of a task in the form of existing input output pairs and then you're given a new input
01:46:10 and you must provide you must produce the correct output and in arc is that every task should
01:46:25 only require cool knowledge priors should not require any outside knowledge so for instance  no language
01:46:37  no english nothing like this  new concepts  taken from  our human experience like trees dogs cats and so on so only
01:46:48  tasks that are reasoning tasks that are built on top of a core knowledge priors and some of the
01:46:54 tasks are actually explicitly trying to probe  specific forms of abstraction right
01:47:03  part of the reason why i wanted to create arc is i'm a big believer in
01:47:12 you know when you're faced with  a problem as murky as understanding how to autonomously
01:47:20 generate abstraction in a machine you have to co-evolve the solution and the problem and so part of the
01:47:29 reason why i design act was to clarify my ideas about the nature of abstraction right and some of the tasks are actually
01:47:37 designed to to probe  bits of that theory and there are things that are
01:47:43 turned out to be very easy for humans to perform including young kids right but turn out to be
01:47:51 near impossible informations so whatever you learn from the nature of abstraction  from from designing that
01:47:59 like what can you clarify what you mean one of the things you wanted to try to understand was this  idea of abstraction
01:48:09 yes so clarifying  my own ideas about abstraction by forcing myself to produce tasks that
01:48:15 would require  the ability to produce that form of abstraction in order to solve them
01:48:22 got it okay so and by the way just  i mean people should check out i'll probably overlay if you're watching the
01:48:27 video part but the the grid input output with the different colors on the grid
01:48:34 and that's it that's i mean it's a very simple world but it's kind of beautiful it's it's
01:48:39 very similar to classic acutes like it's not very original in that sense the main difference with iq tests is that
01:48:46 we make the priors explicit which is not usually the case in iq test so you make it explicit that everything
01:48:51 should only be built out of core knowledge priors i also think it's generally
01:48:58 more more diverse than iq tests in general and it's it perhaps requires a bit more manual work to produce solutions because
01:49:07 you have to to click around on a grid for a while sometimes the grades can be as large as 30 by 30 cells
01:49:15 so how did you come up if you can reveal  with the questions like what's the process of the questions was it
01:49:20 mostly you yeah that came up with the questions what  how difficult is it to come up
01:49:25 with a question like is this scalable to a much larger number if you think you
01:49:32 know with iq tests you might not necessarily want to or need it to be scalable with machines it's possible you
01:49:41 could argue that it needs to be scalable so there are a thousand questions a thousand tasks yes wow including the
01:49:49 test and the private test set i think it's fairly difficult in the sense that a big requirement
01:49:56 is that every task should be novel and unique and unpredictable right like you don't want to create
01:50:05 your your own little world that is  simple enough that it would be possible for a human
01:50:12 to reverse and generate and write down an algorithm that could generate every possible arc task and their solution for
01:50:18 instance that we completely invalidated the test so you're constantly coming up on new stuff
01:50:23 you need yeah you need a source of novelty of unthinkable novelty and one thing i
01:50:30 found is that as a human  you are not a very good source of  unthinkable novelty and so you
01:50:38 have to pace the creation of these tasks quite a bit there are only so many
01:50:43 unique tasks that you can do in a given day so that means coming up with truly original new ideas
01:50:52 did psychedelics help you at all i'm just gonna but i mean that's fascinating to think
01:50:56 about like so you would be like walking or something like that are you constantly thinking of something totally new
01:51:06 yes i mean this is hard this is yeah i i i mean i i'm not saying i've done anywhere near a perfect job at
01:51:13 it  there is some amount of redundancy and there are many imperfections in arc
01:51:17 so that said you should you should consider arc as a work in progress it is not  the definitive state
01:51:26  where where the the arc tasks today are not definitive states of the test i want to
01:51:31 keep refining it in the future i also think it should be possible to open up the creation of tasks
01:51:40 to a broad audience to do crowdsourcing that would involve several levels of filtering obviously
01:51:45 but i think it's possible to apply crowdsourcing to to develop a much bigger and much more diverse arc data set
01:51:53 that would also be free of potentially you know some of my own personal biases but is there
01:51:58 always need to be a part of arc that's the test like is hidden yes absolutely it is
01:52:07 impressive that  the test that you're using to actually benchmark algorithms is not accessible to the
01:52:15 people developing these algorithms because otherwise what's going to happen is that the human engineers are just
01:52:19 going to solve the tasks themselves and and encode their solution in program form
01:52:27 but that again what you're seeing here is the process of intelligence happening in the mind of the human and
01:52:34 and then you're just  capturing its crystallized output but that crystallized output
01:52:39 is not the same thing as the process generated that's right it's not intelligent in itself so what 
01:52:43 by the way the idea of crowdsourcing it is fascinating i think i think the creation of questions
01:52:52 is really exciting for people i think i think there's a lot of really brilliant people out there that love to create
01:52:56 these kinds of stuff yeah one thing that  that kind of surprised me that i wasn't expecting is that
01:53:03 lots of people seem to actually enjoy ark as a as a kind of game and i was really seeing it as as a test
01:53:10 as a benchmark  of  a fluid  general intelligence and lots of people just including kids
01:53:18 just started you know enjoying it as a game so i think that's that's encouraging yeah i'm fascinated by there's a world
01:53:24 of people who create iq questions i think i think that's a cool  it's a cool
01:53:32 activity for machines that for humans and people humans are themselves fascinated by
01:53:39 taking the questions like you know measuring their own intelligence i mean that's just really compelling
01:53:45 it's really interesting to me too it helps one of the cool things about arc you said it's kind of  inspired by iq
01:53:52 tests or whatever follows a similar process but because of its nature because of the context in which it lives
01:54:00 it immediately forces you to think about the nature of intelligence as opposed to just a test of your own like it forces
01:54:07 you to really think there's i don't know if it's if it's within the question inherent in the question or just the
01:54:12 fact that it lives in the test that's supposed to be a test of machine intelligence absolutely as you
01:54:20 as you solve arc tasks as a human you will  be forced to basically introspect yeah higher how you come up with
01:54:29 solutions and that forces you to reflect on  the human problem solving process and the way your own mind  generates
01:54:41  abstract representations of the problems  it's exposed to i i think it's due to the fact that the
01:54:50 set of core knowledge priors that arc is built upon is so small it's all a recombination of a very very
01:55:00 small set of assumptions okay so what's the future of ark so you you held arc as a challenge as part of
01:55:06 like a kegel competition yes calgary competition and  what do you think do you think that's
01:55:14 something that continues for five years ten years like just continues growing yes absolutely
01:55:21 so arc itself will keep evolving so i've talked about crowdsourcing i think that's a that's a
01:55:27 good avenue another thing i'm starting is i'll be collaborating with folks from the psychology department at nyu
01:55:37 to do human testing on arc and i think there are lots of interesting questions you can start asking especially as you
01:55:45 start correlating machine solutions to arc tasks and and the human characteristics of solutions
01:55:51 like for instance you can try to see if there's a relationship between the
01:55:57 human perceived difficulty of a task and the machine person yes and and exactly some measure of machine
01:56:03 perceived difficulties yeah it's a nice big playground in which to explore this very difference
01:56:07 it's the same thing as we talked about the autonomous vehicles the things that could be difficult for humans might be
01:56:12 very different than the things that yes absolutely and  formalizing or making explicit that difference in difficulty will teach
01:56:20 us something may teach us something fundamental about intelligence so one thing i think we did well  with arc
01:56:29 is that it's proving to be a very  actionable test in the sense that  machine performance and arcs started
01:56:37 at very much zero initially while you know humans found actually the tasks very easy
01:56:47 and that that alone was like a big red flashing light saying that something is going on
01:56:52 and that we are missing something and at the same time  machine performance did not stay at
01:56:58 zero for very long actually within two weeks of the carol competition we started having
01:57:04 a non-zero number and now the state of the art is around  twenty percent of the test set  solved
01:57:12 and so arc is actually a challenge where our capabilities start at zero which indicates the need for progress
01:57:20 but it's also not an impossible change it's not accessible you can start making progress basically right away at the same time
01:57:29 we are still very far from having solved it and that's actually a very positive outcome of the
01:57:34 competition is that the competition has has proven that there was no obvious shortcut to solve these tasks
01:57:43 right yeah so the test held up yeah exactly that was the primary reason to do the cargo competition is to check
01:57:51 if some some you know clever person was going to hack the benchmark and that did not happen
01:57:57 right like people who are solving the tasks are essentially doing it   well in a way they're they're
01:58:05 they're actually exploiting some flaws of art that we will need to address in the future especially they're
01:58:11 essentially anticipating what sort of  tasks may be contained in the test sets right right which is kind of
01:58:18 yeah that's the kind of hacking it's it's human hacking of the town yes that that said you know   with the
01:58:23 state of the art it's like  20 percent we're still very very far  from even level which is closer to 100
01:58:32 and so and i i do believe that you know it will it will take a while  until
01:58:39 we reach a human parity on ark and that by the time we have human party
01:58:46 we will have ai systems that are probably pretty close to human level in terms of general fluid intelligence
01:58:52 which is i mean it's they're not going to be necessarily human-like they're not necessarily
01:58:59  you would not necessarily recognize them as you know being an egi but they would be capable of a degree of
01:59:07 generalization that matches the generalization performed by human food intelligence sure i mean this is a good point in
01:59:14 terms of general flu intelligence to mention in your paper you describe different kinds
01:59:20 of generalizations  local broad extreme and there's kind of a hierarchy that you form
01:59:32 what are we talking about what kinds are there right so  generalization is is very old idea
01:59:37 i mean it's even older than machine learning in the context of machine learning you say
01:59:45 a system generalizes if it can  make sense of an input it has it has not yet seen
01:59:51 and that's what i would call a system-centric  generalization you is generalization with respect to novelty
02:00:01  for the specific system you're considering so i think a good test of intelligence should actually
02:00:09  deal with  developer aware generalization which is slightly stronger than system-centric transition
02:00:15 so developer generalization developer the ability to generalize to novelty or uncertainty that
02:00:24 not only the system itself has not accessed to but the developer of the system could not have access to either that
02:00:30 that's a fascinating that's a fascinating meta definition so like the system is  it's basically
02:00:38 the edge case thing we're talking about with autonomous vehicles yes neither the developer nor the system
02:00:44 know about the edge cases so it's up to they get the system should be able to generalize the thing that
02:00:52 that  nobody expected neither the designer of the training data nor obviously the contents of the training
02:01:00 that's a fascinating definition so you can see generalization degrees of generalization as a spectrum
02:01:07 and the lowest level is what machine learning is trying to do is the assumption that any new situation is going to be sampled
02:01:17 from a static distribution of possible situations and that you already have a representative sample of that
02:01:23 distribution that's your training data and so in machine learning you generalize to a new sample from a known distribution
02:01:31 and the ways in which your new sample will be new or different are ways that are
02:01:38 already understood by the developers of the system so you are generalizing to known unknowns
02:01:46 for one specific task that's what you would call robustness you are robust to things like noise
02:01:51 small variations and so on for one a fixed known distribution that that you know through
02:01:59 your training data and a higher degree would be flexibility in machine intelligence so
02:02:07 flexibility would be something like an l5 cell driving car or maybe a robot that can
02:02:16 you know pass the the coffee cup test which is the notion that you would be given a
02:02:21 random kitchen  somewhere in the country and you would have to you know
02:02:27 go make a cup of coffee in that kitchen right so flexibility would be the ability to deal with
02:02:34 unknown unknowns so things that could not  dimensions of viability that could not have been possibly foreseen
02:02:41 by the creators of the system within one specific task so generalizing to the long tail of situations
02:02:47 in self-driving for instance would be flexibility so you have robustness flexibility and finally you would have
02:02:53 extreme generalization which is basically flexibility but instead of just considering one specific
02:03:03 domain like driving or domestic robotics you're considering an open-ended range of possible domains
02:03:11 so a robot would be capable of extreme generalization if let's say it's designed and trained
02:03:19  to to for cooking for instance and if i if i buy the robot and if i'm able  if it's able  to
02:03:26 teach itself gardening in in a couple weeks it would be capable
02:03:32 of extreme generalization for instance so the ultimate goal is extreme generalization yes
02:03:37 so be  creating a system that is so general that it could essentially achieve a human skill parity over
02:03:47 arbitrary tasks and arbitrary domains with the same level of you know improvisation and adaptation power as
02:03:53 humans when when it encounters new situations and it would do so
02:03:59  over basically the same range of possible domains and tasks  as humans and using this essentially
02:04:05 the same amount of training experience of practice as humans would require that will be human level extreme generalization
02:04:14 so i i don't actually think humans are anywhere near the  optimal intelligence bound if there is
02:04:21 such a thing so i think for humans or in general in general i think it's quite likely you know that
02:04:28 there is an a hard limit to how intelligent any system can be but at the same time i
02:04:36 don't think humans are anywhere near that limit yeah last time i think we talked i think
02:04:42 you had this idea that  we're only as intelligent as the problems we face
02:04:50 sort of  yes we are upper bounded by the problem so in a way yes we are we are bounded by on
02:04:55 our environments and we are bounded by the problems we try to solve
02:05:01 yeah yeah what do you make of neuralink and  outsourcing some of the brain power like bring computer
02:05:09 interfaces do you think we can expand our augment our intelligence i am fairly skeptical
02:05:19 of neural interfaces because they are trying to fix one specific bottleneck in in human machine cognition which is
02:05:29 the bandwidth bottleneck input and output of information in the brain and my perception
02:05:37 of the problem is that bandwidth is not at this time a bottleneck at all meaning that we already have sensors
02:05:44 that enable us to to take in far more information than what we can
02:05:51 actually process well to push back on that a little bit  to sort of play dell's advocate a
02:05:55 little bit is if you look at the internet wikipedia let's say wikipedia
02:06:01 i would say that humans after the advent of wikipedia are much more intelligent yes
02:06:08 i think that's a good one but that's also not about that's about externalizing our intelligence
02:06:18 via   information processing systems the accidental function processing system which is very different from
02:06:24  brain computer interfaces right but the question is whether if we have direct access if our
02:06:30 brain has direct access to wikipedia with our brain already has direct access to wikipedia
02:06:36 it's on your phone and you have your hands and your eyes and your ears and so on  to access
02:06:42 that information and the speed at which you can access it is bottlenecked by the customer i think
02:06:48 it's already closed fairly close to optimal which is why speed reading for instance
02:06:54 does not work yeah the faster you read the less you understand but maybe it's because it uses the eyes
02:06:59 so maybe so i don't believe so i think you know the brain is very slow
02:07:06 it typically operates you know the fastest things that happen in the brain at the level of 50 milliseconds
02:07:13  forming a conscious out can potentially take entire seconds right and you can already
02:07:18 read pretty fast so i think the speed at which you can take information
02:07:25 in and even the speed at which you can add with information can only be very incrementally improved
02:07:32 maybe i think if you're a very fast typer if you're a very trained typer the speed at which you can express your
02:07:38 thoughts is already a speed at which you can form your thoughts right so that's kind of an idea that
02:07:46 there are fundamental bottlenecks to the human mind but it's possible that the everything we have in the human mind
02:07:53 is just to be able to survive in the environment and there's a lot more to expand maybe
02:08:01 you know you said this the speed of the thought so yeah i i think augmenting human intelligence is a very valid
02:08:09 and very powerful avenue right and that's what computers are about in fact that's what you know
02:08:14 all of culture and civilization is about they are culture is externalized cognition and we rely on culture
02:08:25 to think constantly yeah yeah i mean that's that's another yeah that's not just not just computers
02:08:29 not just phones and the internet i mean all of culture like language for instance is a form of external
02:08:35 recognition books are obviously external recognition yeah that's right and you you can scale
02:08:42 that external exclamation you know far beyond the capability of the human brain and you could see you
02:08:47 know civ civilization itself is it has capabilities that are far beyond
02:08:54 any individual brain and will keep scaling it because it's not rebound by individual brains
02:09:02 it's a different kind of system yeah and and that system includes non-human non-humans first of all includes all the
02:09:09 other biological systems which are probably contributing to the overall intelligence of the organism
02:09:14 and then yeah computers are part of it no non-human systems probably not contributing much but
02:09:19 ais are definitely contributing to that like google search for instance yeah yeah a huge part
02:09:31 a part we can probably introspect like how the world has changed in the past 20 years it's probably very difficult for us to
02:09:37 be able to understand until of course whoever created the simulation we're in is
02:09:43 probably doing metrics measuring the progress there was probably a big spike in performance  they're enjoying they're enjoying this
02:09:54 so what are your thoughts on the touring test and the lobner prize which is the
02:10:02 you know one of the most famous attempts at the test of human intelligence  sorry of artificial intelligence
02:10:09 by  doing a natural language open dialogue test that's test that's  judged by humans as far as how well the
02:10:18 machine did so i'm not a fan of the chewing test itself or any of its variants for two reasons
02:10:30 so first of all it's it's really copping out of trying to define and measure
02:10:39 intelligence because it's entirely outsourcing that to a panel of human judges and these human judges
02:10:48 they may not themselves have any proper methodology they may not themselves have any proper definition of intelligence
02:10:55 they may not be reliable so the joint is already failing one of the core psychometrics principles
02:11:01 which is reliability because you have biased human judges  it's also violating the
02:11:08 the standardization requirement and the freedom from bias requirement and so it's really a coop out because
02:11:13 you are outsourcing everything that matters which is precisely describing
02:11:19 intelligence and finding a standalone test  to measure it you're outsourcing everything to
02:11:26  to people so it's really cool and by the way  we should keep in mind that  when turing
02:11:34 proposed  the imitation game it was not meaning for the imitation game to be an actual  goal for the field of ai an actual
02:11:43 test of intelligence he was using  it was using the imitation game as a thought experiment in a philosophical discussion
02:11:53 in his  1950 paper he was trying to argue that theoretically it should be possible
02:12:03 for something very much like the human mind indistinguishable from the human mind to be encoded in ensuring machine and at
02:12:09 the time that was that was you know a very daring idea it was stretching
02:12:17 credibility but  nowadays i think it's it's fairly well accepted that the
02:12:22 the mind is an information processing system and that you could probably encode it
02:12:27 into a computer so another reason why i'm not a fan of this type of test is that it the incentives that it creates
02:12:37 are incentives that are not conducive to proper scientific research if your goal is to trick to convince
02:12:46 a panel of human judges that they're talking to a human then you have an incentive to rely
02:12:56 on on tricks and press the digitization in the same way that let's say you're doing physics and you want to solve teleportation
02:13:04 and what if the test that you set out to pass is you need to convince a panel of judges that teleportation
02:13:09 took place and and they're just sitting there and watching what you're doing
02:13:15 and that is something that you can achieve with you know david copperfield could could
02:13:20 achieve it in his in his show at vegas right but is it and what he's doing is very elaborate but it's not actually it's not
02:13:30 physics it's not making any progress you know understanding of the universe right to push back on that it's possible
02:13:36 that's the hope with these kinds of subjective evaluations is that it's easier to solve it
02:13:43 generally than it is to come up with tricks that convince a large number of judgments that's the whole
02:13:49 in practice when it turns out that it's very easy to deceive people in the same way that you know you can
02:13:54 you can do magic in vegas you can actually very easily convince people that they're talking to human when they're actually
02:14:00 talking to liberalism i just disagree i disagree with that i think it's easy i i would i would push
02:14:08 it's not easy it's   it's doable it's very easy because i wouldn't say it's very easy though we are biased
02:14:14 like we have theory of mind we are constantly projecting emotions intentions yes  
02:14:23 agentness agentness is one of our core innate priors right we are projecting these things on everything around us like
02:14:31 if you if you paint a smiley on a rock the rock becomes happy you know eyes and because we have this
02:14:37 extreme bias that permeates everything everything we see around this it's actually pretty easy to trick people
02:14:44 like this it is very very short i so totally disagree with that you brilliantly put there's a huge it's
02:14:50 the anthropomorphization that we naturally do the agentness of that word is that real word but no
02:14:56 it's not a real word i like it but it's exactly why it's useful well it's a useful word let's
02:15:00 make it real it's a huge help but i still think it's really difficult to convince 
02:15:06 if you do like the alexa prize formulation where you know you talk for an hour
02:15:11 like there's formulations of the test you can create where it's very difficult so i like i like the extra price better because
02:15:19 it's more pragmatic it's more practical it's actually incentivizing developers to create something that's useful
02:15:27 yeah as a as as a a human machine interface  so that's slightly better than just the imitation so i like your your
02:15:36 your ideas like a test which hopefully help us in creating intelligent systems as a result like if you create a system
02:15:41 that passes it it'll be useful for creating further intelligence systems
02:15:47 yes at least yeah i mean i'm just to kind of comment i'm a little bit surprised
02:15:54 how little inspiration people draw from the touring test today you know the media and the popular press might
02:16:00 write about it every once in a while the philosophers might talk about it but like most engineers are not really inspired
02:16:07 by it and i know i know you don't like the touring test but  we'll have this argument another
02:16:14 time you know i there's something inspiring about it i think that
02:16:20 as as a philosophical device in a physical discussion i think there is something very interesting about it i
02:16:25 don't think it is in practical terms i don't think it's it's conducive to to progress
02:16:31 and one of the reasons why is that you know i think being very human-like being indistinguishable from a human
02:16:39 is actually the very last step in the creation of machine intelligence that the first
02:16:46 ais that will show strong generalization   in in  that that will actually  implement human like broad cognitive abilities
02:16:54 they will not actually be able to look anything  like humans human likeness is the very last step in that process
02:17:04 and so a good test is a test that points you towards the first step  on the ladder not towards the top of
02:17:09 the ladder right yeah so to push back on that so i guess i usually agree with you on most things
02:17:14 i remember you i think at some point tweeting something about the turing test not being being counterproductive or
02:17:20 something like that and i think a lot of very smart people agree with that i
02:17:28   a  you know  computation speaking not very smart person  disagree with that because i
02:17:33 think there's some magic to the interactivity interactivity with other humans so to push to play devil's advocate on your statement
02:17:42 it's possible that in order to demonstrate the the generalization abilities of a system
02:17:47 you have to show your ability in conversation show your ability to
02:17:55 adjust adapt to the conversation through not just like as a standalone system but
02:18:00 through the process of like the interaction like game theoretic where you're you really are
02:18:09 changing the environment by your actions so in the arc challenge for example you're an observer you can't you can't scare
02:18:17 the test into into changing you can't talk to the test you can't play with it so there's some
02:18:24 aspect of that interactivity that becomes highly subjective but it feels like it could be conducive
02:18:30 to yeah generalization you make a great point the interactivity is a very good setting to force the
02:18:35 system to show adaptation to shoot generalization  that that said you at the same time 
02:18:44 it's not something very scalable because you rely on human judges it's not something reliable because the
02:18:49 images may not may not so you don't like human judges basically yes and i think so i i love
02:18:55 the idea of interactivity i initially wanted an arc test that had some amount of interactivity
02:19:03 where your score on a task would not be one or zero if you can solve it or not but would be the
02:19:11 number of attempts that you can make before you hit the right solution which
02:19:16 means that now you can start applying the scientific method as you solve our tasks that you can
02:19:22 start formulating hypothesis and and and probing the system to see whether the hypothesis is
02:19:27 the observation will match the buddhists or not it would be amazing if you could also even higher level than that measure the
02:19:35 quality of your attempts which of course is impossible but again that's gets subjective yes like how good
02:19:40 was your thinking like it's yeah how efficient was so one thing that's interesting
02:19:47 about this notion of scoring you as how many attempts you need is that you can start producing
02:19:53 tasks that are way more ambiguous right right because you can with the problem with the with
02:20:00 the different attempts you can actually probe that ambiguity right right so that's in a sense which yeah so
02:20:10 it's how good can you adapt to the uncertainty and reduce the uncertainty yes it's half fast with is the efficiency
02:20:21 with which to reduce uncertainty in in program space exactly very difficult to come up with that kind of
02:20:25 test though yeah so  i would love to be able to create something like this in practice
02:20:30 it would be it would be very very difficult but yes but  i mean what you're doing what you've
02:20:35 done with the arc challenge is is  brilliant i'm also not i'm surprised that it's not more popular
02:20:42 but i think it's picking up what does it snitch it does yeah what are your thoughts about
02:20:47 another test that talks with marcus hutter he has the harder prize for
02:20:51 compression of human knowledge and the idea is really sort of quantify like reduce the test of
02:20:57 intelligence purely to just ability to compress what's your thoughts about this intelligence that's compression
02:21:07 i mean it's a it's a very  fun test because it's it's such a simple idea like you're given wikipedia basically
02:21:13 english wikipedia and you must compress it and so it stems from the idea that cognition is compression
02:21:22 that the brain is basically a compression algorithm this is a very old idea it's a very i think striking
02:21:32 and beautiful idea i used to believe it  i eventually had to realize that it was it was
02:21:37 very much a flawed idea so i no longer believe that compression is recognition is compression so but i
02:21:43 can tell you what's the difference so it's very easy to believe
02:21:49 that cognition and compression are the same thing because  so jeff hawkins for instance says
02:21:53 that cognition is prediction and of course prediction is basically
02:21:58 the same thing as compression right it's just including the temporal axis and it's very easy to believe this because
02:22:07 compression is something that we do all the time very naturally we are constantly you know
02:22:13 compressing information we are  constantly trying we have this bias towards simplicity we
02:22:20 we're constantly trying to organize things in our mind and around us to be more regular right so
02:22:27  it's it's a beautiful idea it's very easy to believe  there is a big difference between 
02:22:32 what we do with our brains and and compression so compression is actually kind of
02:22:39 a tool in the human cognitive toolkits that is is used in many ways but it's just a
02:22:44 tool it is not it is a tool for cognition it is not cognition itself
02:22:50 and the big fundamental difference is that cognition is about being able to operate in future situations
02:23:00 that include fundamental uncertainty and novelty so for instance consider a child at age 10 and so they have 10 years
02:23:11 of life experience they've gotten you know pain pleasure rewards and and punishment in a period
02:23:16 of time if you were to generate the shortest behavioral program that would have basically
02:23:27 run that child over this 10 years in an optimal way right the shortest optimal behavioral
02:23:32 program given the experience of that child so far well that program
02:23:38 that that compress program this is what you would get if the mind of the child was a compression algorithm essentially
02:23:45 would be utterly enable inappropriate to process the next 70 years in the in the life of the child
02:23:57 so in the models with we build of the world we are not trying to make them actually
02:24:04 optimally compressed we are we are using compression as a tool to promote simplicity and efficiency in our models
02:24:12 but they are not perfectly compressed because they need to include things that are seemingly useless today
02:24:20 that have seemingly been useless so far but that may turn out to be useful in the future because you just don't
02:24:26 know the future unless that's the fundamental principle  that cognition that intelligence arises from is that
02:24:34 you need to be able to run appropriate behavioral programs except you have absolutely no idea
02:24:39 what sort of context environment situation are going to be running in and you have to deal with that with that
02:24:46 uncertainty with that future normality so an analogy an analogy that you can make is
02:24:54 with investing for instance if i look at the past   you know 20 years of stock market data
02:25:01 and i use a compression algorithm to figure out the best trading strategy it's going to be you
02:25:06 know you buy apple stock then maybe the past few years you buy tesla stock or something but is that strategy still going to be
02:25:14 true for the next 20 years well actually probably not which is why if you're a smart investor you're not
02:25:23 you're not just going to be following the strategy that corresponds to compression of the past
02:25:30  you're going to be following   you're going to have a balanced portfolio yeah right because you just don't know
02:25:38 what's going to run i mean i guess in that same sense the compression is analogous to
02:25:43 what you talked about which is like local or robust generalization versus extreme generalization it's much
02:25:49 closer to that side of being able to generalize in in a local sense that's why you know as
02:25:56 humans as  when we are when we are children in our education so a lot of it is driven by place
02:26:05 even by curiosity  we we are not efficiently compressing things we're actually exploring
02:26:15 we are retaining all kinds of   things from our environment that that seem to be completely useless because
02:26:21 they might turn out to be eventually useful right and it's it's that's what cognition is
02:26:27 really about and that what makes it antagonistic to compression is that it is about hedging for future
02:26:34 uncertainty and that's efficient into compression yes especially hedging so
02:26:41  cognition leverages compression as a tool to promote   to promote efficiency right and so
02:26:47 in that sense in our models it's like einstein said make it simpler but not however that
02:26:53 quote goes but not too simple so you want to compression simplifies things but you don't want to
02:27:00 make it too simple yes so a good model of the world is going to include
02:27:05 all kinds of things that are completely useless actually just because just in case yes because you need diversity in the
02:27:11 same way that in your portfolio you need all kinds of stocks that that may not have performed well so far
02:27:15 but you need diversity and the reason you need diversity because fundamentally you don't know what you're
02:27:21 doing and the same is true of the human mind is that it needs to to behave appropriately in the future and it has no idea what
02:27:29 the future is going to be like it's a bit it's not going to be like the past so compressing the past is not
02:27:34 appropriate because the past is not  it's not proactive in the future yeah history repeats itself but not perfectly
02:27:47 i don't think i asked you last time the most inappropriately absurd question we've talked a lot about intelligence
02:27:57 but you know the bigger question from intelligence is of meaning you know intelligence systems are kind of goal-oriented
02:28:05 there's throws optimizing for goal if you look at the harder prize actually i mean there's
02:28:09 always there's always a clean formulation of a goal but the natural questions for us humans
02:28:15 since we don't know our objective function is what is the meaning of it all so the absurd question is what
02:28:23 francois chole do you think is the meaning of life what's the meaning of life yeah that's a
02:28:29 that's a big question and i think i can i can you know give you my answer at least
02:28:39 one of my answers and so you know the one thing that's  very important  in understanding who we are is that
02:28:50 everything that makes up  that makes up ourselves it makes up we are even even your most personal thoughts
02:28:59 is not actually your own right like even your most personal thoughts are expressed in words that you did not
02:29:06 invent and are built on concepts and images that you did not invent we are very much  cultural beings
02:29:16 right well we are made of culture we are not that what makes us different from animals for instance
02:29:21 right so we are everything about ourselves is an echo of the past an echo of people who lived  before us
02:29:32 right that's who we are and in the same way if we manage to contribute something to the collective edifice of culture
02:29:43 a new idea maybe a beautiful piece of music a work of art a grand theory a new word
02:29:52 maybe that something is is going to become a part of the minds of future humans essentially
02:30:02 forever so everything we do creates ripples right that propagates into the future
02:30:08 and i i and that that's in a way this is this is our path to immortality is that
02:30:17 as we contribute things to culture culture in turn in turn becomes future humans and we keep influencing people you know
02:30:28 thousands of years from now so our actions today create reports and these reports i think
02:30:37 basically sum up the meaning of life like in the same way that we are the the sum of the interactions between many
02:30:46 different reports that came from our past we are ourselves creating reports that will propagate into the future
02:30:53 and that's why you know we should be this seems like perhaps anything to say but we should be
02:31:01 kind to others during our time on earth because every act of kindness creates reports and
02:31:07 and in reverse every act of violence also creates reports and you want you want to carefully choose which kind
02:31:14 of reports you want to create and you want to propagate into the future and in your case
02:31:19 first of all beautifully put but in your case creating ripples into the future human and future agi systems
02:31:34 i don't think there's a better way to end it francois as always for second time and i'm sure many times
02:31:40 in the future it's been a huge honor you know one of the most brilliant people in the machine learning computer science
02:31:48 science world again that's a huge honor thanks for talking today it's been a pleasure thanks a lot for having me
02:31:54 i really appreciate it thanks for listening to this conversation with friend squash chole
02:31:59 and thank you to our sponsors babble master class and cash app click the sponsor links in
02:32:04 the description to get a discount and to support this podcast if you enjoy this thing subscribe on youtube
02:32:11 review five stars on apple podcast follow on spotify support on patreon or connect with me on
02:32:17 twitter at lex friedman and now let me leave you with some words from rene descartes in 1668
02:32:24 an except of which francois includes in is on the measure of intelligence paper if there were machines which bore a
02:32:31 resemblance to our bodies and imitated our actions as closely as possible for all practical purposes
02:32:38 we should still have two very certain means of recognizing that they were not real men the first is
02:32:43 that they could never use words or put together signs as we do in order to declare our thoughts to others
02:32:51 for we can certainly conceive of a machine so constructed that it utters words and even utters
02:32:57 words that correspond to bodily actions causing a change in its organs but it is not conceivable that such a
02:33:03 machine should produce different arrangements of words so as to give it an appropriately
02:33:09 meaningful answer to whatever is said in his presence as the dullest of men can do
02:33:14 here descartes is anticipating the turing test and the argument still continues to this day
02:33:21 secondly he continues even though some machines might do some things as well as we do them or
02:33:26 perhaps even better they would inevitably fail in others which would reveal
02:33:31 that they're acting not from understanding but only from the disposition of their organs
02:33:39 this is incredible quote for whereas reason is a universal instrument which can be used in all kinds of situations
02:33:49 these organs need some particular action hence it is for all practical purposes impossible for machine to have enough
02:33:54 different organs to make it act in all the contingencies of life in the way
02:34:01 in which our reason makes us act that's the debate between mimicry memorization versus understanding
