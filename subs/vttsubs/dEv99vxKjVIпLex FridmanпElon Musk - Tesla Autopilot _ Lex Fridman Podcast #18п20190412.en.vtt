WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:03.080
- The following is a
conversation with Elon Musk.

00:00:03.080 --> 00:00:06.290
He's the CEO of Tesla, SpaceX, Neuralink,

00:00:06.290 --> 00:00:09.340
and a co-founder of
several other companies.

00:00:09.340 --> 00:00:10.970
This conversation is part of

00:00:10.970 --> 00:00:13.300
the Artificial Intelligence Podcast.

00:00:13.300 --> 00:00:15.740
This series includes leading researchers

00:00:15.740 --> 00:00:17.320
in academia and industry,

00:00:17.320 --> 00:00:20.380
including CEOs and CTOs of automotive,

00:00:20.380 --> 00:00:24.200
robotics, AI and technology companies.

00:00:24.200 --> 00:00:26.460
This conversation
happened after the release

00:00:26.460 --> 00:00:28.680
of the paper from our group at MIT

00:00:28.680 --> 00:00:30.570
on driver functional vigilance

00:00:30.570 --> 00:00:32.637
during use of Tesla's Autopilot.

00:00:32.637 --> 00:00:34.630
The Tesla team reached out to me

00:00:34.630 --> 00:00:37.560
offering a podcast
conversation with Mr. Musk.

00:00:37.560 --> 00:00:40.620
I accepted with full control
of questions I could ask

00:00:40.620 --> 00:00:43.650
and the choice of what
is released publicly.

00:00:43.650 --> 00:00:46.920
I ended up editing out
nothing of substance.

00:00:46.920 --> 00:00:49.750
I've never spoken with Elon
before this conversation,

00:00:49.750 --> 00:00:51.820
publicly or privately.

00:00:51.820 --> 00:00:54.450
Neither he nor his
companies have any influence

00:00:54.450 --> 00:00:57.850
on my opinion, nor on
the rigor and integrity

00:00:57.850 --> 00:00:59.770
of the scientific method that I practice

00:00:59.770 --> 00:01:01.850
in my position at MIT.

00:01:01.850 --> 00:01:04.650
Tesla has never financially
supported my research

00:01:04.650 --> 00:01:07.320
and I've never owned a Tesla vehicle,

00:01:07.320 --> 00:01:10.190
and I've never owned Tesla stock.

00:01:10.190 --> 00:01:12.820
This podcast is not a scientific paper,

00:01:12.820 --> 00:01:14.380
it is a conversation.

00:01:14.380 --> 00:01:16.750
I respect Elon as I do all other leaders

00:01:16.750 --> 00:01:18.680
and engineers I've spoken with.

00:01:18.680 --> 00:01:21.460
We agree on some things
and disagree on others.

00:01:21.460 --> 00:01:23.490
My goal, as always with
these conversations,

00:01:23.490 --> 00:01:26.950
is to understand the way
the guest sees the world.

00:01:26.950 --> 00:01:29.940
One particular point of
disagreement in this conversation

00:01:29.940 --> 00:01:33.250
was the extent to which
camera-based driver monitoring

00:01:33.250 --> 00:01:36.110
will improve outcomes and for how long

00:01:36.110 --> 00:01:39.113
it will remain relevant
for AI-assisted driving.

00:01:39.960 --> 00:01:42.520
As someone who works
on and is fascinated by

00:01:42.520 --> 00:01:45.200
human-centered artificial intelligence,

00:01:45.200 --> 00:01:48.730
I believe that, if implemented
and integrated effectively,

00:01:48.730 --> 00:01:51.860
camera-based driver monitoring
is likely to be of benefit

00:01:51.860 --> 00:01:55.660
in both the short term and the long term.

00:01:55.660 --> 00:01:59.250
In contrast, Elon and Tesla's focus

00:01:59.250 --> 00:02:01.210
is on the improvement of Autopilot

00:02:01.210 --> 00:02:04.470
such that its statistical safety benefits

00:02:04.470 --> 00:02:09.050
override any concern for
human behavior and psychology.

00:02:09.050 --> 00:02:12.030
Elon and I may not agree on everything,

00:02:12.030 --> 00:02:14.850
but I deeply respect the
engineering and innovation

00:02:14.850 --> 00:02:16.890
behind the efforts that he leads.

00:02:16.890 --> 00:02:19.860
My goal here is to catalyze a rigorous,

00:02:19.860 --> 00:02:21.800
nuanced and objective discussion

00:02:21.800 --> 00:02:26.270
in industry and academia
on AI-assisted driving,

00:02:26.270 --> 00:02:30.900
one that ultimately makes
for a safer and better world.

00:02:30.900 --> 00:02:34.623
And now, here's my
conversation with Elon Musk.

00:02:35.630 --> 00:02:37.560
What was the vision, the dream,

00:02:37.560 --> 00:02:40.030
of Autopilot in the beginning?

00:02:40.030 --> 00:02:43.720
The big picture system level
when it was first conceived

00:02:43.720 --> 00:02:46.110
and started being installed in 2014,

00:02:46.110 --> 00:02:47.570
the hardware in the cars?

00:02:47.570 --> 00:02:49.780
What was the vision, the dream?

00:02:49.780 --> 00:02:51.400
- I wouldn't characterize
it as a vision or dream,

00:02:51.400 --> 00:02:56.400
it's simply that there are
obviously two massive revolutions

00:02:56.562 --> 00:03:00.130
in the automobile industry.

00:03:00.130 --> 00:03:03.023
One is the transition to electrification,

00:03:04.460 --> 00:03:06.393
and then the other is autonomy.

00:03:07.720 --> 00:03:12.720
And it became obvious to
me that, in the future,

00:03:13.290 --> 00:03:16.280
any car that does not have autonomy

00:03:16.280 --> 00:03:19.170
would be about as useful as a horse.

00:03:19.170 --> 00:03:20.820
Which is not to say that there's no use,

00:03:20.820 --> 00:03:23.750
it's just rare, and
somewhat idiosyncratic,

00:03:23.750 --> 00:03:25.540
if somebody has a horse at this point.

00:03:25.540 --> 00:03:26.620
It's just obvious that cars

00:03:26.620 --> 00:03:28.090
will drive themselves completely,

00:03:28.090 --> 00:03:29.680
it's just a question of time.

00:03:29.680 --> 00:03:34.680
And if we did not participate
in the autonomy revolution,

00:03:37.040 --> 00:03:40.930
then our cars would not
be useful to people,

00:03:40.930 --> 00:03:43.760
relative to cars that are autonomous.

00:03:43.760 --> 00:03:47.250
I mean, an autonomous
car is arguably worth

00:03:48.260 --> 00:03:52.943
five to 10 times more than a
car which is not autonomous.

00:03:53.800 --> 00:03:55.471
- In the long term.

00:03:55.471 --> 00:03:57.730
- Depends what you mean by long term but,

00:03:57.730 --> 00:03:59.580
let's say at least for
the next five years,

00:03:59.580 --> 00:04:00.543
perhaps 10 years.

00:04:01.500 --> 00:04:04.140
- So there are a lot of very
interesting design choices

00:04:04.140 --> 00:04:05.790
with Autopilot early on.

00:04:05.790 --> 00:04:10.010
First is showing on
the instrument cluster,

00:04:10.010 --> 00:04:12.750
or in the Model 3 and
the center stack display,

00:04:12.750 --> 00:04:14.943
what the combined sensor suite sees.

00:04:15.780 --> 00:04:17.970
What was the thinking behind that choice?

00:04:17.970 --> 00:04:20.550
Was there a debate, what was the process?

00:04:20.550 --> 00:04:22.510
- The whole point of the display

00:04:22.510 --> 00:04:25.520
is to provide a health check on

00:04:26.901 --> 00:04:28.160
the vehicle's perception of reality.

00:04:28.160 --> 00:04:30.067
So the vehicle's taking in information

00:04:30.067 --> 00:04:32.280
from a bunch of sensors,
primarily cameras,

00:04:32.280 --> 00:04:37.280
but also radar and
ultrasonics, GPS and so forth.

00:04:37.423 --> 00:04:42.423
And then, that information
is then rendered into

00:04:42.560 --> 00:04:46.030
vector space with a bunch of objects,

00:04:46.030 --> 00:04:49.170
with properties like lane lines

00:04:49.170 --> 00:04:51.230
and traffic lights and other cars.

00:04:51.230 --> 00:04:54.890
And then, in vector
space, that is re-rendered

00:04:54.890 --> 00:04:57.960
onto a display so you can confirm whether

00:04:57.960 --> 00:04:59.810
the car knows what's going on or not,

00:05:00.820 --> 00:05:02.730
by looking out the window.

00:05:02.730 --> 00:05:05.470
- Right, I think that's an
extremely powerful thing

00:05:05.470 --> 00:05:07.960
for people to get an understanding,

00:05:07.960 --> 00:05:09.240
sort of become one with the system

00:05:09.240 --> 00:05:11.750
and understanding what
the system is capable of.

00:05:11.750 --> 00:05:14.850
Now, have you considered showing more?

00:05:14.850 --> 00:05:16.700
So if we look at the computer vision,

00:05:18.020 --> 00:05:19.800
like road segmentation, lane detection,

00:05:19.800 --> 00:05:23.070
vehicle detection, object
detection, underlying the system,

00:05:23.070 --> 00:05:25.750
there is at the edges, some uncertainty.

00:05:25.750 --> 00:05:29.440
Have you considered revealing the parts

00:05:29.440 --> 00:05:33.670
that the uncertainty in
the system, the sort of--

00:05:33.670 --> 00:05:35.600
- Probabilities associated with say,

00:05:35.600 --> 00:05:36.850
image recognition or something like that?

00:05:36.850 --> 00:05:40.130
- Yeah, so right now, it shows
the vehicles in the vicinity,

00:05:40.130 --> 00:05:43.480
a very clean crisp image,
and people do confirm

00:05:43.480 --> 00:05:44.660
that there's a car in front of me

00:05:44.660 --> 00:05:46.700
and the system sees there's
a car in front of me,

00:05:46.700 --> 00:05:49.070
but to help people build an intuition

00:05:49.070 --> 00:05:50.810
of what computer vision is,

00:05:50.810 --> 00:05:53.120
by showing some of the uncertainty.

00:05:53.120 --> 00:05:58.120
- Well, in my car I always look
at this with the debug view.

00:05:58.240 --> 00:06:00.110
And there's two debug views.

00:06:00.110 --> 00:06:04.970
One is augmented vision,
which I'm sure you've seen,

00:06:04.970 --> 00:06:08.570
where it's basically we
draw boxes and labels

00:06:08.570 --> 00:06:10.483
around objects that are recognized.

00:06:11.470 --> 00:06:15.330
And then there's we what
call the visualizer,

00:06:15.330 --> 00:06:18.020
which is basically vector
space representation,

00:06:18.020 --> 00:06:21.053
summing up the input from all sensors.

00:06:22.470 --> 00:06:24.723
That does not show any pictures,

00:06:28.276 --> 00:06:29.570
which basically shows the car's view

00:06:29.570 --> 00:06:32.383
of the world in vector space.

00:06:33.520 --> 00:06:35.532
But I think this is very difficult

00:06:35.532 --> 00:06:37.170
for normal people to understand,

00:06:37.170 --> 00:06:39.550
they're would not know what
thing they're looking at.

00:06:39.550 --> 00:06:41.390
- So it's almost an HMI challenge through

00:06:41.390 --> 00:06:44.830
the current things that are
being displayed is optimized

00:06:44.830 --> 00:06:47.170
for the general public understanding

00:06:47.170 --> 00:06:48.810
of what the system's capable of.

00:06:48.810 --> 00:06:51.750
- If you have no idea how
computer vision works or anything,

00:06:51.750 --> 00:06:53.080
you can still look at the screen

00:06:53.080 --> 00:06:55.840
and see if the car knows what's going on.

00:06:55.840 --> 00:06:58.500
And then if you're a development engineer,

00:06:58.500 --> 00:07:02.430
or if you have the
development build like I do,

00:07:02.430 --> 00:07:06.070
then you can see all
the debug information.

00:07:06.070 --> 00:07:10.353
But this would just be like
total gibberish to most people.

00:07:11.320 --> 00:07:14.290
- What's your view on how
to best distribute effort?

00:07:14.290 --> 00:07:16.890
So there's three, I would
say, technical aspects

00:07:16.890 --> 00:07:19.100
of Autopilot that are really important.

00:07:19.100 --> 00:07:20.530
So it's the underlying algorithms,

00:07:20.530 --> 00:07:22.330
like the neural network architecture,

00:07:22.330 --> 00:07:24.530
there's the data that it's trained on,

00:07:24.530 --> 00:07:27.710
and then there's the hardware
development and maybe others.

00:07:27.710 --> 00:07:32.130
So, look, algorithm, data, hardware.

00:07:32.130 --> 00:07:35.290
You only have so much money,
only have so much time.

00:07:35.290 --> 00:07:37.369
What do you think is
the most important thing

00:07:37.369 --> 00:07:40.070
to allocate resources to?

00:07:40.070 --> 00:07:42.320
Or do you see it as
pretty evenly distributed

00:07:43.180 --> 00:07:44.580
between those three?

00:07:44.580 --> 00:07:46.700
- We automatically get
vast amounts of data

00:07:46.700 --> 00:07:48.720
because all of our cars have

00:07:51.670 --> 00:07:53.640
eight external facing cameras,

00:07:53.640 --> 00:07:58.163
and radar, and usually
12 ultrasonic sensors,

00:07:59.120 --> 00:08:02.563
GPS obviously, and IMU.

00:08:09.908 --> 00:08:12.320
And we've got about
400,000 cars on the road

00:08:12.320 --> 00:08:13.920
that have that level of data.

00:08:13.920 --> 00:08:15.880
Actually, I think you keep quite
close track of it actually.

00:08:15.880 --> 00:08:16.713
- Yes.

00:08:16.713 --> 00:08:20.820
- Yeah, so we're approaching
half a million cars on the road

00:08:20.820 --> 00:08:22.420
that have the full sensor suite.

00:08:24.948 --> 00:08:27.410
I'm not sure how many
other cars on the road

00:08:27.410 --> 00:08:29.430
have this sensor suite,

00:08:29.430 --> 00:08:32.340
but I'd be surprised if
it's more than 5,000,

00:08:32.340 --> 00:08:35.193
which means that we have
99% of all the data.

00:08:36.150 --> 00:08:38.420
- So there's this huge inflow of data.

00:08:38.420 --> 00:08:40.710
- Absolutely, a massive inflow of data.

00:08:40.710 --> 00:08:44.440
And then it's taken us about three years,

00:08:44.440 --> 00:08:45.740
but now we've finally developed

00:08:45.740 --> 00:08:47.690
our full self-driving computer,

00:08:47.690 --> 00:08:52.690
which can process an
order of magnitude as much

00:08:53.850 --> 00:08:56.370
as the NVIDIA system that we
currently have in the cars,

00:08:56.370 --> 00:09:00.290
and to use it, you unplug
the NVIDIA computer

00:09:00.290 --> 00:09:02.643
and plug the Tesla
computer in and that's it.

00:09:05.810 --> 00:09:07.950
In fact, we still are exploring

00:09:07.950 --> 00:09:10.190
the boundaries of its capabilities.

00:09:10.190 --> 00:09:11.990
We're able to run the
cameras at full frame-rate,

00:09:11.990 --> 00:09:15.470
full resolution, not even crop the images,

00:09:15.470 --> 00:09:20.020
and it's still got headroom
even on one of the systems.

00:09:20.020 --> 00:09:23.510
The full self-driving computer
is really two computers,

00:09:23.510 --> 00:09:26.140
two systems on a chip,
that are fully redundant.

00:09:26.140 --> 00:09:27.460
So you could put a boat through

00:09:27.460 --> 00:09:30.230
basically any part of that
system and it still works.

00:09:30.230 --> 00:09:33.537
- The redundancy, are they
perfect copies of each other or--

00:09:33.537 --> 00:09:34.460
- Yeah.

00:09:34.460 --> 00:09:36.010
- Oh, so it's purely for redundancy

00:09:36.010 --> 00:09:38.470
as opposed to an arguing
machine kind of architecture

00:09:38.470 --> 00:09:40.080
where they're both making decisions,

00:09:40.080 --> 00:09:42.207
this is purely for redundancy.

00:09:42.207 --> 00:09:43.870
- Think of it more like it's

00:09:43.870 --> 00:09:45.570
a twin-engine commercial aircraft.

00:09:47.550 --> 00:09:51.823
The system will operate best
if both systems are operating,

00:09:52.920 --> 00:09:55.703
but it's capable of
operating safely on one.

00:09:56.650 --> 00:10:00.300
So, as it is right now, we can just run,

00:10:00.300 --> 00:10:04.460
we haven't even hit
the edge of performance

00:10:04.460 --> 00:10:09.340
so there's no need to actually distribute

00:10:09.340 --> 00:10:13.860
functionality across both SOCs.

00:10:13.860 --> 00:10:17.010
We can actually just run a
full duplicate on each one.

00:10:17.010 --> 00:10:19.270
- So you haven't really explored

00:10:19.270 --> 00:10:20.660
or hit the limit of the system.

00:10:20.660 --> 00:10:22.540
- [Elon] No not yet, the limit, no.

00:10:22.540 --> 00:10:24.760
- So the magic of deep learning

00:10:24.760 --> 00:10:27.290
is that it gets better with data.

00:10:27.290 --> 00:10:29.640
You said there's a huge inflow of data,

00:10:29.640 --> 00:10:32.180
but the thing about driving,
- Yeah.

00:10:32.180 --> 00:10:36.003
- the really valuable data to
learn from is the edge cases.

00:10:39.060 --> 00:10:44.060
I've heard you talk somewhere
about Autopilot disengagements

00:10:44.180 --> 00:10:46.990
being an important moment of time to use.

00:10:46.990 --> 00:10:48.320
Is there other edge cases

00:10:48.320 --> 00:10:52.640
or perhaps can you speak
to those edge cases,

00:10:52.640 --> 00:10:54.700
what aspects of them might be valuable,

00:10:54.700 --> 00:10:56.170
or if you have other ideas,

00:10:56.170 --> 00:10:57.400
how to discover more and more

00:10:57.400 --> 00:10:59.253
and more edge cases in driving?

00:11:00.310 --> 00:11:02.410
- Well there's a lot of
things that are learnt.

00:11:02.410 --> 00:11:04.940
There are certainly edge cases where,

00:11:04.940 --> 00:11:08.120
say somebody's on Autopilot
and they take over,

00:11:08.120 --> 00:11:12.500
and then that's a trigger
that goes out to our system

00:11:12.500 --> 00:11:15.200
and says, okay, did they
take over for convenience,

00:11:15.200 --> 00:11:18.160
or did they take over
because the Autopilot

00:11:18.160 --> 00:11:19.420
wasn't working properly?

00:11:19.420 --> 00:11:21.900
There's also, let's say
we're trying to figure out,

00:11:21.900 --> 00:11:26.393
what is the optimal spline for
traversing an intersection.

00:11:27.920 --> 00:11:31.400
Then the ones where there
are no interventions

00:11:32.550 --> 00:11:33.680
are the right ones.

00:11:33.680 --> 00:11:36.410
So you then you say, okay,
when it looks like this,

00:11:36.410 --> 00:11:37.453
do the following.

00:11:38.320 --> 00:11:40.980
And then you get the optimal spline for

00:11:42.100 --> 00:11:44.823
navigating a complex intersection.

00:11:47.390 --> 00:11:49.190
- So there's kind of the common case,

00:11:49.190 --> 00:11:52.320
So you're trying to capture
a huge amount of samples

00:11:52.320 --> 00:11:54.990
of a particular intersection
when things went right,

00:11:54.990 --> 00:11:57.260
and then there's the edge case

00:11:57.260 --> 00:12:00.170
where, as you said, not for convenience,

00:12:00.170 --> 00:12:03.047
but something didn't go exactly right.

00:12:03.047 --> 00:12:06.030
- So if somebody started
manual control from Autopilot.

00:12:06.030 --> 00:12:07.680
And really, the way to look at this

00:12:07.680 --> 00:12:09.940
is view all input as error.

00:12:09.940 --> 00:12:12.680
If the user had to do
input, there's something,

00:12:12.680 --> 00:12:13.960
all input is error.

00:12:13.960 --> 00:12:16.400
- That's a powerful line
to think of it that way

00:12:16.400 --> 00:12:17.800
'cause it may very well be error,

00:12:17.800 --> 00:12:19.990
but if you wanna exit the highway,

00:12:19.990 --> 00:12:23.130
or if it's a navigation decision

00:12:23.130 --> 00:12:25.430
that Autopilot's not
currently designed to do,

00:12:25.430 --> 00:12:27.850
then the driver takes
over, how do you know

00:12:27.850 --> 00:12:29.060
the difference?
- Yeah, that's gonna change

00:12:29.060 --> 00:12:32.130
with Navigate on Autopilot,
which we've just released,

00:12:32.130 --> 00:12:33.763
and without stalk confirm.

00:12:37.603 --> 00:12:40.066
Assuming control in order
to do a lane change,

00:12:40.066 --> 00:12:44.780
or exit a freeway, or doing
a highway interchange,

00:12:44.780 --> 00:12:46.070
the vast majority of that will go away

00:12:46.070 --> 00:12:48.930
with the release that just went out.

00:12:48.930 --> 00:12:52.960
- Yeah, so that, I don't
think people quite understand

00:12:52.960 --> 00:12:54.550
how big of a step that is.

00:12:54.550 --> 00:12:55.500
- Yeah, they don't.

00:12:56.400 --> 00:12:58.270
If you drive the car then you do.

00:12:58.270 --> 00:12:59.620
- So you still have to keep your hands

00:12:59.620 --> 00:13:00.790
on the steering wheel currently

00:13:00.790 --> 00:13:02.740
when it does the automatic lane change.

00:13:05.990 --> 00:13:08.720
There's these big leaps through
he development of Autopilot,

00:13:08.720 --> 00:13:11.320
through its history and,

00:13:11.320 --> 00:13:13.600
what stands out to you as the big leaps?

00:13:13.600 --> 00:13:16.190
I would say this one,
Navigate on Autopilot

00:13:16.190 --> 00:13:21.130
without having to confirm is a huge leap.

00:13:21.130 --> 00:13:22.855
- It is a huge leap.
- What are the--

00:13:22.855 --> 00:13:24.910
It also automatically overtakes slow cars.

00:13:24.910 --> 00:13:29.910
So it's both navigation and
seeking the fastest lane.

00:13:31.070 --> 00:13:36.070
So it'll overtake slow
cars and exit the freeway

00:13:36.980 --> 00:13:38.733
and take highway interchanges,

00:13:40.661 --> 00:13:44.900
and then we have traffic
light recognition,

00:13:47.200 --> 00:13:50.240
which introduced initially as a warning.

00:13:50.240 --> 00:13:52.330
I mean, on the development
version that I'm driving,

00:13:52.330 --> 00:13:56.940
the car fully stops and
goes at traffic lights.

00:13:56.940 --> 00:13:58.530
- So those are the steps, right?

00:13:58.530 --> 00:14:00.010
You've just mentioned some things

00:14:00.010 --> 00:14:03.320
that are an inkling of a
step towards full autonomy.

00:14:03.320 --> 00:14:07.330
What would you say are
the biggest technological

00:14:07.330 --> 00:14:10.010
roadblocks to full self-driving?

00:14:10.010 --> 00:14:13.573
- Actually, the full self-driving
computer that we just,

00:14:14.430 --> 00:14:17.570
the Tesla, what we call, FSD computer

00:14:17.570 --> 00:14:19.663
that's now in production,

00:14:21.910 --> 00:14:25.580
so if you order any Model S or X,

00:14:25.580 --> 00:14:29.570
or any Model 3 that has the
full self-driving package,

00:14:29.570 --> 00:14:31.053
you'll get the FSD computer.

00:14:32.390 --> 00:14:37.290
That's important to have
enough base computation.

00:14:37.290 --> 00:14:40.173
Then refining the neural net
and the control software.

00:14:41.800 --> 00:14:45.200
All of that can just be provided
as an over-the-air update.

00:14:45.200 --> 00:14:47.110
The thing that's really profound,

00:14:47.110 --> 00:14:51.530
and what I'll be emphasizing
at the investor day

00:14:51.530 --> 00:14:53.320
that we're having focused on autonomy,

00:14:53.320 --> 00:14:56.130
is that the car is
currently being produced,

00:14:56.130 --> 00:14:58.210
with the hard word
currently being produced,

00:14:58.210 --> 00:15:01.040
is capable of full self-driving.

00:15:01.040 --> 00:15:03.657
- But capable is an
interesting word because--

00:15:04.610 --> 00:15:05.516
- [Elon] The hardware is.

00:15:05.516 --> 00:15:06.349
- Yeah, the hardware.

00:15:06.349 --> 00:15:07.620
- And as we refine the software,

00:15:09.430 --> 00:15:11.760
the capabilities will
increase dramatically,

00:15:11.760 --> 00:15:14.010
and then the reliability
will increase dramatically,

00:15:14.010 --> 00:15:16.190
and then it will receive
regulatory approval.

00:15:16.190 --> 00:15:17.680
So essentially, buying a car today

00:15:17.680 --> 00:15:19.230
is an investment in the future.

00:15:23.330 --> 00:15:26.330
I think the most profound thing is that

00:15:26.330 --> 00:15:27.830
if you buy a Tesla today,

00:15:27.830 --> 00:15:30.510
I believe you're buying
an appreciating asset,

00:15:30.510 --> 00:15:33.160
not a depreciating asset.

00:15:33.160 --> 00:15:35.360
- So that's a really
important statement there

00:15:35.360 --> 00:15:37.830
because if hardware is capable enough,

00:15:37.830 --> 00:15:38.780
that's the hard thing

00:15:38.780 --> 00:15:41.320
to upgrade usually.
- Yes, exactly.

00:15:41.320 --> 00:15:43.820
- Then the rest is a software problem--

00:15:43.820 --> 00:15:46.993
- Yes, software has no
marginal cost really.

00:15:47.950 --> 00:15:51.510
- But, what's your intuition
on the software side?

00:15:51.510 --> 00:15:54.690
How hard are the remaining steps

00:15:55.860 --> 00:16:00.860
to get it to where the experience,

00:16:02.650 --> 00:16:05.800
not just the safety,
but the full experience

00:16:05.800 --> 00:16:09.421
is something that people would enjoy?

00:16:09.421 --> 00:16:12.860
- I think people it enjoy
it very much so on highways.

00:16:12.860 --> 00:16:16.820
It's a total game changer
for quality of life,

00:16:16.820 --> 00:16:21.380
for using Tesla Autopilot on the highways.

00:16:21.380 --> 00:16:23.060
So it's really just
extending that functionality

00:16:23.060 --> 00:16:28.060
to city streets, adding in
the traffic light recognition,

00:16:29.260 --> 00:16:31.490
navigating complex intersections,

00:16:31.490 --> 00:16:36.490
and then being able to navigate
complicated parking lots

00:16:37.290 --> 00:16:41.330
so the car can exit a parking
space and come and find you,

00:16:41.330 --> 00:16:44.693
even if it's in a complete
maze of a parking lot.

00:16:46.450 --> 00:16:49.970
And, then it can just drop you off

00:16:49.970 --> 00:16:53.000
and find a parking spot, by itself.

00:16:53.000 --> 00:16:56.225
- Yeah, in terms of enjoyabilty,
and something that people

00:16:56.225 --> 00:16:58.890
would actually find a lotta use from,

00:16:58.890 --> 00:17:03.600
the parking lot, it's rich of annoyance

00:17:03.600 --> 00:17:04.760
when you have to do it manually,

00:17:04.760 --> 00:17:06.710
so there's a lot of benefit to be gained

00:17:06.710 --> 00:17:08.670
from automation there.

00:17:08.670 --> 00:17:10.410
So, let me start injecting the human

00:17:10.410 --> 00:17:12.820
into this discussion a little bit.

00:17:12.820 --> 00:17:15.670
So let's talk about full autonomy,

00:17:15.670 --> 00:17:17.500
if you look at the current
level four vehicles

00:17:17.500 --> 00:17:19.823
being tested on row like Waymo and so on,

00:17:20.730 --> 00:17:23.400
they're only technically autonomous,

00:17:23.400 --> 00:17:25.480
they're really level two systems

00:17:26.770 --> 00:17:28.910
with just a different design philosophy,

00:17:28.910 --> 00:17:30.550
because there's always a safety driver

00:17:30.550 --> 00:17:31.710
in almost all cases, and

00:17:31.710 --> 00:17:33.380
they're monitoring the system.
- Right.

00:17:33.380 --> 00:17:38.380
- Do you see Tesla's full
self-driving as still,

00:17:39.070 --> 00:17:43.210
for a time to come, requiring supervision

00:17:43.210 --> 00:17:44.840
of the human being.

00:17:44.840 --> 00:17:47.510
So its capabilities are
powerful enough to drive

00:17:47.510 --> 00:17:49.080
but nevertheless requires a human

00:17:49.080 --> 00:17:50.970
to still be supervising, just like

00:17:50.970 --> 00:17:55.970
a safety driver is in other
fully autonomous vehicles?

00:17:57.410 --> 00:18:01.590
- I think it will require
detecting hands on wheel

00:18:01.590 --> 00:18:06.590
for at least six months or
something like that from here.

00:18:09.540 --> 00:18:13.883
Really it's a question of,
from a regulatory standpoint,

00:18:16.120 --> 00:18:19.810
how much safer than a person
does Autopilot need to be,

00:18:19.810 --> 00:18:23.183
for it to be okay to not monitor the car.

00:18:24.950 --> 00:18:27.150
And this is a debate that one can have,

00:18:27.150 --> 00:18:32.150
and then, but you need
a large amount of data,

00:18:32.950 --> 00:18:35.280
so that you can prove,
with high confidence,

00:18:35.280 --> 00:18:37.620
statistically speaking, that the car

00:18:37.620 --> 00:18:40.390
is dramatically safer than a person.

00:18:40.390 --> 00:18:42.830
And that adding in the person monitoring

00:18:42.830 --> 00:18:45.960
does not materially affect the safety.

00:18:45.960 --> 00:18:50.170
So it might need to be 200
or 300% safer than a person.

00:18:50.170 --> 00:18:51.200
- And how do you prove that?

00:18:51.200 --> 00:18:52.440
- Incidents per mile.

00:18:52.440 --> 00:18:53.550
- Incidents per mile.
- Yeah.

00:18:53.550 --> 00:18:56.950
- So crashes and fatalities--

00:18:56.950 --> 00:18:58.710
- Yeah, fatalities would be a factor,

00:18:58.710 --> 00:19:00.490
but there are just not enough fatalities

00:19:00.490 --> 00:19:04.060
to be statistically significant, at scale.

00:19:04.060 --> 00:19:07.400
But there are enough crashes,

00:19:07.400 --> 00:19:10.050
there are far more crashes
then there are fatalities.

00:19:11.020 --> 00:19:14.953
So you can assess what is
the probability of a crash.

00:19:16.330 --> 00:19:19.640
Then there's another step
which is probability of injury.

00:19:19.640 --> 00:19:21.730
And probability of permanent injury,

00:19:21.730 --> 00:19:23.900
the probability of death.

00:19:23.900 --> 00:19:27.720
And all of those need to be
much better than a person,

00:19:27.720 --> 00:19:32.720
by at least, perhaps, 200%.

00:19:33.080 --> 00:19:36.080
- And you think there's
the ability to have

00:19:36.080 --> 00:19:38.720
a healthy discourse with
the regulatory bodies

00:19:38.720 --> 00:19:40.100
on this topic?

00:19:40.100 --> 00:19:44.270
- I mean, there's no
question that regulators paid

00:19:44.270 --> 00:19:46.920
a disproportionate amount of attention

00:19:46.920 --> 00:19:48.760
to that which generates press,

00:19:48.760 --> 00:19:50.940
this is just an objective fact.

00:19:50.940 --> 00:19:53.410
And it also generates a lot of press.

00:19:53.410 --> 00:19:58.410
So, in the United States there's, I think,

00:19:58.560 --> 00:20:01.950
almost 40,000 automotive deaths per year.

00:20:01.950 --> 00:20:04.510
But if there are four in Tesla,

00:20:04.510 --> 00:20:07.040
they will probably receive
a thousand times more press

00:20:07.040 --> 00:20:08.860
than anyone else.

00:20:08.860 --> 00:20:11.480
- So the psychology of that
is actually fascinating,

00:20:11.480 --> 00:20:12.590
I don't think we'll have enough time

00:20:12.590 --> 00:20:15.360
to talk about that, but I
have to talk to you about

00:20:15.360 --> 00:20:17.070
the human side of things.

00:20:17.070 --> 00:20:19.803
So, myself and our team
at MIT recently released

00:20:19.803 --> 00:20:22.940
a paper on functional vigilance of drivers

00:20:22.940 --> 00:20:24.640
while using Autopilot.

00:20:24.640 --> 00:20:27.540
This is work we've been
doing since Autopilot

00:20:27.540 --> 00:20:31.070
was first released publicly,
over three years ago,

00:20:31.070 --> 00:20:34.640
collecting video of driver
faces and driver body.

00:20:34.640 --> 00:20:38.490
So I saw that you tweeted
a quote from the abstract,

00:20:38.490 --> 00:20:43.490
so I can at least guess
that you've glanced at it.

00:20:43.521 --> 00:20:44.570
- Yeah, I read it.

00:20:44.570 --> 00:20:46.460
- Can I talk you through what we found?

00:20:46.460 --> 00:20:47.293
- Sure.

00:20:47.293 --> 00:20:52.293
- Okay, it appears that in
the data that we've collected,

00:20:52.480 --> 00:20:55.920
that drivers are maintaining
functional vigilance such that,

00:20:55.920 --> 00:20:58.600
we're looking at 18,000
disengagements from Autopilot,

00:20:58.600 --> 00:21:03.400
18,900, and annotating were they able

00:21:03.400 --> 00:21:05.780
to take over control in a timely manner.

00:21:05.780 --> 00:21:08.770
So they were there,
present, looking at the road

00:21:08.770 --> 00:21:11.460
to take over control, okay.

00:21:11.460 --> 00:21:15.670
So this goes against
what many would predict

00:21:15.670 --> 00:21:19.540
from the body of literature
on vigilance with automation.

00:21:19.540 --> 00:21:22.040
Now the question is, do you think

00:21:22.040 --> 00:21:24.900
these results hold across
the broader population.

00:21:24.900 --> 00:21:27.163
So, ours is just a small subset.

00:21:28.610 --> 00:21:32.930
One of the criticism is that,
there's a small minority

00:21:32.930 --> 00:21:36.120
of drivers that may be highly responsible,

00:21:36.120 --> 00:21:38.870
where their vigilance
decrement would increase

00:21:38.870 --> 00:21:40.390
with Autopilot use.

00:21:40.390 --> 00:21:42.610
- I think this is all
really gonna be swept,

00:21:42.610 --> 00:21:46.650
I mean, the system's improving so much,

00:21:46.650 --> 00:21:50.383
so fast, that this is gonna
be a moot point very soon.

00:21:52.140 --> 00:21:57.140
Where vigilance is, if
something's many times safer

00:21:57.950 --> 00:22:01.630
than a person, then adding a person does,

00:22:01.630 --> 00:22:04.583
the effect on safety is limited.

00:22:05.750 --> 00:22:08.823
And, in fact, it could be negative.

00:22:10.690 --> 00:22:15.000
- That's really interesting,
so the fact that a human may,

00:22:15.000 --> 00:22:18.500
some percent of the population may exhibit

00:22:18.500 --> 00:22:20.710
a vigilance decrement, will not affect

00:22:20.710 --> 00:22:22.400
overall statistics, numbers on safety?

00:22:22.400 --> 00:22:24.853
- No, in fact, I think it will become,

00:22:25.900 --> 00:22:29.290
very, very quickly, maybe even
towards the end of this year,

00:22:29.290 --> 00:22:32.100
but I would say, I'd be
shocked if it's not next year

00:22:32.100 --> 00:22:36.370
at the latest, that
having a human intervene

00:22:36.370 --> 00:22:37.923
will decrease safety.

00:22:39.750 --> 00:22:42.950
Decrease, like imagine
if you're in an elevator.

00:22:42.950 --> 00:22:45.730
Now it used to be that there
were elevator operators.

00:22:45.730 --> 00:22:48.400
And you couldn't go on
an elevator by yourself

00:22:48.400 --> 00:22:50.993
and work the lever to move between floors.

00:22:52.500 --> 00:22:57.010
And now nobody wants an elevator operator,

00:22:57.010 --> 00:23:00.520
because the automated elevator
that stops at the floors

00:23:00.520 --> 00:23:02.743
is much safer than the elevator operator.

00:23:04.060 --> 00:23:05.450
And in fact it would be quite dangerous

00:23:05.450 --> 00:23:06.870
to have someone with a lever

00:23:06.870 --> 00:23:09.830
that can move the elevator between floors.

00:23:09.830 --> 00:23:12.830
- So, that's a really powerful statement,

00:23:12.830 --> 00:23:14.710
and a really interesting one,

00:23:14.710 --> 00:23:16.960
but I also have to ask
from a user experience

00:23:16.960 --> 00:23:18.770
and from a safety perspective,

00:23:18.770 --> 00:23:21.310
one of the passions for me algorithmically

00:23:21.310 --> 00:23:26.020
is camera-based detection
of just sensing the human,

00:23:26.020 --> 00:23:27.830
but detecting what the
driver's looking at,

00:23:27.830 --> 00:23:30.850
cognitive load, body pose,
on the computer vision side

00:23:30.850 --> 00:23:32.300
that's a fascinating problem.

00:23:33.160 --> 00:23:34.910
And there's many in industry who believe

00:23:34.910 --> 00:23:37.570
you have to have camera-based
driver monitoring.

00:23:37.570 --> 00:23:39.850
Do you think there could be benefit gained

00:23:39.850 --> 00:23:41.760
from driver monitoring?

00:23:41.760 --> 00:23:46.660
- If you have a system that's
at or below a human level

00:23:46.660 --> 00:23:49.210
of reliability, then driver
monitoring makes sense.

00:23:50.260 --> 00:23:52.150
But if your system is dramatically better,

00:23:52.150 --> 00:23:55.900
more reliable than a human,
then driver monitoring

00:23:57.624 --> 00:23:59.500
does not help much.

00:23:59.500 --> 00:24:01.593
And, like I said,

00:24:05.791 --> 00:24:07.170
if you're in an elevator,
do you really want

00:24:07.170 --> 00:24:09.840
someone with a big
lever, some random person

00:24:09.840 --> 00:24:11.740
operating the elevator between floors?

00:24:13.020 --> 00:24:14.715
I wouldn't trust that.

00:24:14.715 --> 00:24:16.073
I would rather have the buttons.

00:24:17.500 --> 00:24:20.910
- Okay, you're optimistic
about the pace of improvement

00:24:20.910 --> 00:24:22.900
of the system, from what you've seen

00:24:22.900 --> 00:24:25.500
with the full self-driving car computer.

00:24:25.500 --> 00:24:27.550
- The rate of improvement is exponential.

00:24:28.570 --> 00:24:30.920
- So, one of the other very interesting

00:24:30.920 --> 00:24:33.830
design choices early on
that connects to this,

00:24:33.830 --> 00:24:38.300
is the operational design
domain of Autopilot.

00:24:38.300 --> 00:24:41.743
So, where Autopilot is
able to be turned on.

00:24:43.200 --> 00:24:47.190
So contrast another vehicle
system that we were studying

00:24:47.190 --> 00:24:49.540
is the Cadillac Super
Cruise system that's,

00:24:49.540 --> 00:24:51.520
in terms of ODD, very constrained

00:24:51.520 --> 00:24:53.600
to particular kinds of highways,

00:24:53.600 --> 00:24:56.500
well mapped, tested,
but it's much narrower

00:24:56.500 --> 00:24:58.713
than the ODD of Tesla vehicles.

00:25:00.840 --> 00:25:02.830
- It's like ADD (both laugh).

00:25:02.830 --> 00:25:06.493
- Yeah, that's good, that's a good line.

00:25:08.080 --> 00:25:12.010
What was the design decision in

00:25:12.010 --> 00:25:13.760
that different philosophy of thinking,

00:25:13.760 --> 00:25:15.600
where there's pros and cons.

00:25:15.600 --> 00:25:18.650
What we see with a wide ODD

00:25:18.650 --> 00:25:22.320
is Tesla drivers are able to explore more

00:25:22.320 --> 00:25:23.680
the limitations of the system,

00:25:23.680 --> 00:25:26.230
at least early on, and they understand,

00:25:26.230 --> 00:25:28.270
together with the
instrument cluster display,

00:25:28.270 --> 00:25:30.400
they start to understand
what are the capabilities,

00:25:30.400 --> 00:25:31.970
so that's a benefit.

00:25:31.970 --> 00:25:34.900
The con is you're letting drivers

00:25:34.900 --> 00:25:37.227
use it basically anywhere--

00:25:38.710 --> 00:25:40.930
- Anywhere that it can
detect lanes with confidence.

00:25:40.930 --> 00:25:43.093
- Lanes, was there a philosophy,

00:25:44.640 --> 00:25:46.580
design decisions that were challenging,

00:25:46.580 --> 00:25:48.170
that were being made there?

00:25:48.170 --> 00:25:53.170
Or from the very beginning
was that done on purpose,

00:25:53.600 --> 00:25:54.433
with intent?

00:25:56.110 --> 00:25:57.900
- Frankly it's pretty crazy letting people

00:25:57.900 --> 00:26:01.563
drive a two-ton death machine manually.

00:26:02.910 --> 00:26:06.380
That's crazy, like, in the
future will people be like,

00:26:06.380 --> 00:26:09.100
I can't believe anyone
was just allowed to drive

00:26:09.100 --> 00:26:13.040
one of these two-ton death machines,

00:26:13.040 --> 00:26:14.510
and they just drive wherever they wanted.

00:26:14.510 --> 00:26:16.343
Just like elevators, you could just move

00:26:16.343 --> 00:26:18.240
that elevator with that
lever wherever you wanted,

00:26:18.240 --> 00:26:20.583
can stop it halfway
between floors if you want.

00:26:22.520 --> 00:26:25.223
It's pretty crazy, so,

00:26:27.310 --> 00:26:30.210
it's gonna seem like a
mad thing in the future

00:26:30.210 --> 00:26:31.883
that people were driving cars.

00:26:32.970 --> 00:26:34.620
- So I have a bunch of questions about

00:26:34.620 --> 00:26:37.387
the human psychology,
about behavior and so on--

00:26:38.720 --> 00:26:41.050
- That's moot, it's totally moot.

00:26:41.050 --> 00:26:45.523
- Because you have faith in the AI system,

00:26:46.370 --> 00:26:50.520
not faith but, both on the hardware side

00:26:50.520 --> 00:26:52.960
and the deep learning approach
of learning from data,

00:26:52.960 --> 00:26:55.690
will make it just far safer than humans.

00:26:55.690 --> 00:26:57.270
- Yeah, exactly.

00:26:57.270 --> 00:26:59.470
- Recently there were a few hackers,

00:26:59.470 --> 00:27:02.170
who tricked Autopilot to
act in unexpected ways

00:27:02.170 --> 00:27:03.970
for the adversarial examples.

00:27:03.970 --> 00:27:06.540
So we all know that neural network systems

00:27:06.540 --> 00:27:08.500
are very sensitive to minor disturbances,

00:27:08.500 --> 00:27:11.270
these adversarial examples, on input.

00:27:11.270 --> 00:27:12.300
Do you think it's possible

00:27:12.300 --> 00:27:14.140
to defend against something like this,

00:27:14.140 --> 00:27:19.120
for the industry?
- Sure (both laugh), yeah.

00:27:19.120 --> 00:27:22.693
- Can you elaborate on the
confidence behind that answer?

00:27:25.479 --> 00:27:26.670
- A neural net is just basically a bunch

00:27:26.670 --> 00:27:28.397
of matrix math.

00:27:28.397 --> 00:27:30.743
But you have to be a very sophisticated,

00:27:31.750 --> 00:27:33.370
somebody who really
understands neural nets

00:27:33.370 --> 00:27:37.400
and basically reverse-engineer
how the matrix

00:27:37.400 --> 00:27:40.540
is being built, and then
create a little thing

00:27:40.540 --> 00:27:44.160
that's just exactly causes the matrix math

00:27:44.160 --> 00:27:45.460
to be slightly off.

00:27:45.460 --> 00:27:48.873
But it's very easy to
block that by having,

00:27:49.730 --> 00:27:51.850
what would basically negative recognition,

00:27:51.850 --> 00:27:53.950
it's like if the system sees something

00:27:53.950 --> 00:27:57.753
that looks like a matrix hack, exclude it.

00:27:59.940 --> 00:28:01.643
It's such a easy thing to do.

00:28:02.730 --> 00:28:06.260
- So learn both on the valid
data and the invalid data,

00:28:06.260 --> 00:28:08.260
so basically learn on
the adversarial examples

00:28:08.260 --> 00:28:09.790
to be able to exclude them.

00:28:09.790 --> 00:28:12.360
- Yeah, you like basically wanna both know

00:28:12.360 --> 00:28:16.160
what is a car and what
is definitely not a car.

00:28:16.160 --> 00:28:17.880
And you train for, this is a car,

00:28:17.880 --> 00:28:19.130
and this is definitely not a car.

00:28:19.130 --> 00:28:20.680
Those are two different things.

00:28:21.710 --> 00:28:23.870
People have no idea of neural nets really,

00:28:23.870 --> 00:28:25.550
They probably think neural nets involves,

00:28:25.550 --> 00:28:27.780
a fishing net or something (Lex laughs).

00:28:29.220 --> 00:28:34.220
- So, as you know, taking
a step beyond just Tesla

00:28:34.840 --> 00:28:37.810
and Autopilot, current
deep learning approaches

00:28:37.810 --> 00:28:40.510
still seem, in some ways,

00:28:40.510 --> 00:28:44.690
to be far from general
intelligence systems.

00:28:44.690 --> 00:28:46.770
Do you think the current approaches

00:28:46.770 --> 00:28:49.780
will take us to general intelligence,

00:28:49.780 --> 00:28:53.873
or do totally new ideas
need to be invented?

00:28:55.720 --> 00:28:57.260
- I think we're missing a few key ideas

00:28:57.260 --> 00:29:01.913
for artificial general intelligence.

00:29:05.180 --> 00:29:07.393
But it's gonna be upon us very quickly,

00:29:08.880 --> 00:29:12.480
and then we'll need to
figure out what shall we do,

00:29:12.480 --> 00:29:13.973
if we even have that choice.

00:29:16.340 --> 00:29:18.360
It's amazing how people
can't differentiate

00:29:18.360 --> 00:29:22.760
between, say, the narrow
AI that allows a car

00:29:22.760 --> 00:29:27.760
to figure out what a lane
line is, and navigate streets,

00:29:27.920 --> 00:29:30.540
versus general intelligence.

00:29:30.540 --> 00:29:33.170
Like these are just very different things.

00:29:33.170 --> 00:29:35.840
Like your toaster and your
computer are both machines,

00:29:35.840 --> 00:29:38.680
but one's much more
sophisticated than another.

00:29:38.680 --> 00:29:41.470
- You're confident with
Tesla you can create

00:29:41.470 --> 00:29:43.670
the world's best toaster--

00:29:43.670 --> 00:29:45.260
- The world's best toaster, yes.

00:29:45.260 --> 00:29:46.930
The world's best self-driving...

00:29:50.240 --> 00:29:54.513
yes, to me right now this
seems game, set and match.

00:29:55.409 --> 00:29:57.230
I mean, I don't want us to be complacent

00:29:57.230 --> 00:29:59.010
or over-confident, but that's what it,

00:29:59.010 --> 00:30:02.690
that is just literally
how it appears right now,

00:30:02.690 --> 00:30:06.360
I could be wrong, but it
appears to be the case

00:30:06.360 --> 00:30:09.683
that Tesla is vastly ahead of everyone.

00:30:10.950 --> 00:30:12.470
- Do you think we will ever create

00:30:12.470 --> 00:30:16.390
an AI system that we can
love, and loves us back

00:30:16.390 --> 00:30:18.790
in a deep meaningful way,
like in the movie Her?

00:30:20.290 --> 00:30:23.500
- I think AI will
capable of convincing you

00:30:23.500 --> 00:30:25.880
to fall in love with it very well.

00:30:25.880 --> 00:30:27.780
- And that's different than us humans?

00:30:29.340 --> 00:30:30.340
- You know, we start getting into

00:30:30.340 --> 00:30:33.090
a metaphysical question of, do emotions

00:30:33.090 --> 00:30:34.700
and thoughts exist in a different realm

00:30:34.700 --> 00:30:35.650
than the physical?

00:30:35.650 --> 00:30:38.070
And maybe they do, maybe
they don't, I don't know.

00:30:38.070 --> 00:30:41.903
But from a physics standpoint,
I tend to think of things,

00:30:42.790 --> 00:30:47.490
you know, like physics was
my main sort of training,

00:30:47.490 --> 00:30:51.450
and from a physics
standpoint, essentially,

00:30:51.450 --> 00:30:54.260
if it loves you in a
way that you can't tell

00:30:54.260 --> 00:30:56.253
whether it's real or not, it is real.

00:30:57.660 --> 00:30:59.140
- That's a physics view of love.

00:30:59.140 --> 00:31:04.140
- Yeah (laughs), if you
cannot prove that it does not,

00:31:06.000 --> 00:31:08.120
if there's no test that you can apply

00:31:08.960 --> 00:31:10.133
that would make it,

00:31:13.820 --> 00:31:15.940
allow you to tell the difference,

00:31:15.940 --> 00:31:17.170
then there is no difference.

00:31:17.170 --> 00:31:20.913
- Right, and it's similar to
seeing our world a simulation,

00:31:20.913 --> 00:31:23.500
they may not be a test to
tell the difference between

00:31:23.500 --> 00:31:24.978
what the real world
- Yes.

00:31:24.978 --> 00:31:26.320
- and the simulation, and therefore,

00:31:26.320 --> 00:31:27.890
from a physics perspective,

00:31:27.890 --> 00:31:29.380
it might as well be the same thing.

00:31:29.380 --> 00:31:32.190
- Yes, and there may
be ways to test whether

00:31:32.190 --> 00:31:34.780
it's a simulation, there might be,

00:31:34.780 --> 00:31:36.060
I'm not saying there aren't.

00:31:36.060 --> 00:31:37.390
But you could certainly imagine that

00:31:37.390 --> 00:31:39.380
a simulation could correct,

00:31:39.380 --> 00:31:41.530
that once an entity in
the simulation found

00:31:41.530 --> 00:31:43.120
a way to detect the simulation,

00:31:43.120 --> 00:31:47.430
it could either pause the simulation,

00:31:47.430 --> 00:31:49.930
start a new simulation, or
do one of many other things

00:31:49.930 --> 00:31:51.683
that then corrects for that error.

00:31:53.170 --> 00:31:57.540
- So when, maybe you,
or somebody else creates

00:31:57.540 --> 00:32:02.540
an AGI system, and you get
to ask her one question,

00:32:03.020 --> 00:32:04.420
what would that question be?

00:32:17.050 --> 00:32:18.713
- What's outside the simulation?

00:32:21.730 --> 00:32:23.173
- Elon, thank you so
much for talking today,

00:32:23.173 --> 00:32:24.290
it's a pleasure.

00:32:24.290 --> 00:32:25.440
- All right, thank you.

